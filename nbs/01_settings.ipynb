{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "!pip install -r ../requirements.txt > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "from deeppavlov.core.common.paths import get_settings_path\n",
    "from deeppavlov import configs, build_model, train_model\n",
    "import json\n",
    "from os import path, popen, mkdir\n",
    "from shutil import copyfile\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from pathlib import Path\n",
    "import logging\n",
    "from collections import defaultdict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "09:08:43 INFO:Hello! Welcome to our automated dialog system!\n",
      "09:08:43 DEBUG: Debug Log Active\n",
      "09:08:43 WARNING: Warning Log Active\n",
      "09:08:43 ERROR: Error Log Active \n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "logging.basicConfig(\n",
    "    #filename='example.log',\n",
    "    format='%(asctime)s %(levelname)s:%(message)s',\n",
    "    level=logging.DEBUG,\n",
    "    datefmt='%I:%M:%S'\n",
    ")\n",
    "\n",
    "logging.info(\"Hello! Welcome to our automated dialog system!\")\n",
    "logging.debug(\" Debug Log Active\")\n",
    "logging.warning(' Warning Log Active')\n",
    "logging.error(' Error Log Active ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def change_log_config():\n",
    "    '''Change Deeppavlov configuration files to ERROR mode\n",
    "    '''\n",
    "    settings_file = path.join(get_settings_path(), 'log_config.json')\n",
    "    #logs_key = 'disable_existing_loggers'\n",
    "\n",
    "    settings_json = json.load(open(settings_file))\n",
    "    settings_json['handlers']['file']['level'] = 'ERROR'\n",
    "    settings_json['handlers']['stderr']['level'] = 'ERROR'\n",
    "    settings_json['handlers']['stdout']['level'] = 'ERROR'\n",
    "    settings_json['handlers']['uvicorn_handler']['level'] = 'ERROR'\n",
    "\n",
    "    settings_json['loggers']['deeppavlov']['level'] = 'ERROR'\n",
    "    settings_json['loggers']['deeppavlov']['propagate'] = True\n",
    "\n",
    "    settings_json['loggers']['uvicorn.access']['level'] = 'ERROR'\n",
    "    settings_json['loggers']['uvicorn.access']['propagate'] = True\n",
    "\n",
    "    settings_json['loggers']['uvicorn.error']['level'] = 'ERROR'\n",
    "    settings_json['loggers']['uvicorn.error']['propagate'] = True\n",
    "\n",
    "    #settings_json[logs_key] = False\n",
    "\n",
    "    json.dump(settings_json, open(settings_file, 'w'))\n",
    "\n",
    "\n",
    "def run_shell_installs():\n",
    "    ''' Run install commands\n",
    "    '''\n",
    "    logging.info(f'..Installing NLP libraries')\n",
    "    change_log_config()\n",
    "\n",
    "    command_strings = (\n",
    "        ' pip install deeppavlov', ' python -m deeppavlov install squad',\n",
    "        ' python -m deeppavlov install squad_bert',\n",
    "        ' python -m deeppavlov install fasttext_avg_autofaq',\n",
    "        ' python -m deeppavlov install fasttext_tfidf_autofaq',\n",
    "        ' python -m deeppavlov install tfidf_autofaq',\n",
    "        ' python -m deeppavlov install tfidf_logreg_autofaq ',\n",
    "        ' python -m deeppavlov install tfidf_logreg_en_faq'\n",
    "    )\n",
    "    for command in command_strings:\n",
    "\n",
    "        logging.debug(command)\n",
    "        logging.debug(popen(command).read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dialog System\n",
    "> Question Answering Automated Dialog System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "09:08:43 INFO:..Installing NLP libraries\n",
      "09:08:43 DEBUG: pip install deeppavlov\n",
      "09:08:46 DEBUG:Requirement already satisfied: deeppavlov in /opt/conda/lib/python3.7/site-packages (0.10.0)\n",
      "Requirement already satisfied: numpy==1.18.0 in /opt/conda/lib/python3.7/site-packages (from deeppavlov) (1.18.0)\n",
      "Requirement already satisfied: sacremoses==0.0.35 in /opt/conda/lib/python3.7/site-packages (from deeppavlov) (0.0.35)\n",
      "Requirement already satisfied: pymorphy2-dicts-ru in /opt/conda/lib/python3.7/site-packages (from deeppavlov) (2.4.404381.4453942)\n",
      "Requirement already satisfied: nltk==3.4.5 in /opt/conda/lib/python3.7/site-packages (from deeppavlov) (3.4.5)\n",
      "Requirement already satisfied: uvicorn==0.11.1 in /opt/conda/lib/python3.7/site-packages (from deeppavlov) (0.11.1)\n",
      "Requirement already satisfied: ruamel.yaml==0.15.100 in /opt/conda/lib/python3.7/site-packages (from deeppavlov) (0.15.100)\n",
      "Requirement already satisfied: h5py==2.10.0 in /opt/conda/lib/python3.7/site-packages (from deeppavlov) (2.10.0)\n",
      "Requirement already satisfied: Cython==0.29.14 in /opt/conda/lib/python3.7/site-packages (from deeppavlov) (0.29.14)\n",
      "Requirement already satisfied: scikit-learn==0.21.2 in /opt/conda/lib/python3.7/site-packages (from deeppavlov) (0.21.2)\n",
      "Requirement already satisfied: tqdm==4.41.1 in /opt/conda/lib/python3.7/site-packages (from deeppavlov) (4.41.1)\n",
      "Requirement already satisfied: pytz==2019.1 in /opt/conda/lib/python3.7/site-packages (from deeppavlov) (2019.1)\n",
      "Requirement already satisfied: requests==2.22.0 in /opt/conda/lib/python3.7/site-packages (from deeppavlov) (2.22.0)\n",
      "Requirement already satisfied: pymorphy2==0.8 in /opt/conda/lib/python3.7/site-packages (from deeppavlov) (0.8)\n",
      "Requirement already satisfied: aio-pika==6.4.1 in /opt/conda/lib/python3.7/site-packages (from deeppavlov) (6.4.1)\n",
      "Requirement already satisfied: overrides==2.7.0 in /opt/conda/lib/python3.7/site-packages (from deeppavlov) (2.7.0)\n",
      "Requirement already satisfied: scipy==1.4.1 in /opt/conda/lib/python3.7/site-packages (from deeppavlov) (1.4.1)\n",
      "Requirement already satisfied: pydantic==1.3 in /opt/conda/lib/python3.7/site-packages (from deeppavlov) (1.3)\n",
      "Requirement already satisfied: pytelegrambotapi==3.6.7 in /opt/conda/lib/python3.7/site-packages (from deeppavlov) (3.6.7)\n",
      "Requirement already satisfied: fastapi==0.47.1 in /opt/conda/lib/python3.7/site-packages (from deeppavlov) (0.47.1)\n",
      "Requirement already satisfied: rusenttokenize==0.0.5 in /opt/conda/lib/python3.7/site-packages (from deeppavlov) (0.0.5)\n",
      "Requirement already satisfied: pyopenssl==19.1.0 in /opt/conda/lib/python3.7/site-packages (from deeppavlov) (19.1.0)\n",
      "Requirement already satisfied: pandas==0.25.3 in /opt/conda/lib/python3.7/site-packages (from deeppavlov) (0.25.3)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from sacremoses==0.0.35->deeppavlov) (1.14.0)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from sacremoses==0.0.35->deeppavlov) (0.15.1)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from sacremoses==0.0.35->deeppavlov) (7.1.2)\n",
      "Requirement already satisfied: websockets==8.* in /opt/conda/lib/python3.7/site-packages (from uvicorn==0.11.1->deeppavlov) (8.1)\n",
      "Requirement already satisfied: httptools==0.0.13; sys_platform != \"win32\" and sys_platform != \"cygwin\" and platform_python_implementation != \"pypy\" in /opt/conda/lib/python3.7/site-packages (from uvicorn==0.11.1->deeppavlov) (0.0.13)\n",
      "Requirement already satisfied: h11<0.10,>=0.8 in /opt/conda/lib/python3.7/site-packages (from uvicorn==0.11.1->deeppavlov) (0.9.0)\n",
      "Requirement already satisfied: uvloop>=0.14.0; sys_platform != \"win32\" and sys_platform != \"cygwin\" and platform_python_implementation != \"pypy\" in /opt/conda/lib/python3.7/site-packages (from uvicorn==0.11.1->deeppavlov) (0.14.0)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests==2.22.0->deeppavlov) (1.25.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests==2.22.0->deeppavlov) (2019.11.28)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests==2.22.0->deeppavlov) (3.0.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests==2.22.0->deeppavlov) (2.8)\n",
      "Requirement already satisfied: pymorphy2-dicts<3.0,>=2.4 in /opt/conda/lib/python3.7/site-packages (from pymorphy2==0.8->deeppavlov) (2.4.393442.3710985)\n",
      "Requirement already satisfied: docopt>=0.6 in /opt/conda/lib/python3.7/site-packages (from pymorphy2==0.8->deeppavlov) (0.6.2)\n",
      "Requirement already satisfied: dawg-python>=0.7 in /opt/conda/lib/python3.7/site-packages (from pymorphy2==0.8->deeppavlov) (0.7.2)\n",
      "Requirement already satisfied: aiormq<4,>=3.2.0 in /opt/conda/lib/python3.7/site-packages (from aio-pika==6.4.1->deeppavlov) (3.2.2)\n",
      "Requirement already satisfied: yarl in /opt/conda/lib/python3.7/site-packages (from aio-pika==6.4.1->deeppavlov) (1.4.2)\n",
      "Requirement already satisfied: starlette<=0.12.9,>=0.12.9 in /opt/conda/lib/python3.7/site-packages (from fastapi==0.47.1->deeppavlov) (0.12.9)\n",
      "Requirement already satisfied: cryptography>=2.8 in /opt/conda/lib/python3.7/site-packages (from pyopenssl==19.1.0->deeppavlov) (2.8)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /opt/conda/lib/python3.7/site-packages (from pandas==0.25.3->deeppavlov) (2.8.1)\n",
      "Requirement already satisfied: pamqp==2.3.0 in /opt/conda/lib/python3.7/site-packages (from aiormq<4,>=3.2.0->aio-pika==6.4.1->deeppavlov) (2.3.0)\n",
      "Requirement already satisfied: multidict>=4.0 in /opt/conda/lib/python3.7/site-packages (from yarl->aio-pika==6.4.1->deeppavlov) (4.7.6)\n",
      "Requirement already satisfied: cffi!=1.11.3,>=1.8 in /opt/conda/lib/python3.7/site-packages (from cryptography>=2.8->pyopenssl==19.1.0->deeppavlov) (1.14.0)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.7/site-packages (from cffi!=1.11.3,>=1.8->cryptography>=2.8->pyopenssl==19.1.0->deeppavlov) (2.20)\n",
      "\n",
      "09:08:46 DEBUG: python -m deeppavlov install squad\n",
      "09:08:49 DEBUG:Requirement already satisfied: tensorflow==1.15.2 in /opt/conda/lib/python3.7/site-packages (1.15.2)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.15.2) (3.2.1)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.15.2) (1.12.1)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.15.2) (1.1.2)\n",
      "Requirement already satisfied: numpy<2.0,>=1.16.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.15.2) (1.18.0)\n",
      "Requirement already satisfied: protobuf>=3.6.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.15.2) (3.12.2)\n",
      "Requirement already satisfied: gast==0.2.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.15.2) (0.2.2)\n",
      "Requirement already satisfied: google-pasta>=0.1.6 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.15.2) (0.2.0)\n",
      "Requirement already satisfied: six>=1.10.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.15.2) (1.14.0)\n",
      "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.15.2) (0.34.2)\n",
      "Requirement already satisfied: tensorflow-estimator==1.15.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.15.2) (1.15.1)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.15.2) (1.29.0)\n",
      "Requirement already satisfied: keras-applications>=1.0.8 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.15.2) (1.0.8)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.15.2) (1.1.0)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.15.2) (0.9.0)\n",
      "Requirement already satisfied: tensorboard<1.16.0,>=1.15.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.15.2) (1.15.0)\n",
      "Requirement already satisfied: astor>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.15.2) (0.8.1)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from protobuf>=3.6.1->tensorflow==1.15.2) (46.0.0.post20200311)\n",
      "Requirement already satisfied: h5py in /opt/conda/lib/python3.7/site-packages (from keras-applications>=1.0.8->tensorflow==1.15.2) (2.10.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.7/site-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (3.2.2)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /opt/conda/lib/python3.7/site-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (1.0.1)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /opt/conda/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (1.5.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (3.1.0)\n",
      "\n",
      "09:08:49 DEBUG: python -m deeppavlov install squad_bert\n",
      "09:08:57 DEBUG:Requirement already satisfied: tensorflow==1.15.2 in /opt/conda/lib/python3.7/site-packages (1.15.2)\n",
      "Requirement already satisfied: astor>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.15.2) (0.8.1)\n",
      "Requirement already satisfied: google-pasta>=0.1.6 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.15.2) (0.2.0)\n",
      "Requirement already satisfied: gast==0.2.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.15.2) (0.2.2)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.15.2) (1.29.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.15.2) (1.1.2)\n",
      "Requirement already satisfied: numpy<2.0,>=1.16.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.15.2) (1.18.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.15.2) (3.2.1)\n",
      "Requirement already satisfied: tensorflow-estimator==1.15.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.15.2) (1.15.1)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.15.2) (1.1.0)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.15.2) (1.12.1)\n",
      "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.15.2) (0.34.2)\n",
      "Requirement already satisfied: six>=1.10.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.15.2) (1.14.0)\n",
      "Requirement already satisfied: tensorboard<1.16.0,>=1.15.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.15.2) (1.15.0)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.15.2) (0.9.0)\n",
      "Requirement already satisfied: protobuf>=3.6.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.15.2) (3.12.2)\n",
      "Requirement already satisfied: keras-applications>=1.0.8 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.15.2) (1.0.8)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (46.0.0.post20200311)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.7/site-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (3.2.2)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /opt/conda/lib/python3.7/site-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (1.0.1)\n",
      "Requirement already satisfied: h5py in /opt/conda/lib/python3.7/site-packages (from keras-applications>=1.0.8->tensorflow==1.15.2) (2.10.0)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /opt/conda/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (1.5.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (3.1.0)\n",
      "Collecting git+https://github.com/deepmipt/bert.git@feat/multi_gpu\n",
      "  Cloning https://github.com/deepmipt/bert.git (to revision feat/multi_gpu) to /tmp/pip-req-build-d2gazx8s\n",
      "Requirement already satisfied (use --upgrade to upgrade): bert-dp==1.0 from git+https://github.com/deepmipt/bert.git@feat/multi_gpu in /opt/conda/lib/python3.7/site-packages\n",
      "Building wheels for collected packages: bert-dp\n",
      "  Building wheel for bert-dp (setup.py): started\n",
      "  Building wheel for bert-dp (setup.py): finished with status 'done'\n",
      "  Created wheel for bert-dp: filename=bert_dp-1.0-py3-none-any.whl size=23580 sha256=eef14d7cb585547b2cca8fa9bd8d298a9240b5c271db0013f172f4af5bcc289d\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-ji69l2y6/wheels/44/29/b2/ee614cb7f97ba5c2d220029eaede3af4b74331ad31d6e2f4eb\n",
      "Successfully built bert-dp\n",
      "\n",
      "09:08:57 DEBUG: python -m deeppavlov install fasttext_avg_autofaq\n",
      "09:09:01 DEBUG:Requirement already satisfied: fasttext==0.9.1 in /opt/conda/lib/python3.7/site-packages (0.9.1)\n",
      "Requirement already satisfied: pybind11>=2.2 in /opt/conda/lib/python3.7/site-packages (from fasttext==0.9.1) (2.5.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from fasttext==0.9.1) (1.18.0)\n",
      "Requirement already satisfied: setuptools>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from fasttext==0.9.1) (46.0.0.post20200311)\n",
      "\n",
      "09:09:01 DEBUG: python -m deeppavlov install fasttext_tfidf_autofaq\n",
      "09:09:05 DEBUG:Requirement already satisfied: fasttext==0.9.1 in /opt/conda/lib/python3.7/site-packages (0.9.1)\n",
      "Requirement already satisfied: pybind11>=2.2 in /opt/conda/lib/python3.7/site-packages (from fasttext==0.9.1) (2.5.0)\n",
      "Requirement already satisfied: setuptools>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from fasttext==0.9.1) (46.0.0.post20200311)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from fasttext==0.9.1) (1.18.0)\n",
      "\n",
      "09:09:05 DEBUG: python -m deeppavlov install tfidf_autofaq\n",
      "09:09:06 DEBUG:\n",
      "09:09:06 DEBUG: python -m deeppavlov install tfidf_logreg_autofaq \n",
      "09:09:07 DEBUG:\n",
      "09:09:07 DEBUG: python -m deeppavlov install tfidf_logreg_en_faq\n",
      "09:09:14 DEBUG:Requirement already satisfied: spacy==2.2.3 in /opt/conda/lib/python3.7/site-packages (2.2.3)\n",
      "Requirement already satisfied: srsly<1.1.0,>=0.1.0 in /opt/conda/lib/python3.7/site-packages (from spacy==2.2.3) (1.0.2)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /opt/conda/lib/python3.7/site-packages (from spacy==2.2.3) (1.0.0)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from spacy==2.2.3) (0.6.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/conda/lib/python3.7/site-packages (from spacy==2.2.3) (2.22.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /opt/conda/lib/python3.7/site-packages (from spacy==2.2.3) (1.18.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from spacy==2.2.3) (2.0.3)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from spacy==2.2.3) (3.0.2)\n",
      "Requirement already satisfied: thinc<7.4.0,>=7.3.0 in /opt/conda/lib/python3.7/site-packages (from spacy==2.2.3) (7.3.1)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from spacy==2.2.3) (46.0.0.post20200311)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /opt/conda/lib/python3.7/site-packages (from spacy==2.2.3) (1.1.3)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.7/site-packages (from spacy==2.2.3) (1.0.2)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from spacy==2.2.3) (0.4.1)\n",
      "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /opt/conda/lib/python3.7/site-packages (from catalogue<1.1.0,>=0.0.7->spacy==2.2.3) (1.5.0)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy==2.2.3) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy==2.2.3) (1.25.7)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy==2.2.3) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy==2.2.3) (2019.11.28)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /opt/conda/lib/python3.7/site-packages (from thinc<7.4.0,>=7.3.0->spacy==2.2.3) (4.41.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy==2.2.3) (3.1.0)\n",
      "Requirement already satisfied: en_core_web_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz#egg=en_core_web_sm==2.2.5 in /opt/conda/lib/python3.7/site-packages (2.2.5)\n",
      "Requirement already satisfied: spacy>=2.2.2 in /opt/conda/lib/python3.7/site-packages (from en_core_web_sm==2.2.5) (2.2.3)\n",
      "Requirement already satisfied: thinc<7.4.0,>=7.3.0 in /opt/conda/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.3.1)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n",
      "Requirement already satisfied: srsly<1.1.0,>=0.1.0 in /opt/conda/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /opt/conda/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.18.0)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /opt/conda/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.6.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.2)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (46.0.0.post20200311)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/conda/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.22.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.3)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /opt/conda/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /opt/conda/lib/python3.7/site-packages (from thinc<7.4.0,>=7.3.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (4.41.1)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2019.11.28)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.8)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.25.7)\n",
      "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /opt/conda/lib/python3.7/site-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.5.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.1.0)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "run_shell_installs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def action_over_list_f(arr, v):\n",
    "    ''' v[0] and v[1] are dictionaries\n",
    "        arr is array of dictionaries \n",
    "    '''\n",
    "\n",
    "    k_id, v_id = next(iter(v[0].items()))\n",
    "\n",
    "    for p, a in enumerate(arr):\n",
    "        if k_id in a.keys() and a[k_id] == v_id:\n",
    "            for k_rep, v_rep in v[1].items():\n",
    "                arr[p][k_rep] = v_rep\n",
    "\n",
    "\n",
    "def replacement_f(model_config, **args):\n",
    "    '''Replaces the model config dictionary with new values\n",
    "    '''\n",
    "    for k, v in args.items():\n",
    "        if isinstance(v, dict):\n",
    "            replacement_f(model_config[k], **v)\n",
    "        if isinstance(v, str):\n",
    "            model_config[k] = v\n",
    "        if isinstance(model_config[k], list):\n",
    "            action_over_list_f(model_config[k], v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test action_over_list_f\n",
    "from random import randint\n",
    "\n",
    "\n",
    "def gen_list_keys_for_tests():\n",
    "    '''This is functio is used for tests\n",
    "    '''\n",
    "\n",
    "    str_n = lambda x: f'{x}_{randint(1,10):1}'\n",
    "    gen_dict_list = lambda: {\n",
    "        'id': str_n('id'),\n",
    "        'key1': str_n('v1'),\n",
    "        'key2': str_n('v2'),\n",
    "        'key3': str_n('v3')\n",
    "    }\n",
    "\n",
    "    pipe_list = [gen_dict_list() for _ in range(randint(3, 10))]\n",
    "\n",
    "    rand_id = pipe_list[randint(0, len(pipe_list) - 1)]['id']\n",
    "    rand_key = f'key{randint(1, 3)}' \n",
    "\n",
    "    new_rand_val = str_n('new')\n",
    "    args = {\n",
    "        'chains': {\n",
    "            'pipe': [{\n",
    "                'id': rand_id\n",
    "            }, {\n",
    "               rand_key : new_rand_val\n",
    "            }]\n",
    "        }\n",
    "    }\n",
    "\n",
    "    return pipe_list, rand_id, rand_key, args, new_rand_val\n",
    "\n",
    "\n",
    "def test_action_over_list_f():\n",
    "\n",
    "\n",
    "    pipe_list, rand_id, rand_key, args, new_rand_val = gen_list_keys_for_tests()\n",
    "\n",
    "    assert all(\n",
    "        new_rand_val not in pipe_elem.values() for pipe_elem in pipe_list\n",
    "    )\n",
    "\n",
    "    action_over_list_f(pipe_list, args['chains']['pipe'])\n",
    "\n",
    "    assert any(\n",
    "        rand_key in pipe_elem.keys() and\n",
    "        new_rand_val in pipe_elem.values() for pipe_elem in pipe_list\n",
    "    )\n",
    "\n",
    "\n",
    "def test_replacement_f_list():\n",
    "\n",
    "    pipe_list, rand_id, rand_key, args, new_rand_val = gen_list_keys_for_tests()\n",
    "\n",
    "    mod_conf = {'chains': {'pipe': pipe_list}}\n",
    "\n",
    "    assert all(\n",
    "        new_rand_val not in pipe_elem.values()\n",
    "        for pipe_elem in mod_conf['chains']['pipe']\n",
    "    )\n",
    "\n",
    "    replacement_f(model_config=mod_conf, **args)\n",
    "    assert any(\n",
    "        rand_key in pipe_elem.keys() and\n",
    "        new_rand_val in pipe_elem.values()\n",
    "        for pipe_elem in mod_conf['chains']['pipe']\n",
    "    )\n",
    "\n",
    "\n",
    "def test_replacement_f_val():\n",
    "    args = {'key3': 'newvalue'}\n",
    "    mod_conf = {'key1': 'val1', 'key2': 'val2', 'key3': 'val3'}\n",
    "    replacement_f(model_config=mod_conf, **args)\n",
    "    assert all(\n",
    "        arg_k in mod_conf.keys() and arg_v in mod_conf.values()\n",
    "        for arg_k, arg_v in args.items()\n",
    "    )\n",
    "\n",
    "\n",
    "def test_replacement_f_dict():\n",
    "    args = {'1_key_3': {'2_key_2': 'newvalue'}}\n",
    "    mod_conf = {'1_key_3': {'2_key_2': 'oldvalue'}, '0_key_': '0_val'}\n",
    "    replacement_f(model_config=mod_conf, **args)\n",
    "    assert mod_conf['1_key_3']['2_key_2'] == 'newvalue'\n",
    "\n",
    "\n",
    "test_action_over_list_f()\n",
    "test_replacement_f_list()\n",
    "test_replacement_f_val()\n",
    "test_replacement_f_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def updates_faq_config_file(\n",
    "    configs_path,\n",
    "    **args\n",
    "):\n",
    "    '''Updates deepplavov json config file \n",
    "    '''\n",
    "    #set FAQ data in config file\n",
    "    model_config = json.load(open(configs_path))\n",
    "\n",
    "    if 'data_url' in model_config['dataset_reader']:\n",
    "        del model_config['dataset_reader']['data_url']\n",
    "\n",
    "    replacement_f(model_config=model_config,**args)\n",
    "\n",
    "    json.dump(model_config, open(configs_path, 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test updates_faq_config_file\n",
    "import tempfile\n",
    "from shutil import copyfile\n",
    "\n",
    "\n",
    "def gen_list_keys_for_tests():\n",
    "\n",
    "    str_n = lambda x: f'{x}_{randint(1,10):1}'\n",
    "    gen_dict_list = lambda: {\n",
    "        'id': str_n('id'),\n",
    "        'key1': str_n('v1'),\n",
    "        'key2': str_n('v2'),\n",
    "        'key3': str_n('v3')\n",
    "    }\n",
    "\n",
    "    pipe_list = [gen_dict_list() for _ in range(randint(3, 10))]\n",
    "\n",
    "    rand_id = pipe_list[randint(0, len(pipe_list) - 1)]['id']\n",
    "    rand_key =  f'key{randint(1, 3)}' \n",
    "\n",
    "    new_rand_val = str_n('new')\n",
    "    pipe_dict = {'pipe': [{'id': rand_id}, {rand_key: new_rand_val}]}\n",
    "    args = {'chainer': pipe_dict}\n",
    "\n",
    "    return pipe_list, rand_id, rand_key, args, new_rand_val\n",
    "\n",
    "\n",
    "def test_updates_faq_config_file_update_string():\n",
    "\n",
    "    with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "\n",
    "        tmp_config_file = path.join(tmpdirname, 'tmp_file.json')\n",
    "\n",
    "        copyfile(configs.faq.tfidf_logreg_en_faq, tmp_config_file)\n",
    "\n",
    "        assert path.isfile(tmp_config_file)\n",
    "\n",
    "        updates_faq_config_file(\n",
    "            configs_path=tmp_config_file,\n",
    "            dataset_reader={'data_path': 'fictional_csv_file.csv'}\n",
    "        )\n",
    "\n",
    "        config_json = json.load(open(tmp_config_file))\n",
    "        assert 'data_path' in config_json['dataset_reader']\n",
    "\n",
    "\n",
    "def test_updates_faq_config_file_update_list():\n",
    "\n",
    "    with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "\n",
    "        tmp_config_file = path.join(tmpdirname, 'tmp_file.json')\n",
    "\n",
    "        pipe_list, rand_id, rand_key, args, new_rand_val = gen_list_keys_for_tests(\n",
    "        )\n",
    "        mod_conf = {\n",
    "            'chainer': {\n",
    "                'pipe': pipe_list\n",
    "            },\n",
    "            'dataset_reader': 'dataset_reader_dictionary'\n",
    "        }\n",
    "\n",
    "        json.dump(mod_conf, open(tmp_config_file, 'w'))\n",
    "\n",
    "        assert path.isfile(tmp_config_file)\n",
    "\n",
    "        updates_faq_config_file(configs_path=tmp_config_file, **args)\n",
    "\n",
    "        config_json = json.load(open(tmp_config_file))\n",
    "   \n",
    "        assert any(\n",
    "            rand_key in pipe_elem.keys() and new_rand_val in pipe_elem.values()\n",
    "            for pipe_elem in config_json['chainer']['pipe']\n",
    "        )\n",
    "\n",
    "\n",
    "test_updates_faq_config_file_update_string()\n",
    "test_updates_faq_config_file_update_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def select_faq_responses(faq_model, question):\n",
    "    '''Calls Deeppavlov FAQ model\n",
    "    '''\n",
    "    return faq_model([question])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "09:09:15 DEBUG:Starting new HTTP connection (1): files.deeppavlov.ai:80\n",
      "09:09:15 DEBUG:http://files.deeppavlov.ai:80 \"GET /faq/mipt/en_mipt_faq_v4.tar.gz.md5 HTTP/1.1\" 200 189\n",
      "09:09:15 DEBUG:Starting new HTTP connection (1): files.deeppavlov.ai:80\n",
      "09:09:16 DEBUG:http://files.deeppavlov.ai:80 \"GET /faq/mipt/en_mipt_faq_v4.tar.gz HTTP/1.1\" 200 12276\n",
      "100%|██████████| 12.3k/12.3k [00:00<00:00, 6.87MB/s]\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "09:09:17 DEBUG:Starting new HTTP connection (1): files.deeppavlov.ai:80\n",
      "09:09:18 DEBUG:http://files.deeppavlov.ai:80 \"GET /faq/mipt/en_mipt_faq_v4.tar.gz.md5 HTTP/1.1\" 200 189\n",
      "09:09:18 DEBUG:Starting new HTTP connection (1): files.deeppavlov.ai:80\n",
      "09:09:18 DEBUG:http://files.deeppavlov.ai:80 \"GET /faq/mipt/en_mipt_faq_v4.tar.gz HTTP/1.1\" 200 12276\n",
      "100%|██████████| 12.3k/12.3k [00:00<00:00, 5.89MB/s]\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "09:09:21 DEBUG:Starting new HTTP connection (1): files.deeppavlov.ai:80\n",
      "09:09:21 DEBUG:http://files.deeppavlov.ai:80 \"GET /faq/mipt/en_mipt_faq_v4.tar.gz.md5 HTTP/1.1\" 200 189\n",
      "09:09:21 DEBUG:Starting new HTTP connection (1): files.deeppavlov.ai:80\n",
      "09:09:22 DEBUG:http://files.deeppavlov.ai:80 \"GET /faq/mipt/en_mipt_faq_v4.tar.gz HTTP/1.1\" 200 12276\n",
      "100%|██████████| 12.3k/12.3k [00:00<00:00, 12.7MB/s]\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "#test faq responses\n",
    "import tempfile\n",
    "from shutil import copyfile\n",
    "\n",
    "\n",
    "def gen_mock_csv_file(tmpdirname, faqs):\n",
    "\n",
    "    temp_faq_csv = path.join(tmpdirname, 'tmp_faq.csv')\n",
    "\n",
    "    pd.DataFrame(faqs).to_csv(temp_faq_csv, index=False)\n",
    "\n",
    "    return temp_faq_csv\n",
    "\n",
    "\n",
    "def gen_mock_vocab_answers(tmpdirname, vocabs):\n",
    "\n",
    "    temp_dict_file = path.join(tmpdirname, 'temp_vocab_answers.dict')\n",
    "    vocabs_text = '\\n'.join(\n",
    "        t + '\\t' + str(f) for t, f in zip(vocabs['text'], vocabs['freq'])\n",
    "    )\n",
    "\n",
    "    f = open(temp_dict_file, 'w')\n",
    "    f.write(vocabs_text)\n",
    "    f.close()\n",
    "\n",
    "    return temp_dict_file\n",
    "\n",
    "\n",
    "def gen_faq_config(tmpdirname, vocab_file, faq_file):\n",
    "\n",
    "    temp_configs_faq = path.join(tmpdirname, 'temp_config_faq.json')\n",
    "    copyfile(configs.faq.tfidf_logreg_en_faq, temp_configs_faq)\n",
    "\n",
    "    changes_dict = {'save_path': vocab_file, 'load_path': vocab_file}\n",
    "    id_dict = {'id': 'answers_vocab'}\n",
    "\n",
    "    updates_faq_config_file(\n",
    "        configs_path=temp_configs_faq,\n",
    "        chainer={'pipe': [id_dict, changes_dict]},\n",
    "        dataset_reader={'data_path': faq_file}\n",
    "    )\n",
    "\n",
    "    return temp_configs_faq\n",
    "\n",
    "\n",
    "def test_faq_response_with_minimum_faqs_in_dataframe_fail_case():\n",
    "    with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "\n",
    "        faqs = {\n",
    "            'Question': ['Is Covid erradicated?'],\n",
    "            'Answer': ['Definitely not!']\n",
    "        }\n",
    "\n",
    "        vocabs = {'text': ['This is a vocab example'], 'freq': [1]}\n",
    "\n",
    "        faq_file = gen_mock_csv_file(tmpdirname, faqs)\n",
    "        vocab_file = gen_mock_vocab_answers(tmpdirname, vocabs)\n",
    "\n",
    "        configs_file = gen_faq_config(tmpdirname, vocab_file, faq_file)\n",
    "\n",
    "        try:\n",
    "            select_faq_responses(\n",
    "                question='Is Enrique the prettiest person in town?',\n",
    "                faq_model=train_model(configs_file, download=True)\n",
    "            )\n",
    "            assert False\n",
    "        except ValueError as e:\n",
    "            assert True\n",
    "\n",
    "\n",
    "def test_faq_response_with_minimum_faqs_in_dataframe_success_case():\n",
    "    with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "\n",
    "        faqs = {\n",
    "            'Question': ['Is Covid erradicated?', 'Who is the current POTUS?'],\n",
    "            'Answer': ['Definitely not!', 'Donald Trump']\n",
    "        }\n",
    "\n",
    "        vocabs = {'text': ['This is a vocab example'], 'freq': [1]}\n",
    "\n",
    "        faq_file = gen_mock_csv_file(tmpdirname, faqs)\n",
    "        vocab_file = gen_mock_vocab_answers(tmpdirname, vocabs)\n",
    "\n",
    "        configs_file = gen_faq_config(tmpdirname, vocab_file, faq_file)\n",
    "\n",
    "        assert select_faq_responses(\n",
    "            question='Is Enrique the prettiest person in town?',\n",
    "            faq_model=train_model(configs_file, download=True)\n",
    "        ) == ['Donald Trump']\n",
    "\n",
    "        \n",
    "        \n",
    "def test_faq_response_with_minimum_answers_vocab_success_case():\n",
    "    with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "\n",
    "        faqs = {\n",
    "            'Question': ['Is Covid erradicated?', 'Who is the current POTUS?'],\n",
    "            'Answer': ['Definitely not!', 'Donald Trump']\n",
    "        }\n",
    "\n",
    "        vocabs = {'text': [], 'freq': []}\n",
    "\n",
    "        faq_file = gen_mock_csv_file(tmpdirname, faqs)\n",
    "        vocab_file = gen_mock_vocab_answers(tmpdirname, vocabs)\n",
    "\n",
    "        configs_file = gen_faq_config(tmpdirname, vocab_file, faq_file)\n",
    "\n",
    "        select_faq_responses(\n",
    "            question='Is Enrique the prettiest person in town?',\n",
    "            faq_model=train_model(configs_file, download=True)\n",
    "        ) == ['Donald Trump']\n",
    "\n",
    "test_faq_response_with_minimum_faqs_in_dataframe_fail_case()\n",
    "test_faq_response_with_minimum_faqs_in_dataframe_success_case()\n",
    "test_faq_response_with_minimum_answers_vocab_success_case()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def select_squad_responses(\n",
    "    contexts, squad_model, question, best_results=1\n",
    "):\n",
    "    '''Calls Deeppavlov BERT and RNET Context Question Answering\n",
    "    '''\n",
    "    responses = contexts.context.apply(\n",
    "        lambda context: squad_model([context], [question])\n",
    "    ).values\n",
    "    \n",
    "    logging.debug(f'Responses: {responses}')\n",
    "    top_responses = [\n",
    "        r[0][0] for r in sorted(responses, key=lambda x: -1 * x[2][0])\n",
    "        [:best_results]\n",
    "    ]\n",
    "\n",
    "    logging.debug(f'Top Responses: {top_responses}')\n",
    "    return responses, top_responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "09:09:27 DEBUG:Starting new HTTP connection (1): files.deeppavlov.ai:80\n",
      "09:09:28 DEBUG:http://files.deeppavlov.ai:80 \"GET /deeppavlov_data/bert/cased_L-12_H-768_A-12.zip.md5 HTTP/1.1\" 200 386\n",
      "09:09:29 DEBUG:Starting new HTTP connection (1): files.deeppavlov.ai:80\n",
      "09:09:29 DEBUG:http://files.deeppavlov.ai:80 \"GET /deeppavlov_data/squad_bert.tar.gz.md5 HTTP/1.1\" 200 184\n",
      "09:09:46 INFO:Restoring parameters from /home/jovyan/.deeppavlov/models/squad_bert/model\n",
      "09:09:47 DEBUG:Responses: []\n",
      "09:09:47 DEBUG:Top Responses: []\n",
      "09:09:49 DEBUG:Responses: [list([['Elon Musk'], [203], [50257280.0]])]\n",
      "09:09:49 DEBUG:Top Responses: ['Elon Musk']\n",
      "09:09:50 DEBUG:Responses: [list([['TJ'], [44], [6978.86279296875]])\n",
      " list([['north of mexico'], [22], [81567.328125]])]\n",
      "09:09:50 DEBUG:Top Responses: ['north of mexico', 'TJ']\n",
      "09:09:50 DEBUG:Starting new HTTP connection (1): files.deeppavlov.ai:80\n",
      "09:09:51 DEBUG:http://files.deeppavlov.ai:80 \"GET /embeddings/wiki-news-300d-1M-char.vec.md5 HTTP/1.1\" 200 61\n",
      "09:09:51 DEBUG:Starting new HTTP connection (1): files.deeppavlov.ai:80\n",
      "09:09:51 DEBUG:http://files.deeppavlov.ai:80 \"GET /embeddings/wiki-news-300d-1M.vec.md5 HTTP/1.1\" 200 56\n",
      "09:09:55 DEBUG:Starting new HTTP connection (1): files.deeppavlov.ai:80\n",
      "09:09:56 DEBUG:http://files.deeppavlov.ai:80 \"GET /deeppavlov_data/squad_model_1.4_cpu_compatible.tar.gz.md5 HTTP/1.1\" 200 389\n",
      "09:10:15 INFO:Restoring parameters from /home/jovyan/.deeppavlov/models/squad_model/model\n",
      "09:10:16 DEBUG:Responses: []\n",
      "09:10:16 DEBUG:Top Responses: []\n",
      "09:10:18 DEBUG:Responses: [list([['Elon Musk'], [203], [36056324.0]])]\n",
      "09:10:18 DEBUG:Top Responses: ['Elon Musk']\n",
      "09:10:18 DEBUG:Responses: [list([['TJ'], [44], [564.4996948242188]])\n",
      " list([['north of mexico'], [22], [138151.90625]])]\n",
      "09:10:18 DEBUG:Top Responses: ['north of mexico', 'TJ']\n"
     ]
    }
   ],
   "source": [
    "#test select_squad_responses\n",
    "import tempfile\n",
    "from shutil import copyfile\n",
    "\n",
    "empty = {'topic': [], 'context': []}\n",
    "spacex = {\n",
    "    'topic': ['SpaceX'],\n",
    "    'context':\n",
    "        [\n",
    "            '''Space Exploration Technologies Corp., trading as SpaceX, is an American aerospace manufacturer and space transportation\n",
    "services company headquartered in Hawthorne, California. It was founded in 2002 by Elon Musk with the goal of reducing space \n",
    "transportation costs to enable the colonization of Mars. SpaceX has developed several launch vehicles, the Starlink satellite\n",
    "constellation, and the Dragon spacecraft. It is widely considered among the most successful private spaceflight companies.'''\n",
    "        ]\n",
    "}\n",
    "\n",
    "intekglobal = {\n",
    "    'topic': ['Intekglobal', 'InG'],\n",
    "    'context':\n",
    "        [\n",
    "            'Intekglobal has its headquarters located in TJ',\n",
    "            'Intekglobal is in the north of mexico'\n",
    "        ]\n",
    "}\n",
    "\n",
    "\n",
    "def assert_squad_model(\n",
    "    contexts, squad_model, question, expected_responses, **args\n",
    "):\n",
    "    responses, top_responses = select_squad_responses(\n",
    "        contexts=pd.DataFrame(contexts),\n",
    "        squad_model=squad_model,\n",
    "        question=question,\n",
    "        **args\n",
    "    )\n",
    "    assert top_responses == expected_responses\n",
    "\n",
    "\n",
    "def test_squad_bert():\n",
    "\n",
    "    bert = build_model(configs.squad.squad_bert, download=True)\n",
    "\n",
    "    assert_squad_model(\n",
    "        empty,\n",
    "        bert,\n",
    "        'Is an empty response expected?',\n",
    "        expected_responses=[],\n",
    "        best_results=2\n",
    "    )\n",
    "\n",
    "    assert_squad_model(\n",
    "        spacex, bert, 'Who founded SpaceX?', expected_responses=['Elon Musk']\n",
    "    )\n",
    "\n",
    "    assert_squad_model(\n",
    "        intekglobal,\n",
    "        bert,\n",
    "        'Where is Intekglobal located?',\n",
    "        expected_responses=['north of mexico','TJ'],\n",
    "        best_results=2\n",
    "    )\n",
    "\n",
    "\n",
    "def test_squad_rnet():\n",
    "\n",
    "    bert = build_model(configs.squad.squad, download=True)\n",
    "\n",
    "    assert_squad_model(\n",
    "        empty,\n",
    "        bert,\n",
    "        'Is an empty response expected?',\n",
    "        expected_responses=[],\n",
    "        best_results=5\n",
    "    )\n",
    "\n",
    "    assert_squad_model(\n",
    "        spacex, bert, 'Who founded SpaceX?', expected_responses=['Elon Musk']\n",
    "    )\n",
    "\n",
    "    assert_squad_model(\n",
    "        intekglobal,\n",
    "        bert,\n",
    "        'Where is Intekglobal located?',\n",
    "        expected_responses=['north of mexico','TJ'],\n",
    "        best_results=2\n",
    "    )\n",
    "\n",
    "test_squad_bert()\n",
    "test_squad_rnet()\n",
    "del spacex, empty, intekglobal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def load_qa_models(\n",
    "    config_rnet=configs.squad.squad,\n",
    "    config_bert=configs.squad.squad_bert,\n",
    "    config_tfidf=configs.faq.tfidf_logreg_en_faq,\n",
    "    download=True\n",
    "):\n",
    "    ''' Load the squad and faq models\n",
    "    '''\n",
    "    qa_models = {\n",
    "        'squad':\n",
    "            {\n",
    "                'rnet': build_model(config_rnet, download=download),\n",
    "                'bert': build_model(config_bert, download=download)\n",
    "            },\n",
    "        'faq': {\n",
    "            'tfidf': train_model(config_tfidf, download=download)\n",
    "        }\n",
    "    }\n",
    "    return qa_models\n",
    "\n",
    "\n",
    "def format_responses(question, responses):\n",
    "    '''Format question-response pair\n",
    "    '''\n",
    "    formatted_response = f'{question}:\\n\\n'\n",
    "    for k, res in enumerate(responses):\n",
    "        formatted_response += f'{k+1}: {res}\\n'\n",
    "    return formatted_response\n",
    "\n",
    "\n",
    "def get_responses(contexts, question, qa_models, nb_squad_results=1):\n",
    "    ''' Get response from a quesiton using qa_models and contexts\n",
    "    '''\n",
    "    responses = []\n",
    "    for squad_model in qa_models['squad'].values():\n",
    "        responses.extend(\n",
    "            select_squad_responses(\n",
    "                contexts, squad_model, question, best_results=nb_squad_results\n",
    "            )[1]\n",
    "        )\n",
    "    for faq_model in qa_models['faq'].values():\n",
    "        responses.extend(select_faq_responses(faq_model, question))\n",
    "    return responses, format_responses(\n",
    "        question, set([r for r in responses if r.strip()])\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "09:10:40 INFO:Restoring parameters from /home/jovyan/.deeppavlov/models/squad_model/model\n",
      "09:10:59 INFO:Restoring parameters from /home/jovyan/.deeppavlov/models/squad_bert/model\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "09:11:04 DEBUG:Responses: [list([['TJ'], [44], [120.95974731445312]])\n",
      " list([['north of mexico'], [22], [174602.40625]])]\n",
      "09:11:04 DEBUG:Top Responses: ['north of mexico', 'TJ']\n",
      "09:11:07 DEBUG:Responses: [list([['TJ'], [44], [22507.34375]])\n",
      " list([['north of mexico'], [22], [269778.3125]])]\n",
      "09:11:07 DEBUG:Top Responses: ['north of mexico', 'TJ']\n",
      "09:11:07 DEBUG:['north of mexico', 'TJ', 'north of mexico', 'TJ', 'Yes it is!']\n",
      "09:11:07 DEBUG:Where is Intekglobal?:\n",
      "\n",
      "1: north of mexico\n",
      "2: TJ\n",
      "3: Yes it is!\n",
      "\n",
      "09:11:30 INFO:Restoring parameters from /home/jovyan/.deeppavlov/models/squad_model/model\n",
      "09:11:59 INFO:Restoring parameters from /home/jovyan/.deeppavlov/models/squad_bert/model\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "09:12:06 DEBUG:Responses: []\n",
      "09:12:06 DEBUG:Top Responses: []\n",
      "09:12:06 DEBUG:Responses: []\n",
      "09:12:06 DEBUG:Top Responses: []\n",
      "09:12:06 DEBUG:['Two']\n",
      "09:12:06 DEBUG:What is the minimun number of FAQ questions:\n",
      "\n",
      "1: Two\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# test get_responses\n",
    "import tempfile\n",
    "from shutil import copyfile\n",
    "\n",
    "intekglobal_context = {\n",
    "    'topic': ['Intekglobal', 'InG'],\n",
    "    'context':\n",
    "        [\n",
    "            'Intekglobal has its headquarters located in TJ',\n",
    "            'Intekglobal is in the north of mexico'\n",
    "        ]\n",
    "}\n",
    "\n",
    "intekglobal_faqs = {\n",
    "    'Question': ['Is Intekglobal an IT company?', 'Where can I apply?'],\n",
    "    'Answer':\n",
    "        ['Yes it is!', 'Please refer the our website for further information']\n",
    "}\n",
    "\n",
    "\n",
    "def mock_faq_files(tmpdirname, faqs):\n",
    "\n",
    "    faq_files = {\n",
    "        'data': path.join(tmpdirname, 'temp_faq.csv'),\n",
    "        'config': path.join(tmpdirname, 'temp_config_faq.json')\n",
    "    }\n",
    "\n",
    "    pd.DataFrame(faqs).to_csv(faq_files['data'], index=False)\n",
    "    copyfile(configs.faq.tfidf_logreg_en_faq, faq_files['config'])\n",
    "\n",
    "    updates_faq_config_file(\n",
    "        configs_path=faq_files['config'],\n",
    "        dataset_reader={'data_path': faq_files['data']}\n",
    "    )\n",
    "\n",
    "    return faq_files\n",
    "\n",
    "\n",
    "def test_get_intekglobal_responses():\n",
    "    with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "\n",
    "        faq_files = mock_faq_files(tmpdirname, intekglobal_faqs)\n",
    "        qa_models = load_qa_models(\n",
    "            config_tfidf=faq_files['config'], download=False\n",
    "        )\n",
    "\n",
    "        responses, format_responses = get_responses(\n",
    "            pd.DataFrame(intekglobal_context),\n",
    "            'Where is Intekglobal?',\n",
    "            qa_models,\n",
    "            nb_squad_results=2\n",
    "        )\n",
    "\n",
    "        logging.debug(responses)\n",
    "        logging.debug(format_responses)\n",
    "        assert all(\n",
    "            response in ('north of mexico', 'TJ', 'Yes it is!')\n",
    "            for response in responses\n",
    "        )\n",
    "\n",
    "\n",
    "def test_get_responses_with_empty_context():\n",
    "    with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "        min_faqs = {'Question': ['Minimum number of questions?','This is the other question?'], 'Answer': ['Two','yes']}\n",
    "        faq_files = mock_faq_files(tmpdirname, min_faqs)\n",
    "\n",
    "        qa_models = load_qa_models(\n",
    "            config_tfidf=faq_files['config'], download=False\n",
    "        )\n",
    "        empty_context = {'topic': [], 'context': []}\n",
    "\n",
    "        responses, format_responses = get_responses(\n",
    "            pd.DataFrame(empty_context),\n",
    "            'What is the minimun number of FAQ questions',\n",
    "            qa_models,\n",
    "            nb_squad_results=2\n",
    "        )\n",
    "\n",
    "        logging.debug(responses)\n",
    "        logging.debug(format_responses)\n",
    "        assert responses == ['Two']\n",
    "\n",
    "\n",
    "test_get_intekglobal_responses()\n",
    "test_get_responses_with_empty_context()\n",
    "\n",
    "del intekglobal_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_input(text):\n",
    "    '''This redundancy is needed for testing'''\n",
    "    return input(text)\n",
    "\n",
    "\n",
    "def question_response(data, qa_models, num_returned_values_per_squad_model=1):\n",
    "    ''' Receive response and call get_response()\n",
    "    '''\n",
    "    question = get_input('Introduce question:\\n')\n",
    "\n",
    "    _, formatted_responses = get_responses(\n",
    "        data['context']['df'], question, qa_models, nb_squad_results=1\n",
    "    )\n",
    "    \n",
    "    return question, formatted_responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Test FAQ dialog system's part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "09:12:37 INFO:Restoring parameters from /home/jovyan/.deeppavlov/models/squad_model/model\n",
      "09:13:10 INFO:Restoring parameters from /home/jovyan/.deeppavlov/models/squad_bert/model\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "09:13:20 DEBUG:Responses: [list([['Intekglobal'], [0], [160.03579711914062]])\n",
      " list([['Intekglobal'], [3], [244.86245727539062]])\n",
      " list([['one of the smartest minds on the planet'], [19], [1664652.0]])]\n",
      "09:13:20 DEBUG:Top Responses: ['one of the smartest minds on the planet']\n",
      "09:13:24 DEBUG:Responses: [list([[''], [-1], [0.0011892060283571482]])\n",
      " list([[''], [-1], [0.01691678911447525]])\n",
      " list([['one of the smartest minds on the planet, \\n                   he currently works as Intekglobal employee'], [19], [18812.87109375]])]\n",
      "09:13:24 DEBUG:Top Responses: ['one of the smartest minds on the planet, \\n                   he currently works as Intekglobal employee']\n",
      "09:13:24 DEBUG:  Who is Enrique Jimenez?\n",
      "09:13:24 DEBUG:  Who is Enrique Jimenez?:\n",
      "\n",
      "1: one of the smartest minds on the planet, \n",
      "                   he currently works as Intekglobal employee\n",
      "2: one of the smartest minds on the planet\n",
      "3: yes\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tempfile\n",
    "from unittest.mock import patch\n",
    "from shutil import copyfile\n",
    "from collections import defaultdict\n",
    "\n",
    "def mock_faq_files(tmpdirname, faqs, faq_dic):\n",
    "\n",
    "    faq_dic['path'] = path.join(tmpdirname, 'temp_faq.csv')\n",
    "    faq_dic['config'] = path.join(tmpdirname, 'temp_config_faq.json')\n",
    "    faq_dic['df'] = pd.DataFrame(faqs)\n",
    "    faq_dic['df'].to_csv(faq_dic['path'], index=False)\n",
    "\n",
    "    copyfile(configs.faq.tfidf_logreg_en_faq, faq_dic['config'])\n",
    "\n",
    "    updates_faq_config_file(\n",
    "        configs_path=faq_dic['config'],\n",
    "        dataset_reader={'data_path': faq_dic['path']}\n",
    "    )\n",
    "\n",
    "\n",
    "def mock_context_file(tmpdirname, contexts, context_dic):\n",
    "\n",
    "    context_dic['path'] = path.join(tmpdirname, 'temp_context.csv')\n",
    "    context_dic['df'] = pd.DataFrame(contexts)\n",
    "    context_dic['df'].to_csv(context_dic['path'], index=False)\n",
    "\n",
    "\n",
    "@patch('__main__.get_input')\n",
    "def test_context_response_with_no_updates(mock_input):\n",
    "    mock_input.side_effect = ['Who is Enrique Jimenez?']\n",
    "    data = {'context': defaultdict(str), 'faq': defaultdict(str)}\n",
    "    contexts = {\n",
    "        'context':\n",
    "            [\n",
    "                'Intekglobal has its headquarters located in TJ',\n",
    "                'In Intekglobal we care about you',\n",
    "                '''Enrique Jimenez is one of the smartest minds on the planet, \n",
    "                   he currently works as Intekglobal employee'''\n",
    "            ],\n",
    "        'topic': ['headquarters', 'mission', 'Enrique\\'s biography']\n",
    "    }\n",
    "\n",
    "    faqs = {\n",
    "        'Question':\n",
    "            ['Minimum number of questions?', 'This is the other question?'],\n",
    "        'Answer': ['Two', 'yes']\n",
    "    }\n",
    "\n",
    "    with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "\n",
    "        mock_faq_files(tmpdirname, faqs, data['faq'])\n",
    "        mock_context_file(tmpdirname, contexts, data['context'])\n",
    "\n",
    "        qa_models = load_qa_models(\n",
    "            config_tfidf=data['faq']['config'], download=False\n",
    "        )\n",
    "\n",
    "        question,responses = question_response(data, qa_models)\n",
    "        logging.debug(f'  {question}')\n",
    "        logging.debug(f'  {responses}')\n",
    "        assert 'Who is Enrique Jimenez?' == question\n",
    "        assert 'one of the smartest minds on the planet' in responses\n",
    "\n",
    "test_context_response_with_no_updates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def new_question_answer(data, qa_models):\n",
    "    ''' Asks for a new question-answer pair; store the result in the \n",
    "        faq dataframe and retrain the faq-model\n",
    "    '''\n",
    "    \n",
    "    question = get_input('Introduce question:\\n')\n",
    "\n",
    "    new_faq = pd.DataFrame(\n",
    "        {\n",
    "            'Question': [question],\n",
    "            'Answer': [get_input('Introduce the answer:\\n')]\n",
    "        }\n",
    "    )\n",
    "    data['faq']['df'] = data['faq']['df'].append(new_faq)\n",
    "    data['faq']['df'].to_csv(data['faq']['path'], index=False)\n",
    "    qa_models['faq']['tfidf'] = train_model(\n",
    "        data['faq']['config'], download=False\n",
    "    )\n",
    "    logging.info('FAQ dataset and model updated..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "09:13:50 INFO:Restoring parameters from /home/jovyan/.deeppavlov/models/squad_model/model\n",
      "09:14:23 INFO:Restoring parameters from /home/jovyan/.deeppavlov/models/squad_bert/model\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "09:14:35 INFO:FAQ dataset and model updated..\n"
     ]
    }
   ],
   "source": [
    "#tests\n",
    "\n",
    "\n",
    "@patch('__main__.get_input')\n",
    "def test_new_question_answer(mock_input):\n",
    "    question = 'What is Intekglobal?'\n",
    "    new_answer = 'Intekglobal is one of the best companies in the world'\n",
    "    mock_input.side_effect = [question, new_answer]\n",
    "\n",
    "    data = {'context': defaultdict(str), 'faq': defaultdict(str)}\n",
    "\n",
    "    faqs = {\n",
    "        'Question': ['Who  owns Tesla Company?', 'Is this is heaven?'],\n",
    "        'Answer': [\n",
    "            'Elon Musk is the owner of Tesla', 'No, it is life on earth'\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "\n",
    "        mock_faq_files(tmpdirname, faqs, data['faq'])\n",
    "        qa_models = load_qa_models(\n",
    "            config_tfidf=data['faq']['config'], download=False\n",
    "        )\n",
    "        new_question_answer(data, qa_models)\n",
    "        updated_faq = pd.read_csv(data['faq']['path'])\n",
    "\n",
    "        assert updated_faq[updated_faq['Answer'] == new_answer].shape[0] == 1\n",
    "\n",
    "\n",
    "test_new_question_answer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "def new_context(data):\n",
    "    ''' Stores the new context in the context dataframe\n",
    "    '''\n",
    "\n",
    "    new_context = pd.DataFrame(\n",
    "        {\n",
    "            'topic': [get_input('Give context a title:\\n')],\n",
    "            'context': [get_input('Introduce the context:\\n')]\n",
    "        }\n",
    "    )\n",
    "    data['context']['df'] = data['context']['df'].append(new_context)\n",
    "    data['context']['df'].to_csv(data['context']['path'], index=False)\n",
    "\n",
    "    logging.info('contexts dataset updated..')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "09:14:35 DEBUG:<function new_context at 0x7f5d18456680>\n",
      "/opt/conda/lib/python3.7/site-packages/pandas/core/frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "09:14:35 INFO:contexts dataset updated..\n"
     ]
    }
   ],
   "source": [
    "@patch('__main__.get_input')\n",
    "def test_new_context(mock_input):\n",
    "    data = {'context': defaultdict(str), 'faq': defaultdict(str)}\n",
    "\n",
    "    new_topic = 'AI Tool & Chatbot Development'\n",
    "    new_context_str = '''\n",
    "\n",
    "A chatbot is an important tool for simulating intelligent conversations with humans.\n",
    "Intekglobal chatbots efficiently live message on platforms such as Facebook Messenger, \n",
    "Slack, and Telegram. But chatbots are more than just a cool technology advancement.\n",
    "\n",
    "'''\n",
    "    contexts = {\n",
    "        'context':\n",
    "            [\n",
    "                '''One of the greatest punk rock bands from all the time\n",
    "                is the Ramones.\n",
    "                '''\n",
    "            ],\n",
    "        'topic': ['Ramones']\n",
    "    }\n",
    "    \n",
    "    mock_input.side_effect = [new_topic, new_context_str]\n",
    "    with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "\n",
    "        \n",
    "        logging.debug(str(new_context))\n",
    "        mock_context_file(tmpdirname, contexts, data['context'])\n",
    "        new_context(data)\n",
    "        updated_faq = pd.read_csv(data['context']['path'])\n",
    "\n",
    "        assert updated_faq[updated_faq.topic == new_topic].shape[0] == 1\n",
    "\n",
    "\n",
    "test_new_context()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def set_minimal_faq_questions(data):\n",
    "    ''' Sets the faq configurations that assure a proper operation\n",
    "    '''\n",
    "    if data['df'].shape[0] > 1:\n",
    "        return\n",
    "    minimal_questions = [\n",
    "        'Is this the Intekglobal Dialog System?',\n",
    "        'What is the purpose of these two automated questions?'\n",
    "    ]\n",
    "    minimal_answers = [\n",
    "        'This is the Intekglobal Dialog System', 'To populate the FAQ data file'\n",
    "    ]\n",
    "    minimal_faqs_df = pd.DataFrame(\n",
    "        {\n",
    "            'Question': minimal_questions,\n",
    "            'Answer': minimal_answers\n",
    "        }\n",
    "    )\n",
    "    data['df'] = pd.concat([data['df'], minimal_faqs_df])\n",
    "    data['df'].to_csv(data['path'], index=False)\n",
    "    logging.info(f' File created at {data[\"path\"]}')\n",
    "\n",
    "\n",
    "def set_minimal_contexts(data):\n",
    "    ''' Sets the context configurations that assure a proper operation\n",
    "    '''\n",
    "    if data['df'].shape[0] > 0:\n",
    "        return\n",
    "\n",
    "    minimal_context_df = pd.DataFrame({'topic': [], 'context': []})\n",
    "    data['df'] = minimal_context_df\n",
    "    data['df'].to_csv(data['path'], index=False)\n",
    "\n",
    "    logging.info(f' File created at {data[\"path\"]}')\n",
    "\n",
    "\n",
    "def set_data_dict(file, data, question_type, data_dir):\n",
    "    '''Creates unexistent files\n",
    "    '''\n",
    "\n",
    "    data['path'] = file if file is not None else path.join(\n",
    "        data_dir, question_type + '_data.csv'\n",
    "    )\n",
    "\n",
    "    data['df'] = pd.read_csv(data['path']) if path.isfile(data['path']\n",
    "                                                         ) else pd.DataFrame()\n",
    "\n",
    "    if question_type == 'faq':\n",
    "        set_minimal_faq_questions(data)\n",
    "    if question_type == 'context':\n",
    "        set_minimal_contexts(data)\n",
    "\n",
    "\n",
    "def load_and_prepare_data(context_data_file, faq_data_file, data, configs_faq):\n",
    "    '''Calls the context and faq configuration routines\n",
    "    '''\n",
    "\n",
    "    PARENT_DIR = popen('$PWD').read().strip()\n",
    "\n",
    "    if faq_data_file or context_data_file is None:\n",
    "        DATA_DIR = path.join(PARENT_DIR, 'data')\n",
    "\n",
    "        if not path.isdir(DATA_DIR):\n",
    "            mkdir(DATA_DIR)\n",
    "            logging.info(f'Data directory created at {DATA_DIR}')\n",
    "\n",
    "    if configs_faq is None:\n",
    "        configs_faq = configs.faq.tfidf_logreg_en_faq\n",
    "\n",
    "    data['faq']['config'] = configs_faq\n",
    "\n",
    "    set_data_dict(\n",
    "        file=faq_data_file,\n",
    "        data=data['faq'],\n",
    "        question_type='faq',\n",
    "        data_dir=DATA_DIR\n",
    "    )\n",
    "    set_data_dict(\n",
    "        file=context_data_file,\n",
    "        data=data['context'],\n",
    "        question_type='context',\n",
    "        data_dir=DATA_DIR\n",
    "    )\n",
    "\n",
    "    updates_faq_config_file(\n",
    "        configs_path=configs_faq,\n",
    "        dataset_reader={'data_path': data['faq']['path']}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "09:14:35 INFO: File created at /tmp/tmpe1fzbn5r/tmp_data.csv\n",
      "09:14:35 INFO: File created at /tmp/tmpi4mgn_tl/tmp_data.csv\n",
      "09:14:35 INFO: File created at /tmp/tmp006qpt5n/context_data.csv\n",
      "09:14:35 DEBUG:{'context': defaultdict(<class 'str'>, {'path': '/tmp/tmp006qpt5n/context_data.csv', 'df': Empty DataFrame\n",
      "Columns: [topic, context]\n",
      "Index: []})}\n",
      "09:14:35 INFO:Data directory created at /tmp/tmpn3sct0bz/data\n",
      "09:14:35 INFO: File created at /tmp/tmpn3sct0bz/data/faq_data.csv\n",
      "09:14:35 INFO: File created at /tmp/tmpn3sct0bz/data/context_data.csv\n"
     ]
    }
   ],
   "source": [
    "#tests\n",
    "import tempfile,logging\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from shutil import rmtree\n",
    "from os import path,popen\n",
    "from unittest.mock import patch\n",
    "#from let_me_answer_for_you.settings import *\n",
    "\n",
    "\n",
    "def test_set_minimal_faqs_with_more_than_one_question():\n",
    "    with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "        data_file = path.join(tmpdirname, 'tmp_data.csv')\n",
    "        questions = ['a?', 'b?']\n",
    "        answers = ['a', 'b']\n",
    "        df = pd.DataFrame({'Question': questions, 'Answer': answers})\n",
    "        df.to_csv(data_file, index=False)\n",
    "        data = {'df': df, 'path': data_file}\n",
    "        set_minimal_faq_questions(data)\n",
    "\n",
    "        assert data['df'].shape[0] == 2\n",
    "\n",
    "\n",
    "def test_set_minimal_faqs_with_less_than_two_questions():\n",
    "    with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "        data_file = path.join(tmpdirname, 'tmp_data.csv')\n",
    "        questions = ['a?']\n",
    "        answers = ['a']\n",
    "        df = pd.DataFrame({'Question': questions, 'Answer': answers})\n",
    "        df.to_csv(data_file, index=False)\n",
    "        data = {'df': df, 'path': data_file}\n",
    "\n",
    "        assert data['df'].shape[0] == 1\n",
    "\n",
    "        set_minimal_faq_questions(data)\n",
    "\n",
    "        assert data['df'].shape[0] == 3\n",
    "        assert any(\n",
    "            data['df'].Question == 'Is this the Intekglobal Dialog System?'\n",
    "        )\n",
    "\n",
    "\n",
    "def test_set_minimal_contexts():\n",
    "    with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "        data_file = path.join(tmpdirname, 'tmp_data.csv')\n",
    "        data = {'df': pd.DataFrame(), 'path': data_file}\n",
    "        set_minimal_contexts(data)\n",
    "        assert path.isfile(data['path'])\n",
    "        assert all(data['df'].columns == ['topic', 'context'])\n",
    "\n",
    "\n",
    "def test_set_data_dict_no_file():\n",
    "    with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "        data = {'context': defaultdict(str)}\n",
    "        set_data_dict(\n",
    "            file=None,\n",
    "            data=data['context'],\n",
    "            data_dir=tmpdirname,\n",
    "            question_type='context'\n",
    "        )\n",
    "        logging.debug(data)\n",
    "        assert path.isfile(data['context']['path'])\n",
    "\n",
    "\n",
    "@patch('__main__.popen')\n",
    "def test_load_and_prepare_data(mock_popen):\n",
    "    with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "        mock_popen(\"$PWD\").read().strip.side_effect = [tmpdirname]\n",
    "        data = {'context': defaultdict(str), 'faq': defaultdict(str)}\n",
    "\n",
    "        load_and_prepare_data(\n",
    "            context_data_file=None,\n",
    "            faq_data_file=None,\n",
    "            data=data,\n",
    "            configs_faq=None\n",
    "        )\n",
    "        data_dir = path.join(tmpdirname, 'data')\n",
    "        #config_file = path.join(tmpdirname,'config_faq.json')\n",
    "        assert path.isdir(data_dir)\n",
    "\n",
    "\n",
    "test_set_minimal_faqs_with_more_than_one_question()\n",
    "test_set_minimal_faqs_with_less_than_two_questions()\n",
    "test_set_minimal_contexts()\n",
    "test_set_data_dict_no_file()\n",
    "test_load_and_prepare_data()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
