{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "!pip install -r ../requirements.txt > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "from deeppavlov.core.common.paths import get_settings_path\n",
    "from deeppavlov import configs, build_model, train_model\n",
    "import json\n",
    "from os import path, popen, mkdir\n",
    "from shutil import copyfile\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from pathlib import Path\n",
    "import logging\n",
    "from collections import defaultdict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "09:56:32 INFO:Hello! Welcome to our automated dialog system!\n",
      "09:56:32 DEBUG: Debug Log Active\n",
      "09:56:32 WARNING: Warning Log Active\n",
      "09:56:32 ERROR: Error Log Active \n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "logging.basicConfig(\n",
    "    #filename='example.log',\n",
    "    format='%(asctime)s %(levelname)s:%(message)s',\n",
    "    level=logging.DEBUG,\n",
    "    datefmt='%I:%M:%S'\n",
    ")\n",
    "\n",
    "logging.info(\"Hello! Welcome to our automated dialog system!\")\n",
    "logging.debug(\" Debug Log Active\")\n",
    "logging.warning(' Warning Log Active')\n",
    "logging.error(' Error Log Active ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Settings and Configuration\n",
    "> Question Answering Automated Dialog System "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def change_log_config():\n",
    "    '''Change Deeppavlov configuration files to ERROR mode\n",
    "    '''\n",
    "    settings_file = path.join(get_settings_path(), 'log_config.json')\n",
    "    #logs_key = 'disable_existing_loggers'\n",
    "\n",
    "    settings_json = json.load(open(settings_file))\n",
    "    settings_json['handlers']['file']['level'] = 'ERROR'\n",
    "    settings_json['handlers']['stderr']['level'] = 'ERROR'\n",
    "    settings_json['handlers']['stdout']['level'] = 'ERROR'\n",
    "    settings_json['handlers']['uvicorn_handler']['level'] = 'ERROR'\n",
    "\n",
    "    settings_json['loggers']['deeppavlov']['level'] = 'ERROR'\n",
    "    settings_json['loggers']['deeppavlov']['propagate'] = True\n",
    "\n",
    "    settings_json['loggers']['uvicorn.access']['level'] = 'ERROR'\n",
    "    settings_json['loggers']['uvicorn.access']['propagate'] = True\n",
    "\n",
    "    settings_json['loggers']['uvicorn.error']['level'] = 'ERROR'\n",
    "    settings_json['loggers']['uvicorn.error']['propagate'] = True\n",
    "\n",
    "\n",
    "    json.dump(settings_json, open(settings_file, 'w'))\n",
    "\n",
    "\n",
    "def run_shell_installs():\n",
    "    ''' Run install commands\n",
    "    '''\n",
    "    logging.info(f'..Installing NLP libraries')\n",
    "    change_log_config()\n",
    "\n",
    "    command_strings = (\n",
    "        ' pip install deeppavlov', ' python -m deeppavlov install squad',\n",
    "        ' python -m deeppavlov install squad_bert',\n",
    "        ' python -m deeppavlov install fasttext_avg_autofaq',\n",
    "        ' python -m deeppavlov install fasttext_tfidf_autofaq',\n",
    "        ' python -m deeppavlov install tfidf_autofaq',\n",
    "        ' python -m deeppavlov install tfidf_logreg_autofaq ',\n",
    "        ' python -m deeppavlov install tfidf_logreg_en_faq'\n",
    "    )\n",
    "    for command in command_strings:\n",
    "\n",
    "        logging.debug(command)\n",
    "        logging.debug(popen(command).read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "09:56:32 INFO:..Installing NLP libraries\n",
      "09:56:32 DEBUG: pip install deeppavlov\n",
      "09:56:33 DEBUG:Requirement already satisfied: deeppavlov in /opt/conda/lib/python3.7/site-packages (0.11.0)\n",
      "Requirement already satisfied: uvicorn==0.11.1 in /opt/conda/lib/python3.7/site-packages (from deeppavlov) (0.11.1)\n",
      "Requirement already satisfied: pydantic==1.3 in /opt/conda/lib/python3.7/site-packages (from deeppavlov) (1.3)\n",
      "Requirement already satisfied: nltk==3.4.5 in /opt/conda/lib/python3.7/site-packages (from deeppavlov) (3.4.5)\n",
      "Requirement already satisfied: rusenttokenize==0.0.5 in /opt/conda/lib/python3.7/site-packages (from deeppavlov) (0.0.5)\n",
      "Requirement already satisfied: fastapi==0.47.1 in /opt/conda/lib/python3.7/site-packages (from deeppavlov) (0.47.1)\n",
      "Requirement already satisfied: tqdm==4.41.1 in /opt/conda/lib/python3.7/site-packages (from deeppavlov) (4.41.1)\n",
      "Requirement already satisfied: aio-pika==6.4.1 in /opt/conda/lib/python3.7/site-packages (from deeppavlov) (6.4.1)\n",
      "Requirement already satisfied: pyopenssl==19.1.0 in /opt/conda/lib/python3.7/site-packages (from deeppavlov) (19.1.0)\n",
      "Requirement already satisfied: pytelegrambotapi==3.6.7 in /opt/conda/lib/python3.7/site-packages (from deeppavlov) (3.6.7)\n",
      "Requirement already satisfied: scipy==1.4.1 in /opt/conda/lib/python3.7/site-packages (from deeppavlov) (1.4.1)\n",
      "Requirement already satisfied: overrides==2.7.0 in /opt/conda/lib/python3.7/site-packages (from deeppavlov) (2.7.0)\n",
      "Requirement already satisfied: sacremoses==0.0.35 in /opt/conda/lib/python3.7/site-packages (from deeppavlov) (0.0.35)\n",
      "Requirement already satisfied: scikit-learn==0.21.2 in /opt/conda/lib/python3.7/site-packages (from deeppavlov) (0.21.2)\n",
      "Requirement already satisfied: pandas==0.25.3 in /opt/conda/lib/python3.7/site-packages (from deeppavlov) (0.25.3)\n",
      "Requirement already satisfied: ruamel.yaml==0.15.100 in /opt/conda/lib/python3.7/site-packages (from deeppavlov) (0.15.100)\n",
      "Requirement already satisfied: numpy==1.18.0 in /opt/conda/lib/python3.7/site-packages (from deeppavlov) (1.18.0)\n",
      "Requirement already satisfied: requests==2.22.0 in /opt/conda/lib/python3.7/site-packages (from deeppavlov) (2.22.0)\n",
      "Requirement already satisfied: pymorphy2==0.8 in /opt/conda/lib/python3.7/site-packages (from deeppavlov) (0.8)\n",
      "Requirement already satisfied: pytz==2019.1 in /opt/conda/lib/python3.7/site-packages (from deeppavlov) (2019.1)\n",
      "Requirement already satisfied: h5py==2.10.0 in /opt/conda/lib/python3.7/site-packages (from deeppavlov) (2.10.0)\n",
      "Requirement already satisfied: click==7.1.2 in /opt/conda/lib/python3.7/site-packages (from deeppavlov) (7.1.2)\n",
      "Requirement already satisfied: pymorphy2-dicts-ru in /opt/conda/lib/python3.7/site-packages (from deeppavlov) (2.4.404381.4453942)\n",
      "Requirement already satisfied: Cython==0.29.14 in /opt/conda/lib/python3.7/site-packages (from deeppavlov) (0.29.14)\n",
      "Requirement already satisfied: httptools==0.0.13; sys_platform != \"win32\" and sys_platform != \"cygwin\" and platform_python_implementation != \"pypy\" in /opt/conda/lib/python3.7/site-packages (from uvicorn==0.11.1->deeppavlov) (0.0.13)\n",
      "Requirement already satisfied: h11<0.10,>=0.8 in /opt/conda/lib/python3.7/site-packages (from uvicorn==0.11.1->deeppavlov) (0.9.0)\n",
      "Requirement already satisfied: uvloop>=0.14.0; sys_platform != \"win32\" and sys_platform != \"cygwin\" and platform_python_implementation != \"pypy\" in /opt/conda/lib/python3.7/site-packages (from uvicorn==0.11.1->deeppavlov) (0.14.0)\n",
      "Requirement already satisfied: websockets==8.* in /opt/conda/lib/python3.7/site-packages (from uvicorn==0.11.1->deeppavlov) (8.1)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from nltk==3.4.5->deeppavlov) (1.14.0)\n",
      "Requirement already satisfied: starlette<=0.12.9,>=0.12.9 in /opt/conda/lib/python3.7/site-packages (from fastapi==0.47.1->deeppavlov) (0.12.9)\n",
      "Requirement already satisfied: yarl in /opt/conda/lib/python3.7/site-packages (from aio-pika==6.4.1->deeppavlov) (1.4.2)\n",
      "Requirement already satisfied: aiormq<4,>=3.2.0 in /opt/conda/lib/python3.7/site-packages (from aio-pika==6.4.1->deeppavlov) (3.2.2)\n",
      "Requirement already satisfied: cryptography>=2.8 in /opt/conda/lib/python3.7/site-packages (from pyopenssl==19.1.0->deeppavlov) (2.8)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from sacremoses==0.0.35->deeppavlov) (0.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /opt/conda/lib/python3.7/site-packages (from pandas==0.25.3->deeppavlov) (2.8.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests==2.22.0->deeppavlov) (2019.11.28)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests==2.22.0->deeppavlov) (2.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests==2.22.0->deeppavlov) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests==2.22.0->deeppavlov) (1.25.7)\n",
      "Requirement already satisfied: dawg-python>=0.7 in /opt/conda/lib/python3.7/site-packages (from pymorphy2==0.8->deeppavlov) (0.7.2)\n",
      "Requirement already satisfied: pymorphy2-dicts<3.0,>=2.4 in /opt/conda/lib/python3.7/site-packages (from pymorphy2==0.8->deeppavlov) (2.4.393442.3710985)\n",
      "Requirement already satisfied: docopt>=0.6 in /opt/conda/lib/python3.7/site-packages (from pymorphy2==0.8->deeppavlov) (0.6.2)\n",
      "Requirement already satisfied: multidict>=4.0 in /opt/conda/lib/python3.7/site-packages (from yarl->aio-pika==6.4.1->deeppavlov) (4.7.6)\n",
      "Requirement already satisfied: pamqp==2.3.0 in /opt/conda/lib/python3.7/site-packages (from aiormq<4,>=3.2.0->aio-pika==6.4.1->deeppavlov) (2.3.0)\n",
      "Requirement already satisfied: cffi!=1.11.3,>=1.8 in /opt/conda/lib/python3.7/site-packages (from cryptography>=2.8->pyopenssl==19.1.0->deeppavlov) (1.14.0)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.7/site-packages (from cffi!=1.11.3,>=1.8->cryptography>=2.8->pyopenssl==19.1.0->deeppavlov) (2.20)\n",
      "\n",
      "09:56:33 DEBUG: python -m deeppavlov install squad\n",
      "09:56:36 DEBUG:Requirement already satisfied: tensorflow==1.15.2 in /opt/conda/lib/python3.7/site-packages (1.15.2)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.15.2) (1.1.2)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.15.2) (1.30.0)\n",
      "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.15.2) (0.34.2)\n",
      "Requirement already satisfied: tensorflow-estimator==1.15.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.15.2) (1.15.1)\n",
      "Requirement already satisfied: google-pasta>=0.1.6 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.15.2) (0.2.0)\n",
      "Requirement already satisfied: protobuf>=3.6.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.15.2) (3.12.2)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.15.2) (3.2.1)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.15.2) (1.1.0)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.15.2) (1.12.1)\n",
      "Requirement already satisfied: tensorboard<1.16.0,>=1.15.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.15.2) (1.15.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.16.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.15.2) (1.18.0)\n",
      "Requirement already satisfied: gast==0.2.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.15.2) (0.2.2)\n",
      "Requirement already satisfied: astor>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.15.2) (0.8.1)\n",
      "Requirement already satisfied: six>=1.10.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.15.2) (1.14.0)\n",
      "Requirement already satisfied: keras-applications>=1.0.8 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.15.2) (1.0.8)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.15.2) (0.9.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from protobuf>=3.6.1->tensorflow==1.15.2) (46.0.0.post20200311)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /opt/conda/lib/python3.7/site-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (1.0.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.7/site-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (3.2.2)\n",
      "Requirement already satisfied: h5py in /opt/conda/lib/python3.7/site-packages (from keras-applications>=1.0.8->tensorflow==1.15.2) (2.10.0)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /opt/conda/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (1.5.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (3.1.0)\n",
      "\n",
      "09:56:36 DEBUG: python -m deeppavlov install squad_bert\n",
      "09:56:43 DEBUG:Requirement already satisfied: tensorflow==1.15.2 in /opt/conda/lib/python3.7/site-packages (1.15.2)\n",
      "Requirement already satisfied: tensorflow-estimator==1.15.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.15.2) (1.15.1)\n",
      "Requirement already satisfied: numpy<2.0,>=1.16.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.15.2) (1.18.0)\n",
      "Requirement already satisfied: keras-applications>=1.0.8 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.15.2) (1.0.8)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.15.2) (1.30.0)\n",
      "Requirement already satisfied: tensorboard<1.16.0,>=1.15.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.15.2) (1.15.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.15.2) (3.2.1)\n",
      "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.15.2) (0.34.2)\n",
      "Requirement already satisfied: gast==0.2.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.15.2) (0.2.2)\n",
      "Requirement already satisfied: astor>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.15.2) (0.8.1)\n",
      "Requirement already satisfied: six>=1.10.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.15.2) (1.14.0)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.15.2) (0.9.0)\n",
      "Requirement already satisfied: protobuf>=3.6.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.15.2) (3.12.2)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.15.2) (1.1.2)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.15.2) (1.1.0)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.15.2) (1.12.1)\n",
      "Requirement already satisfied: google-pasta>=0.1.6 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.15.2) (0.2.0)\n",
      "Requirement already satisfied: h5py in /opt/conda/lib/python3.7/site-packages (from keras-applications>=1.0.8->tensorflow==1.15.2) (2.10.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (46.0.0.post20200311)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /opt/conda/lib/python3.7/site-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (1.0.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.7/site-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (3.2.2)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /opt/conda/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (1.5.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (3.1.0)\n",
      "Collecting git+https://github.com/deepmipt/bert.git@feat/multi_gpu\n",
      "  Cloning https://github.com/deepmipt/bert.git (to revision feat/multi_gpu) to /tmp/pip-req-build-ypc45dr9\n",
      "Requirement already satisfied (use --upgrade to upgrade): bert-dp==1.0 from git+https://github.com/deepmipt/bert.git@feat/multi_gpu in /opt/conda/lib/python3.7/site-packages\n",
      "Building wheels for collected packages: bert-dp\n",
      "  Building wheel for bert-dp (setup.py): started\n",
      "  Building wheel for bert-dp (setup.py): finished with status 'done'\n",
      "  Created wheel for bert-dp: filename=bert_dp-1.0-py3-none-any.whl size=23580 sha256=e2b36bbb821048fd74e13b4746f87d8b15e3ffd76578661bebe9879dbe696983\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-gzd6kh15/wheels/44/29/b2/ee614cb7f97ba5c2d220029eaede3af4b74331ad31d6e2f4eb\n",
      "Successfully built bert-dp\n",
      "\n",
      "09:56:43 DEBUG: python -m deeppavlov install fasttext_avg_autofaq\n",
      "09:56:46 DEBUG:Requirement already satisfied: fasttext==0.9.1 in /opt/conda/lib/python3.7/site-packages (0.9.1)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from fasttext==0.9.1) (1.18.0)\n",
      "Requirement already satisfied: pybind11>=2.2 in /opt/conda/lib/python3.7/site-packages (from fasttext==0.9.1) (2.5.0)\n",
      "Requirement already satisfied: setuptools>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from fasttext==0.9.1) (46.0.0.post20200311)\n",
      "\n",
      "09:56:46 DEBUG: python -m deeppavlov install fasttext_tfidf_autofaq\n",
      "09:56:48 DEBUG:Requirement already satisfied: fasttext==0.9.1 in /opt/conda/lib/python3.7/site-packages (0.9.1)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from fasttext==0.9.1) (1.18.0)\n",
      "Requirement already satisfied: setuptools>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from fasttext==0.9.1) (46.0.0.post20200311)\n",
      "Requirement already satisfied: pybind11>=2.2 in /opt/conda/lib/python3.7/site-packages (from fasttext==0.9.1) (2.5.0)\n",
      "\n",
      "09:56:48 DEBUG: python -m deeppavlov install tfidf_autofaq\n",
      "09:56:50 DEBUG:\n",
      "09:56:50 DEBUG: python -m deeppavlov install tfidf_logreg_autofaq \n",
      "09:56:51 DEBUG:\n",
      "09:56:51 DEBUG: python -m deeppavlov install tfidf_logreg_en_faq\n",
      "09:56:58 DEBUG:Requirement already satisfied: en_core_web_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz#egg=en_core_web_sm==2.2.5 in /opt/conda/lib/python3.7/site-packages (2.2.5)\n",
      "Requirement already satisfied: spacy>=2.2.2 in /opt/conda/lib/python3.7/site-packages (from en_core_web_sm==2.2.5) (2.2.3)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /opt/conda/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /opt/conda/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.7.0)\n",
      "Requirement already satisfied: srsly<1.1.0,>=0.1.0 in /opt/conda/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n",
      "Requirement already satisfied: thinc<7.4.0,>=7.3.0 in /opt/conda/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.3.1)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/conda/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.22.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.2)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /opt/conda/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.18.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (46.0.0.post20200311)\n",
      "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /opt/conda/lib/python3.7/site-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.5.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /opt/conda/lib/python3.7/site-packages (from thinc<7.4.0,>=7.3.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (4.41.1)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2019.11.28)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.25.7)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.1.0)\n",
      "Requirement already satisfied: spacy==2.2.3 in /opt/conda/lib/python3.7/site-packages (2.2.3)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from spacy==2.2.3) (46.0.0.post20200311)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /opt/conda/lib/python3.7/site-packages (from spacy==2.2.3) (1.0.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.7/site-packages (from spacy==2.2.3) (1.0.2)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from spacy==2.2.3) (0.4.1)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /opt/conda/lib/python3.7/site-packages (from spacy==2.2.3) (1.1.3)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /opt/conda/lib/python3.7/site-packages (from spacy==2.2.3) (1.18.0)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from spacy==2.2.3) (0.7.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/conda/lib/python3.7/site-packages (from spacy==2.2.3) (2.22.0)\n",
      "Requirement already satisfied: srsly<1.1.0,>=0.1.0 in /opt/conda/lib/python3.7/site-packages (from spacy==2.2.3) (1.0.2)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from spacy==2.2.3) (3.0.2)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from spacy==2.2.3) (2.0.3)\n",
      "Requirement already satisfied: thinc<7.4.0,>=7.3.0 in /opt/conda/lib/python3.7/site-packages (from spacy==2.2.3) (7.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /opt/conda/lib/python3.7/site-packages (from catalogue<1.1.0,>=0.0.7->spacy==2.2.3) (1.5.0)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy==2.2.3) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy==2.2.3) (2019.11.28)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy==2.2.3) (2.8)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy==2.2.3) (1.25.7)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /opt/conda/lib/python3.7/site-packages (from thinc<7.4.0,>=7.3.0->spacy==2.2.3) (4.41.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy==2.2.3) (3.1.0)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "run_shell_installs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def action_over_list_f(arr, v):\n",
    "    ''' v[0] and v[1] are dictionaries\n",
    "        arr is array of dictionaries \n",
    "    '''\n",
    "\n",
    "    k_id, v_id = next(iter(v[0].items()))\n",
    "\n",
    "    for p, a in enumerate(arr):\n",
    "        if k_id in a.keys() and a[k_id] == v_id:\n",
    "            for k_rep, v_rep in v[1].items():\n",
    "                arr[p][k_rep] = v_rep\n",
    "\n",
    "\n",
    "def replacement_f(model_config, **args):\n",
    "    '''Replaces the model config dictionary with new values\n",
    "    provided in **args\n",
    "    '''\n",
    "    for k, v in args.items():\n",
    "        if isinstance(v, dict):\n",
    "            replacement_f(model_config[k], **v)\n",
    "        if isinstance(v, str):\n",
    "            model_config[k] = v\n",
    "        if isinstance(model_config[k], list):\n",
    "            action_over_list_f(model_config[k], v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test action_over_list_f\n",
    "from random import randint\n",
    "\n",
    "\n",
    "def gen_list_keys_for_tests():\n",
    "    '''This function is used for tests\n",
    "    '''\n",
    "\n",
    "    str_n = lambda x: f'{x}_{randint(1,10):1}'\n",
    "    gen_dict_list = lambda: {\n",
    "        'id': str_n('id'),\n",
    "        'key1': str_n('v1'),\n",
    "        'key2': str_n('v2'),\n",
    "        'key3': str_n('v3')\n",
    "    }\n",
    "\n",
    "    pipe_list = [gen_dict_list() for _ in range(randint(3, 10))]\n",
    "\n",
    "    rand_id = pipe_list[randint(0, len(pipe_list) - 1)]['id']\n",
    "    rand_key = f'key{randint(1, 3)}' \n",
    "\n",
    "    new_rand_val = str_n('new')\n",
    "    args = {\n",
    "        'chains': {\n",
    "            'pipe': [{\n",
    "                'id': rand_id\n",
    "            }, {\n",
    "               rand_key : new_rand_val\n",
    "            }]\n",
    "        }\n",
    "    }\n",
    "\n",
    "    return pipe_list, rand_id, rand_key, args, new_rand_val\n",
    "\n",
    "\n",
    "def test_action_over_list_f():\n",
    "\n",
    "\n",
    "    pipe_list, rand_id, rand_key, args, new_rand_val = gen_list_keys_for_tests()\n",
    "\n",
    "    assert all(\n",
    "        new_rand_val not in pipe_elem.values() for pipe_elem in pipe_list\n",
    "    )\n",
    "\n",
    "    action_over_list_f(pipe_list, args['chains']['pipe'])\n",
    "\n",
    "    assert any(\n",
    "        rand_key in pipe_elem.keys() and\n",
    "        new_rand_val in pipe_elem.values() for pipe_elem in pipe_list\n",
    "    )\n",
    "\n",
    "\n",
    "def test_replacement_f_list():\n",
    "\n",
    "    pipe_list, rand_id, rand_key, args, new_rand_val = gen_list_keys_for_tests()\n",
    "\n",
    "    mod_conf = {'chains': {'pipe': pipe_list}}\n",
    "\n",
    "    assert all(\n",
    "        new_rand_val not in pipe_elem.values()\n",
    "        for pipe_elem in mod_conf['chains']['pipe']\n",
    "    )\n",
    "\n",
    "    replacement_f(model_config=mod_conf, **args)\n",
    "    assert any(\n",
    "        rand_key in pipe_elem.keys() and\n",
    "        new_rand_val in pipe_elem.values()\n",
    "        for pipe_elem in mod_conf['chains']['pipe']\n",
    "    )\n",
    "\n",
    "\n",
    "def test_replacement_f_val():\n",
    "    args = {'key3': 'newvalue'}\n",
    "    mod_conf = {'key1': 'val1', 'key2': 'val2', 'key3': 'val3'}\n",
    "    replacement_f(model_config=mod_conf, **args)\n",
    "    assert all(\n",
    "        arg_k in mod_conf.keys() and arg_v in mod_conf.values()\n",
    "        for arg_k, arg_v in args.items()\n",
    "    )\n",
    "\n",
    "\n",
    "def test_replacement_f_dict():\n",
    "    args = {'1_key_3': {'2_key_2': 'newvalue'}}\n",
    "    mod_conf = {'1_key_3': {'2_key_2': 'oldvalue'}, '0_key_': '0_val'}\n",
    "    replacement_f(model_config=mod_conf, **args)\n",
    "    assert mod_conf['1_key_3']['2_key_2'] == 'newvalue'\n",
    "\n",
    "\n",
    "test_action_over_list_f()\n",
    "test_replacement_f_list()\n",
    "test_replacement_f_val()\n",
    "test_replacement_f_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def updates_faq_config_file(\n",
    "    configs_path,\n",
    "    **args\n",
    "):\n",
    "    '''Updates deepplavov json config file \n",
    "    '''\n",
    "    #set FAQ data in config file\n",
    "    model_config = json.load(open(configs_path))\n",
    "\n",
    "    if 'data_url' in model_config['dataset_reader']:\n",
    "        del model_config['dataset_reader']['data_url']\n",
    "\n",
    "    replacement_f(model_config=model_config,**args)\n",
    "\n",
    "    json.dump(model_config, open(configs_path, 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test updates_faq_config_file\n",
    "import tempfile\n",
    "from shutil import copyfile\n",
    "\n",
    "\n",
    "def gen_list_keys_for_tests():\n",
    "\n",
    "    str_n = lambda x: f'{x}_{randint(1,10):1}'\n",
    "    gen_dict_list = lambda: {\n",
    "        'id': str_n('id'),\n",
    "        'key1': str_n('v1'),\n",
    "        'key2': str_n('v2'),\n",
    "        'key3': str_n('v3')\n",
    "    }\n",
    "\n",
    "    pipe_list = [gen_dict_list() for _ in range(randint(3, 10))]\n",
    "\n",
    "    rand_id = pipe_list[randint(0, len(pipe_list) - 1)]['id']\n",
    "    rand_key =  f'key{randint(1, 3)}' \n",
    "\n",
    "    new_rand_val = str_n('new')\n",
    "    pipe_dict = {'pipe': [{'id': rand_id}, {rand_key: new_rand_val}]}\n",
    "    args = {'chainer': pipe_dict}\n",
    "\n",
    "    return pipe_list, rand_id, rand_key, args, new_rand_val\n",
    "\n",
    "\n",
    "def test_updates_faq_config_file_update_string():\n",
    "\n",
    "    with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "\n",
    "        tmp_config_file = path.join(tmpdirname, 'tmp_file.json')\n",
    "\n",
    "        copyfile(configs.faq.tfidf_logreg_en_faq, tmp_config_file)\n",
    "\n",
    "        assert path.isfile(tmp_config_file)\n",
    "\n",
    "        updates_faq_config_file(\n",
    "            configs_path=tmp_config_file,\n",
    "            dataset_reader={'data_path': 'fictional_csv_file.csv'}\n",
    "        )\n",
    "\n",
    "        config_json = json.load(open(tmp_config_file))\n",
    "        assert 'data_path' in config_json['dataset_reader']\n",
    "\n",
    "\n",
    "def test_updates_faq_config_file_update_list():\n",
    "\n",
    "    with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "\n",
    "        tmp_config_file = path.join(tmpdirname, 'tmp_file.json')\n",
    "\n",
    "        pipe_list, rand_id, rand_key, args, new_rand_val = gen_list_keys_for_tests(\n",
    "        )\n",
    "        mod_conf = {\n",
    "            'chainer': {\n",
    "                'pipe': pipe_list\n",
    "            },\n",
    "            'dataset_reader': 'dataset_reader_dictionary'\n",
    "        }\n",
    "\n",
    "        json.dump(mod_conf, open(tmp_config_file, 'w'))\n",
    "\n",
    "        assert path.isfile(tmp_config_file)\n",
    "\n",
    "        updates_faq_config_file(configs_path=tmp_config_file, **args)\n",
    "\n",
    "        config_json = json.load(open(tmp_config_file))\n",
    "   \n",
    "        assert any(\n",
    "            rand_key in pipe_elem.keys() and new_rand_val in pipe_elem.values()\n",
    "            for pipe_elem in config_json['chainer']['pipe']\n",
    "        )\n",
    "\n",
    "\n",
    "test_updates_faq_config_file_update_string()\n",
    "test_updates_faq_config_file_update_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def select_faq_responses(faq_model, question):\n",
    "    '''Calls Deeppavlov FAQ model\n",
    "    '''\n",
    "    return faq_model([question])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "09:56:59 DEBUG:Starting new HTTP connection (1): files.deeppavlov.ai:80\n",
      "09:56:59 DEBUG:http://files.deeppavlov.ai:80 \"GET /faq/mipt/en_mipt_faq_v4.tar.gz.md5 HTTP/1.1\" 200 189\n",
      "09:56:59 DEBUG:Starting new HTTP connection (1): files.deeppavlov.ai:80\n",
      "09:57:00 DEBUG:http://files.deeppavlov.ai:80 \"GET /faq/mipt/en_mipt_faq_v4.tar.gz HTTP/1.1\" 200 12276\n",
      "100%|██████████| 12.3k/12.3k [00:00<00:00, 7.71MB/s]\n",
      "[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package perluniprops to\n",
      "[nltk_data]     /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package perluniprops is already up-to-date!\n",
      "[nltk_data] Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]     /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package nonbreaking_prefixes is already up-to-date!\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "09:57:02 DEBUG:Starting new HTTP connection (1): files.deeppavlov.ai:80\n",
      "09:57:02 DEBUG:http://files.deeppavlov.ai:80 \"GET /faq/mipt/en_mipt_faq_v4.tar.gz.md5 HTTP/1.1\" 200 189\n",
      "09:57:02 DEBUG:Starting new HTTP connection (1): files.deeppavlov.ai:80\n",
      "09:57:02 DEBUG:http://files.deeppavlov.ai:80 \"GET /faq/mipt/en_mipt_faq_v4.tar.gz HTTP/1.1\" 200 12276\n",
      "100%|██████████| 12.3k/12.3k [00:00<00:00, 9.63MB/s]\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "09:57:07 DEBUG:Starting new HTTP connection (1): files.deeppavlov.ai:80\n",
      "09:57:07 DEBUG:http://files.deeppavlov.ai:80 \"GET /faq/mipt/en_mipt_faq_v4.tar.gz.md5 HTTP/1.1\" 200 189\n",
      "09:57:07 DEBUG:Starting new HTTP connection (1): files.deeppavlov.ai:80\n",
      "09:57:08 DEBUG:http://files.deeppavlov.ai:80 \"GET /faq/mipt/en_mipt_faq_v4.tar.gz HTTP/1.1\" 200 12276\n",
      "100%|██████████| 12.3k/12.3k [00:00<00:00, 6.32MB/s]\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "#test faq responses\n",
    "import tempfile\n",
    "from shutil import copyfile\n",
    "\n",
    "\n",
    "def gen_mock_csv_file(tmpdirname, faqs):\n",
    "\n",
    "    temp_faq_csv = path.join(tmpdirname, 'tmp_faq.csv')\n",
    "\n",
    "    pd.DataFrame(faqs).to_csv(temp_faq_csv, index=False)\n",
    "\n",
    "    return temp_faq_csv\n",
    "\n",
    "\n",
    "def gen_mock_vocab_answers(tmpdirname, vocabs):\n",
    "\n",
    "    temp_dict_file = path.join(tmpdirname, 'temp_vocab_answers.dict')\n",
    "    vocabs_text = '\\n'.join(\n",
    "        t + '\\t' + str(f) for t, f in zip(vocabs['text'], vocabs['freq'])\n",
    "    )\n",
    "\n",
    "    f = open(temp_dict_file, 'w')\n",
    "    f.write(vocabs_text)\n",
    "    f.close()\n",
    "\n",
    "    return temp_dict_file\n",
    "\n",
    "\n",
    "def gen_faq_config(tmpdirname, vocab_file, faq_file):\n",
    "\n",
    "    temp_configs_faq = path.join(tmpdirname, 'temp_config_faq.json')\n",
    "    copyfile(configs.faq.tfidf_logreg_en_faq, temp_configs_faq)\n",
    "\n",
    "    changes_dict = {'save_path': vocab_file, 'load_path': vocab_file}\n",
    "    id_dict = {'id': 'answers_vocab'}\n",
    "\n",
    "    updates_faq_config_file(\n",
    "        configs_path=temp_configs_faq,\n",
    "        chainer={'pipe': [id_dict, changes_dict]},\n",
    "        dataset_reader={'data_path': faq_file}\n",
    "    )\n",
    "\n",
    "    return temp_configs_faq\n",
    "\n",
    "\n",
    "def test_faq_response_with_minimum_faqs_in_dataframe_fail_case():\n",
    "    with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "\n",
    "        faqs = {\n",
    "            'Question': ['Is Covid erradicated?'],\n",
    "            'Answer': ['Definitely not!']\n",
    "        }\n",
    "\n",
    "        vocabs = {'text': ['This is a vocab example'], 'freq': [1]}\n",
    "\n",
    "        faq_file = gen_mock_csv_file(tmpdirname, faqs)\n",
    "        vocab_file = gen_mock_vocab_answers(tmpdirname, vocabs)\n",
    "\n",
    "        configs_file = gen_faq_config(tmpdirname, vocab_file, faq_file)\n",
    "\n",
    "        try:\n",
    "            select_faq_responses(\n",
    "                question='Is Enrique the prettiest person in town?',\n",
    "                faq_model=train_model(configs_file, download=True)\n",
    "            )\n",
    "            assert False\n",
    "        except ValueError as e:\n",
    "            assert True\n",
    "\n",
    "\n",
    "def test_faq_response_with_minimum_faqs_in_dataframe_success_case():\n",
    "    with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "\n",
    "        faqs = {\n",
    "            'Question': ['Is Covid erradicated?', 'Who is the current POTUS?'],\n",
    "            'Answer': ['Definitely not!', 'Donald Trump']\n",
    "        }\n",
    "\n",
    "        vocabs = {'text': ['This is a vocab example'], 'freq': [1]}\n",
    "\n",
    "        faq_file = gen_mock_csv_file(tmpdirname, faqs)\n",
    "        vocab_file = gen_mock_vocab_answers(tmpdirname, vocabs)\n",
    "\n",
    "        configs_file = gen_faq_config(tmpdirname, vocab_file, faq_file)\n",
    "\n",
    "        assert select_faq_responses(\n",
    "            question='Is Enrique the prettiest person in town?',\n",
    "            faq_model=train_model(configs_file, download=True)\n",
    "        ) == ['Donald Trump']\n",
    "\n",
    "        \n",
    "        \n",
    "def test_faq_response_with_minimum_answers_vocab_success_case():\n",
    "    with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "\n",
    "        faqs = {\n",
    "            'Question': ['Is Covid erradicated?', 'Who is the current POTUS?'],\n",
    "            'Answer': ['Definitely not!', 'Donald Trump']\n",
    "        }\n",
    "\n",
    "        vocabs = {'text': [], 'freq': []}\n",
    "\n",
    "        faq_file = gen_mock_csv_file(tmpdirname, faqs)\n",
    "        vocab_file = gen_mock_vocab_answers(tmpdirname, vocabs)\n",
    "\n",
    "        configs_file = gen_faq_config(tmpdirname, vocab_file, faq_file)\n",
    "\n",
    "        select_faq_responses(\n",
    "            question='Is Enrique the prettiest person in town?',\n",
    "            faq_model=train_model(configs_file, download=True)\n",
    "        ) == ['Donald Trump']\n",
    "\n",
    "test_faq_response_with_minimum_faqs_in_dataframe_fail_case()\n",
    "test_faq_response_with_minimum_faqs_in_dataframe_success_case()\n",
    "test_faq_response_with_minimum_answers_vocab_success_case()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def select_squad_responses(\n",
    "    contexts, squad_model, question, best_results=1\n",
    "):\n",
    "    '''Calls Deeppavlov BERT and RNET Context Question Answering\n",
    "    '''\n",
    "    responses = contexts.context.apply(\n",
    "        lambda context: squad_model([context], [question])\n",
    "    ).values\n",
    "    \n",
    "    logging.debug(f'Responses: {responses}')\n",
    "    top_responses = [\n",
    "        r[0][0] for r in sorted(responses, key=lambda x: -1 * x[2][0])\n",
    "        [:best_results]\n",
    "    ]\n",
    "\n",
    "    logging.debug(f'Top Responses: {top_responses}')\n",
    "    return responses, top_responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "09:57:11 DEBUG:Starting new HTTP connection (1): files.deeppavlov.ai:80\n",
      "09:57:11 DEBUG:http://files.deeppavlov.ai:80 \"GET /deeppavlov_data/bert/cased_L-12_H-768_A-12.zip.md5 HTTP/1.1\" 200 386\n",
      "09:57:13 DEBUG:Starting new HTTP connection (1): files.deeppavlov.ai:80\n",
      "09:57:13 DEBUG:http://files.deeppavlov.ai:80 \"GET /deeppavlov_data/squad_bert.tar.gz.md5 HTTP/1.1\" 200 184\n",
      "09:57:16 WARNING:From /opt/conda/lib/python3.7/site-packages/bert_dp/tokenization.py:125: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
      "\n",
      "09:57:16 WARNING:From /opt/conda/lib/python3.7/site-packages/deeppavlov/core/models/tf_model.py:37: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "09:57:16 WARNING:From /opt/conda/lib/python3.7/site-packages/deeppavlov/core/models/tf_model.py:222: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "09:57:16 WARNING:From /opt/conda/lib/python3.7/site-packages/deeppavlov/core/models/tf_model.py:222: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "09:57:16 WARNING:From /opt/conda/lib/python3.7/site-packages/deeppavlov/core/models/tf_model.py:193: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n",
      "09:57:16 WARNING:From /opt/conda/lib/python3.7/site-packages/deeppavlov/models/bert/bert_squad.py:81: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "09:57:16 WARNING:From /opt/conda/lib/python3.7/site-packages/deeppavlov/models/bert/bert_squad.py:178: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "09:57:16 WARNING:From /opt/conda/lib/python3.7/site-packages/bert_dp/modeling.py:178: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "09:57:16 WARNING:From /opt/conda/lib/python3.7/site-packages/bert_dp/modeling.py:418: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n",
      "09:57:16 WARNING:From /opt/conda/lib/python3.7/site-packages/bert_dp/modeling.py:499: The name tf.assert_less_equal is deprecated. Please use tf.compat.v1.assert_less_equal instead.\n",
      "\n",
      "09:57:16 WARNING:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "09:57:17 WARNING:From /opt/conda/lib/python3.7/site-packages/bert_dp/modeling.py:366: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "09:57:17 WARNING:From /opt/conda/lib/python3.7/site-packages/bert_dp/modeling.py:680: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.Dense instead.\n",
      "09:57:17 WARNING:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/layers/core.py:187: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n",
      "09:57:17 WARNING:From /opt/conda/lib/python3.7/site-packages/bert_dp/modeling.py:283: The name tf.erf is deprecated. Please use tf.math.erf instead.\n",
      "\n",
      "09:57:19 WARNING:From /opt/conda/lib/python3.7/site-packages/deeppavlov/models/bert/bert_squad.py:154: The name tf.matrix_band_part is deprecated. Please use tf.linalg.band_part instead.\n",
      "\n",
      "09:57:19 WARNING:From /opt/conda/lib/python3.7/site-packages/deeppavlov/models/bert/bert_squad.py:166: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "09:57:19 WARNING:From /opt/conda/lib/python3.7/site-packages/deeppavlov/core/models/tf_model.py:234: The name tf.train.AdadeltaOptimizer is deprecated. Please use tf.compat.v1.train.AdadeltaOptimizer instead.\n",
      "\n",
      "09:57:19 WARNING:From /opt/conda/lib/python3.7/site-packages/deeppavlov/core/models/tf_model.py:127: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n",
      "\n",
      "09:57:19 WARNING:From /opt/conda/lib/python3.7/site-packages/deeppavlov/core/models/tf_model.py:127: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
      "\n",
      "09:57:28 WARNING:From /opt/conda/lib/python3.7/site-packages/deeppavlov/models/bert/bert_squad.py:89: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
      "\n",
      "09:57:32 WARNING:From /opt/conda/lib/python3.7/site-packages/deeppavlov/models/bert/bert_squad.py:94: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "09:57:32 WARNING:From /opt/conda/lib/python3.7/site-packages/deeppavlov/core/models/tf_model.py:54: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
      "\n",
      "09:57:32 INFO:Restoring parameters from /home/jovyan/.deeppavlov/models/squad_bert/model\n",
      "09:57:34 DEBUG:Responses: []\n",
      "09:57:34 DEBUG:Top Responses: []\n",
      "09:57:36 DEBUG:Responses: [list([['Elon Musk'], [203], [50257280.0]])]\n",
      "09:57:36 DEBUG:Top Responses: ['Elon Musk']\n",
      "09:57:38 DEBUG:Responses: [list([['TJ'], [44], [6978.86279296875]])\n",
      " list([['north of mexico'], [22], [81567.328125]])]\n",
      "09:57:38 DEBUG:Top Responses: ['north of mexico', 'TJ']\n",
      "09:57:38 DEBUG:Starting new HTTP connection (1): files.deeppavlov.ai:80\n",
      "09:57:38 DEBUG:http://files.deeppavlov.ai:80 \"GET /embeddings/wiki-news-300d-1M-char.vec.md5 HTTP/1.1\" 200 61\n",
      "09:57:38 DEBUG:Starting new HTTP connection (1): files.deeppavlov.ai:80\n",
      "09:57:39 DEBUG:http://files.deeppavlov.ai:80 \"GET /embeddings/wiki-news-300d-1M.vec.md5 HTTP/1.1\" 200 56\n",
      "09:57:43 DEBUG:Starting new HTTP connection (1): files.deeppavlov.ai:80\n",
      "09:57:43 DEBUG:http://files.deeppavlov.ai:80 \"GET /deeppavlov_data/squad_model_1.4_cpu_compatible.tar.gz.md5 HTTP/1.1\" 200 389\n",
      "09:57:45 WARNING:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/contrib/cudnn_rnn/python/ops/cudnn_rnn_ops.py:122: GRUCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.GRUCell, and will be replaced by that in Tensorflow 2.0.\n",
      "09:57:45 WARNING:From /opt/conda/lib/python3.7/site-packages/deeppavlov/core/layers/tf_layers.py:595: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n",
      "09:57:45 WARNING:From /opt/conda/lib/python3.7/site-packages/deeppavlov/core/layers/tf_layers.py:600: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "09:57:45 WARNING:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/contrib/cudnn_rnn/python/ops/cudnn_rnn_ops.py:133: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.add_weight` method instead.\n",
      "09:57:45 WARNING:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/contrib/cudnn_rnn/python/ops/cudnn_rnn_ops.py:139: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "09:57:45 WARNING:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/contrib/cudnn_rnn/python/ops/cudnn_rnn_ops.py:155: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "09:57:45 WARNING:From /opt/conda/lib/python3.7/site-packages/deeppavlov/core/layers/tf_layers.py:812: calling reverse_sequence (from tensorflow.python.ops.array_ops) with seq_dim is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "seq_dim is deprecated, use seq_axis instead\n",
      "09:57:45 WARNING:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/util/deprecation.py:507: calling reverse_sequence (from tensorflow.python.ops.array_ops) with batch_dim is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "batch_dim is deprecated, use batch_axis instead\n",
      "09:57:46 WARNING:From /opt/conda/lib/python3.7/site-packages/deeppavlov/models/squad/utils.py:101: The name tf.AUTO_REUSE is deprecated. Please use tf.compat.v1.AUTO_REUSE instead.\n",
      "\n",
      "09:57:48 WARNING:From /opt/conda/lib/python3.7/site-packages/deeppavlov/models/squad/utils.py:139: The name tf.get_variable_scope is deprecated. Please use tf.compat.v1.get_variable_scope instead.\n",
      "\n",
      "09:57:59 WARNING:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/ops/clip_ops.py:172: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "09:58:07 INFO:Restoring parameters from /home/jovyan/.deeppavlov/models/squad_model/model\n",
      "09:58:09 DEBUG:Responses: []\n",
      "09:58:09 DEBUG:Top Responses: []\n",
      "09:58:11 DEBUG:Responses: [list([['Elon Musk'], [203], [36056324.0]])]\n",
      "09:58:11 DEBUG:Top Responses: ['Elon Musk']\n",
      "09:58:11 DEBUG:Responses: [list([['TJ'], [44], [564.4996948242188]])\n",
      " list([['north of mexico'], [22], [138151.90625]])]\n",
      "09:58:11 DEBUG:Top Responses: ['north of mexico', 'TJ']\n"
     ]
    }
   ],
   "source": [
    "#test select_squad_responses\n",
    "import tempfile\n",
    "from shutil import copyfile\n",
    "\n",
    "empty = {'topic': [], 'context': []}\n",
    "spacex = {\n",
    "    'topic': ['SpaceX'],\n",
    "    'context':\n",
    "        [\n",
    "            '''Space Exploration Technologies Corp., trading as SpaceX, is an American aerospace manufacturer and space transportation\n",
    "services company headquartered in Hawthorne, California. It was founded in 2002 by Elon Musk with the goal of reducing space \n",
    "transportation costs to enable the colonization of Mars. SpaceX has developed several launch vehicles, the Starlink satellite\n",
    "constellation, and the Dragon spacecraft. It is widely considered among the most successful private spaceflight companies.'''\n",
    "        ]\n",
    "}\n",
    "\n",
    "intekglobal = {\n",
    "    'topic': ['Intekglobal', 'InG'],\n",
    "    'context':\n",
    "        [\n",
    "            'Intekglobal has its headquarters located in TJ',\n",
    "            'Intekglobal is in the north of mexico'\n",
    "        ]\n",
    "}\n",
    "\n",
    "\n",
    "def assert_squad_model(\n",
    "    contexts, squad_model, question, expected_responses, **args\n",
    "):\n",
    "    responses, top_responses = select_squad_responses(\n",
    "        contexts=pd.DataFrame(contexts),\n",
    "        squad_model=squad_model,\n",
    "        question=question,\n",
    "        **args\n",
    "    )\n",
    "    assert top_responses == expected_responses\n",
    "\n",
    "\n",
    "def test_squad_bert():\n",
    "\n",
    "    bert = build_model(configs.squad.squad_bert, download=True)\n",
    "\n",
    "    assert_squad_model(\n",
    "        empty,\n",
    "        bert,\n",
    "        'Is an empty response expected?',\n",
    "        expected_responses=[],\n",
    "        best_results=2\n",
    "    )\n",
    "\n",
    "    assert_squad_model(\n",
    "        spacex, bert, 'Who founded SpaceX?', expected_responses=['Elon Musk']\n",
    "    )\n",
    "\n",
    "    assert_squad_model(\n",
    "        intekglobal,\n",
    "        bert,\n",
    "        'Where is Intekglobal located?',\n",
    "        expected_responses=['north of mexico','TJ'],\n",
    "        best_results=2\n",
    "    )\n",
    "\n",
    "\n",
    "def test_squad_rnet():\n",
    "\n",
    "    bert = build_model(configs.squad.squad, download=True)\n",
    "\n",
    "    assert_squad_model(\n",
    "        empty,\n",
    "        bert,\n",
    "        'Is an empty response expected?',\n",
    "        expected_responses=[],\n",
    "        best_results=5\n",
    "    )\n",
    "\n",
    "    assert_squad_model(\n",
    "        spacex, bert, 'Who founded SpaceX?', expected_responses=['Elon Musk']\n",
    "    )\n",
    "\n",
    "    assert_squad_model(\n",
    "        intekglobal,\n",
    "        bert,\n",
    "        'Where is Intekglobal located?',\n",
    "        expected_responses=['north of mexico','TJ'],\n",
    "        best_results=2\n",
    "    )\n",
    "\n",
    "test_squad_bert()\n",
    "test_squad_rnet()\n",
    "del spacex, empty, intekglobal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def load_qa_models(\n",
    "    config_rnet=configs.squad.squad,\n",
    "    config_bert=configs.squad.squad_bert,\n",
    "    config_tfidf=configs.faq.tfidf_logreg_en_faq,\n",
    "    download=True\n",
    "):\n",
    "    ''' Load the squad and faq models\n",
    "    INPUT:\\n\n",
    "    - config_rnet -> path to json config file \n",
    "    - config_bert -> path to json config file  \n",
    "    - config_tfidf -> path to json config file  \n",
    "    - download -> download files (True/False)  \n",
    "    \n",
    "    The default for the config files are the deeppavlov config files. The default download is True.\n",
    "    '''\n",
    "    qa_models = {\n",
    "        'squad':\n",
    "            {\n",
    "                'rnet': build_model(config_rnet, download=download),\n",
    "                'bert': build_model(config_bert, download=download)\n",
    "            },\n",
    "        'faq': {\n",
    "            'tfidf': train_model(config_tfidf, download=download)\n",
    "        }\n",
    "    }\n",
    "    return qa_models\n",
    "\n",
    "def get_responses(contexts, question, qa_models, nb_squad_results=1):\n",
    "    ''' Generates responses from a question\\n\n",
    "    INPUT: \\n\n",
    "    - question -> question string \\n\n",
    "    - contexts -> list of contexts      \n",
    "    - qa_models -> dictionary of available models (see load_qa_models)    \n",
    "    '''\n",
    "    responses = {'squad': defaultdict(list), 'faq': defaultdict(list)}\n",
    "    for squad_name, squad_model in qa_models['squad'].items():\n",
    "        responses['squad'][squad_name] = select_squad_responses(\n",
    "            contexts, squad_model, question, best_results=nb_squad_results\n",
    "        )[1]\n",
    "    for faq_name, faq_model in qa_models['faq'].items():\n",
    "        responses['faq'][faq_name] = select_faq_responses(faq_model, question)\n",
    "    return question, responses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "09:58:39 INFO:Restoring parameters from /home/jovyan/.deeppavlov/models/squad_model/model\n",
      "09:59:08 INFO:Restoring parameters from /home/jovyan/.deeppavlov/models/squad_bert/model\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "09:59:28 DEBUG:Responses: [list([['TJ'], [44], [120.95974731445312]])\n",
      " list([['north of mexico'], [22], [174602.40625]])]\n",
      "09:59:28 DEBUG:Top Responses: ['north of mexico', 'TJ']\n",
      "09:59:31 DEBUG:Responses: [list([['TJ'], [44], [22507.34375]])\n",
      " list([['north of mexico'], [22], [269778.3125]])]\n",
      "09:59:31 DEBUG:Top Responses: ['north of mexico', 'TJ']\n",
      "09:59:31 DEBUG: Question: Where is Intekglobal?\n",
      "09:59:31 DEBUG: Responses: {'squad': defaultdict(<class 'list'>, {'rnet': ['north of mexico', 'TJ'], 'bert': ['north of mexico', 'TJ']}), 'faq': defaultdict(<class 'list'>, {'tfidf': ['Yes it is!']})}\n",
      "09:59:53 INFO:Restoring parameters from /home/jovyan/.deeppavlov/models/squad_model/model\n",
      "10:00:21 INFO:Restoring parameters from /home/jovyan/.deeppavlov/models/squad_bert/model\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "10:00:27 DEBUG:Responses: []\n",
      "10:00:27 DEBUG:Top Responses: []\n",
      "10:00:27 DEBUG:Responses: []\n",
      "10:00:27 DEBUG:Top Responses: []\n",
      "10:00:27 DEBUG: Question: What is the minimun number of FAQ questions\n",
      "10:00:27 DEBUG: Responses: {'squad': defaultdict(<class 'list'>, {'rnet': [], 'bert': []}), 'faq': defaultdict(<class 'list'>, {'tfidf': ['Two']})}\n"
     ]
    }
   ],
   "source": [
    "# test get_responses\n",
    "import tempfile\n",
    "from shutil import copyfile\n",
    "\n",
    "intekglobal_context = {\n",
    "    'topic': ['Intekglobal', 'InG'],\n",
    "    'context':\n",
    "        [\n",
    "            'Intekglobal has its headquarters located in TJ',\n",
    "            'Intekglobal is in the north of mexico'\n",
    "        ]\n",
    "}\n",
    "\n",
    "intekglobal_faqs = {\n",
    "    'Question': ['Is Intekglobal an IT company?', 'Where can I apply?'],\n",
    "    'Answer':\n",
    "        ['Yes it is!', 'Please refer the our website for further information']\n",
    "}\n",
    "\n",
    "\n",
    "def mock_faq_files(tmpdirname, faqs):\n",
    "\n",
    "    faq_files = {\n",
    "        'data': path.join(tmpdirname, 'temp_faq.csv'),\n",
    "        'config': path.join(tmpdirname, 'temp_config_faq.json')\n",
    "    }\n",
    "\n",
    "    pd.DataFrame(faqs).to_csv(faq_files['data'], index=False)\n",
    "    copyfile(configs.faq.tfidf_logreg_en_faq, faq_files['config'])\n",
    "\n",
    "    updates_faq_config_file(\n",
    "        configs_path=faq_files['config'],\n",
    "        dataset_reader={'data_path': faq_files['data']}\n",
    "    )\n",
    "\n",
    "    return faq_files\n",
    "\n",
    "\n",
    "def test_get_intekglobal_responses():\n",
    "    with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "\n",
    "        faq_files = mock_faq_files(tmpdirname, intekglobal_faqs)\n",
    "        qa_models = load_qa_models(\n",
    "            config_tfidf=faq_files['config'], download=False\n",
    "        )\n",
    "\n",
    "        question, responses = get_responses(\n",
    "            pd.DataFrame(intekglobal_context),\n",
    "            'Where is Intekglobal?',\n",
    "            qa_models,\n",
    "            nb_squad_results=2\n",
    "        )\n",
    "\n",
    "        logging.debug(f' Question: {question}')\n",
    "        logging.debug(f\" Responses: {responses}\")\n",
    "        assert all(\n",
    "            response in ('north of mexico', 'TJ', 'Yes it is!')\n",
    "            for model_responses in responses['squad'].values()\n",
    "            for response in model_responses\n",
    "        )\n",
    "\n",
    "\n",
    "def test_get_responses_with_empty_context():\n",
    "    with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "        min_faqs = {\n",
    "            'Question':\n",
    "                ['Minimum number of questions?', 'This is the other question?'],\n",
    "            'Answer': ['Two', 'yes']\n",
    "        }\n",
    "        faq_files = mock_faq_files(tmpdirname, min_faqs)\n",
    "\n",
    "        qa_models = load_qa_models(\n",
    "            config_tfidf=faq_files['config'], download=False\n",
    "        )\n",
    "        empty_context = {'topic': [], 'context': []}\n",
    "\n",
    "        question, responses = get_responses(\n",
    "            pd.DataFrame(empty_context),\n",
    "            'What is the minimun number of FAQ questions',\n",
    "            qa_models,\n",
    "            nb_squad_results=2\n",
    "        )\n",
    "\n",
    "        logging.debug(f' Question: {question}')\n",
    "        logging.debug(f' Responses: {responses}')\n",
    "        assert responses['faq']['tfidf'] == ['Two']\n",
    "\n",
    "\n",
    "test_get_intekglobal_responses()\n",
    "test_get_responses_with_empty_context()\n",
    "\n",
    "del intekglobal_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def format_responses(dict_responses):\n",
    "    '''Format question-response pair\\n\n",
    "    INPUT:\\n\n",
    "    - dictionary of responses \\n\n",
    "    OUTPUT:\\n\n",
    "    -  list of flattened responses\\n\n",
    "    -  response as string\n",
    "    '''\n",
    "    flatten_responses = [\n",
    "        res for model_responses in dict_responses.values()\n",
    "        for res in model_responses.values()\n",
    "    ]\n",
    "    flatten_responses = [r for res in flatten_responses for r in res]\n",
    "    logging.debug(flatten_responses)\n",
    "\n",
    "    formatted_response = f'\\n Answers:\\n\\n'\n",
    "    for k, res in enumerate(set(flatten_responses)):\n",
    "        formatted_response += f'{k+1}: {res}\\n'\n",
    "\n",
    "    logging.debug(formatted_response)\n",
    "    return flatten_responses, formatted_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10:00:28 DEBUG:['sq_11', 'sq_12', 'sq_21', 'fq_11', 'fq_21', 'fq_22']\n",
      "10:00:28 DEBUG:\n",
      " Answers:\n",
      "\n",
      "1: fq_21\n",
      "2: sq_21\n",
      "3: fq_11\n",
      "4: sq_11\n",
      "5: sq_12\n",
      "6: fq_22\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#test\n",
    "def test_format_responses():\n",
    "    dict_responses = {\n",
    "        'sq': {\n",
    "            '1': ['sq_11', 'sq_12'],\n",
    "            '2': ['sq_21']\n",
    "        },\n",
    "        'fq': {\n",
    "            '3': ['fq_11'],\n",
    "            '4': ['fq_21', 'fq_22']\n",
    "        }\n",
    "    }\n",
    "    flatten_responses, formatted_response = format_responses(\n",
    "        dict_responses=dict_responses\n",
    "    )\n",
    "    expected_arr =[\n",
    "        'sq_11', 'sq_12', 'sq_21', 'fq_11', 'fq_21', 'fq_22'\n",
    "    ]\n",
    "    assert flatten_responses == expected_arr\n",
    "    assert all(res in formatted_response for res in expected_arr)\n",
    "test_format_responses()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_input(text):\n",
    "    '''This redundancy is needed for testing'''\n",
    "    return input(text)\n",
    "\n",
    "\n",
    "def question_response(data, qa_models, num_returned_values_per_squad_model=1):\n",
    "    ''' Receive response and call get_response()\n",
    "    '''\n",
    "    question = get_input('Introduce question:\\n')\n",
    "\n",
    "    _, responses = get_responses(\n",
    "        data['context']['df'], question, qa_models, nb_squad_results=1\n",
    "    )\n",
    "    _,formatted_responses = format_responses(responses)\n",
    "    \n",
    "    return question, formatted_responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Test FAQ dialog system's part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10:00:46 INFO:Restoring parameters from /home/jovyan/.deeppavlov/models/squad_model/model\n",
      "10:01:02 INFO:Restoring parameters from /home/jovyan/.deeppavlov/models/squad_bert/model\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "10:01:09 DEBUG:Responses: [list([['Intekglobal'], [0], [160.03579711914062]])\n",
      " list([['Intekglobal'], [3], [244.86245727539062]])\n",
      " list([['one of the smartest minds on the planet'], [19], [1664652.0]])]\n",
      "10:01:09 DEBUG:Top Responses: ['one of the smartest minds on the planet']\n",
      "10:01:13 DEBUG:Responses: [list([[''], [-1], [0.0011892060283571482]])\n",
      " list([[''], [-1], [0.01691678911447525]])\n",
      " list([['one of the smartest minds on the planet, \\n                   he currently works as Intekglobal employee'], [19], [18812.87109375]])]\n",
      "10:01:13 DEBUG:Top Responses: ['one of the smartest minds on the planet, \\n                   he currently works as Intekglobal employee']\n",
      "10:01:13 DEBUG:['one of the smartest minds on the planet', 'one of the smartest minds on the planet, \\n                   he currently works as Intekglobal employee', 'yes']\n",
      "10:01:13 DEBUG:\n",
      " Answers:\n",
      "\n",
      "1: one of the smartest minds on the planet, \n",
      "                   he currently works as Intekglobal employee\n",
      "2: one of the smartest minds on the planet\n",
      "3: yes\n",
      "\n",
      "10:01:13 DEBUG:  Who is Enrique Jimenez?\n",
      "10:01:13 DEBUG:  \n",
      " Answers:\n",
      "\n",
      "1: one of the smartest minds on the planet, \n",
      "                   he currently works as Intekglobal employee\n",
      "2: one of the smartest minds on the planet\n",
      "3: yes\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tempfile\n",
    "from unittest.mock import patch\n",
    "from shutil import copyfile\n",
    "from collections import defaultdict\n",
    "\n",
    "def mock_faq_files(tmpdirname, faqs, faq_dic):\n",
    "\n",
    "    faq_dic['path'] = path.join(tmpdirname, 'temp_faq.csv')\n",
    "    faq_dic['config'] = path.join(tmpdirname, 'temp_config_faq.json')\n",
    "    faq_dic['df'] = pd.DataFrame(faqs)\n",
    "    faq_dic['df'].to_csv(faq_dic['path'], index=False)\n",
    "\n",
    "    copyfile(configs.faq.tfidf_logreg_en_faq, faq_dic['config'])\n",
    "\n",
    "    updates_faq_config_file(\n",
    "        configs_path=faq_dic['config'],\n",
    "        dataset_reader={'data_path': faq_dic['path']}\n",
    "    )\n",
    "\n",
    "\n",
    "def mock_context_file(tmpdirname, contexts, context_dic):\n",
    "\n",
    "    context_dic['path'] = path.join(tmpdirname, 'temp_context.csv')\n",
    "    context_dic['df'] = pd.DataFrame(contexts)\n",
    "    context_dic['df'].to_csv(context_dic['path'], index=False)\n",
    "\n",
    "\n",
    "@patch('__main__.get_input')\n",
    "def test_context_response_with_no_updates(mock_input):\n",
    "    mock_input.side_effect = ['Who is Enrique Jimenez?']\n",
    "    data = {'context': defaultdict(str), 'faq': defaultdict(str)}\n",
    "    contexts = {\n",
    "        'context':\n",
    "            [\n",
    "                'Intekglobal has its headquarters located in TJ',\n",
    "                'In Intekglobal we care about you',\n",
    "                '''Enrique Jimenez is one of the smartest minds on the planet, \n",
    "                   he currently works as Intekglobal employee'''\n",
    "            ],\n",
    "        'topic': ['headquarters', 'mission', 'Enrique\\'s biography']\n",
    "    }\n",
    "\n",
    "    faqs = {\n",
    "        'Question':\n",
    "            ['Minimum number of questions?', 'This is the other question?'],\n",
    "        'Answer': ['Two', 'yes']\n",
    "    }\n",
    "\n",
    "    with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "\n",
    "        mock_faq_files(tmpdirname, faqs, data['faq'])\n",
    "        mock_context_file(tmpdirname, contexts, data['context'])\n",
    "\n",
    "        qa_models = load_qa_models(\n",
    "            config_tfidf=data['faq']['config'], download=False\n",
    "        )\n",
    "\n",
    "        question,responses = question_response(data, qa_models)\n",
    "        logging.debug(f'  {question}')\n",
    "        logging.debug(f'  {responses}')\n",
    "        assert 'Who is Enrique Jimenez?' == question\n",
    "        assert 'one of the smartest minds on the planet' in responses\n",
    "\n",
    "test_context_response_with_no_updates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def new_question_answer(data, qa_models):\n",
    "    ''' Asks for a new question-answer pair; store the result in the \n",
    "        faq dataframe and retrain the faq-model\\n\n",
    "    INPUT:\\n\n",
    "    - dictionary of data\n",
    "    - dictionary of question-answer models\n",
    "    OUTPUT:\\n\n",
    "    - None: Updates the dictionaries of data and models\n",
    "    '''\n",
    "    \n",
    "    question = get_input('Introduce question:\\n')\n",
    "\n",
    "    new_faq = pd.DataFrame(\n",
    "        {\n",
    "            'Question': [question],\n",
    "            'Answer': [get_input('Introduce the answer:\\n')]\n",
    "        }\n",
    "    )\n",
    "    data['faq']['df'] = data['faq']['df'].append(new_faq)\n",
    "    data['faq']['df'].to_csv(data['faq']['path'], index=False)\n",
    "    qa_models['faq']['tfidf'] = train_model(\n",
    "        data['faq']['config'], download=False\n",
    "    )\n",
    "    logging.info('FAQ dataset and model updated..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10:01:41 INFO:Restoring parameters from /home/jovyan/.deeppavlov/models/squad_model/model\n",
      "10:02:09 INFO:Restoring parameters from /home/jovyan/.deeppavlov/models/squad_bert/model\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "10:02:20 INFO:FAQ dataset and model updated..\n"
     ]
    }
   ],
   "source": [
    "#tests\n",
    "\n",
    "\n",
    "@patch('__main__.get_input')\n",
    "def test_new_question_answer(mock_input):\n",
    "    question = 'What is Intekglobal?'\n",
    "    new_answer = 'Intekglobal is one of the best companies in the world'\n",
    "    mock_input.side_effect = [question, new_answer]\n",
    "\n",
    "    data = {'context': defaultdict(str), 'faq': defaultdict(str)}\n",
    "\n",
    "    faqs = {\n",
    "        'Question': ['Who  owns Tesla Company?', 'Is this is heaven?'],\n",
    "        'Answer': [\n",
    "            'Elon Musk is the owner of Tesla', 'No, it is life on earth'\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "\n",
    "        mock_faq_files(tmpdirname, faqs, data['faq'])\n",
    "        qa_models = load_qa_models(\n",
    "            config_tfidf=data['faq']['config'], download=False\n",
    "        )\n",
    "        new_question_answer(data, qa_models)\n",
    "        updated_faq = pd.read_csv(data['faq']['path'])\n",
    "\n",
    "        assert updated_faq[updated_faq['Answer'] == new_answer].shape[0] == 1\n",
    "\n",
    "\n",
    "test_new_question_answer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "def new_context(data):\n",
    "    ''' Stores the new context in the context dataframe\\n\n",
    "    INPUT:\\n\n",
    "    - dictionary of data\n",
    "    OUTPUT:\\n\n",
    "    - None: Updates the dictionary of data\n",
    "    '''\n",
    "\n",
    "    new_context = pd.DataFrame(\n",
    "        {\n",
    "            'topic': [get_input('Give context a title:\\n')],\n",
    "            'context': [get_input('Introduce the context:\\n')]\n",
    "        }\n",
    "    )\n",
    "    data['context']['df'] = data['context']['df'].append(new_context)\n",
    "    data['context']['df'].to_csv(data['context']['path'], index=False)\n",
    "\n",
    "    logging.info('contexts dataset updated..')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10:02:20 DEBUG:<function new_context at 0x7f60e148c170>\n",
      "/opt/conda/lib/python3.7/site-packages/pandas/core/frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "10:02:20 INFO:contexts dataset updated..\n"
     ]
    }
   ],
   "source": [
    "@patch('__main__.get_input')\n",
    "def test_new_context(mock_input):\n",
    "    data = {'context': defaultdict(str), 'faq': defaultdict(str)}\n",
    "\n",
    "    new_topic = 'AI Tool & Chatbot Development'\n",
    "    new_context_str = '''\n",
    "\n",
    "A chatbot is an important tool for simulating intelligent conversations with humans.\n",
    "Intekglobal chatbots efficiently live message on platforms such as Facebook Messenger, \n",
    "Slack, and Telegram. But chatbots are more than just a cool technology advancement.\n",
    "\n",
    "'''\n",
    "    contexts = {\n",
    "        'context':\n",
    "            [\n",
    "                '''One of the greatest punk rock bands from all the time\n",
    "                is the Ramones.\n",
    "                '''\n",
    "            ],\n",
    "        'topic': ['Ramones']\n",
    "    }\n",
    "    \n",
    "    mock_input.side_effect = [new_topic, new_context_str]\n",
    "    with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "\n",
    "        \n",
    "        logging.debug(str(new_context))\n",
    "        mock_context_file(tmpdirname, contexts, data['context'])\n",
    "        new_context(data)\n",
    "        updated_faq = pd.read_csv(data['context']['path'])\n",
    "\n",
    "        assert updated_faq[updated_faq.topic == new_topic].shape[0] == 1\n",
    "\n",
    "\n",
    "test_new_context()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def set_minimal_faq_questions(data):\n",
    "    ''' Sets the faq configurations that assure a proper operation.\n",
    "    \n",
    "    If inexistent, a non-empty dataframe for faq is created with 'Question' and 'Answer as columns'\n",
    "    '''\n",
    "    if data['df'].shape[0] > 1:\n",
    "        return\n",
    "    minimal_questions = [\n",
    "        'Is this the Intekglobal Dialog System?',\n",
    "        'What is the purpose of these two automated questions?'\n",
    "    ]\n",
    "    minimal_answers = [\n",
    "        'This is a default reponse of the Dialog System, ' +\n",
    "        'please populate your dataset with better responses',\n",
    "        'The purpose of this automated answer is to initalize the FAQ system, if you are '\n",
    "        +\n",
    "        'seeing this, you probably need to feed your datasets with more samples'\n",
    "    ]\n",
    "    minimal_faqs_df = pd.DataFrame(\n",
    "        {\n",
    "            'Question': minimal_questions,\n",
    "            'Answer': minimal_answers\n",
    "        }\n",
    "    )\n",
    "    data['df'] = pd.concat([data['df'], minimal_faqs_df])\n",
    "    data['df'].to_csv(data['path'], index=False)\n",
    "    logging.info(f' File created at {data[\"path\"]}')\n",
    "\n",
    "\n",
    "def set_minimal_contexts(data):\n",
    "    ''' Sets the context configurations that assure a proper operation.\n",
    "    \n",
    "    If inexistent, a empy dataframe is created with 'topic' and 'context' as columns\n",
    "    '''\n",
    "    if data['df'].shape[0] > 0:\n",
    "        return\n",
    "\n",
    "    minimal_context_df = pd.DataFrame({'topic': [], 'context': []})\n",
    "    data['df'] = minimal_context_df\n",
    "    data['df'].to_csv(data['path'], index=False)\n",
    "\n",
    "    logging.info(f' File created at {data[\"path\"]}')\n",
    "\n",
    "\n",
    "def set_data_dict(file, data, question_type, data_dir):\n",
    "    '''Creates unexistent files\n",
    "    '''\n",
    "\n",
    "    data['path'] = file if file is not None else path.join(\n",
    "        data_dir, question_type + '_data.csv'\n",
    "    )\n",
    "\n",
    "    data['df'] = pd.read_csv(data['path']) if path.isfile(data['path']\n",
    "                                                         ) else pd.DataFrame()\n",
    "\n",
    "    if question_type == 'faq':\n",
    "        set_minimal_faq_questions(data)\n",
    "    if question_type == 'context':\n",
    "        set_minimal_contexts(data)\n",
    "\n",
    "\n",
    "def load_and_prepare_data(context_data_file, faq_data_file, data, configs_faq):\n",
    "    '''Calls the context and faq configuration routines.\n",
    "    \n",
    "    If dataframe files missing, it will create them in data directory.\n",
    "    \n",
    "    If the data frames are provided they must have the following columns  for proper functioning:\n",
    "    \n",
    "    - context: 'topic', 'context'\n",
    "    - faq: 'Question, 'Answer'\n",
    "    '''\n",
    "\n",
    "    PARENT_DIR = popen('$PWD').read().strip()\n",
    "\n",
    "    if faq_data_file or context_data_file is None:\n",
    "        DATA_DIR = path.join(PARENT_DIR, 'data')\n",
    "\n",
    "        if not path.isdir(DATA_DIR):\n",
    "            mkdir(DATA_DIR)\n",
    "            logging.info(f'Data directory created at {DATA_DIR}')\n",
    "\n",
    "    if configs_faq is None:\n",
    "        configs_faq = configs.faq.tfidf_logreg_en_faq\n",
    "\n",
    "    data['faq']['config'] = configs_faq\n",
    "\n",
    "    set_data_dict(\n",
    "        file=faq_data_file,\n",
    "        data=data['faq'],\n",
    "        question_type='faq',\n",
    "        data_dir=DATA_DIR\n",
    "    )\n",
    "    set_data_dict(\n",
    "        file=context_data_file,\n",
    "        data=data['context'],\n",
    "        question_type='context',\n",
    "        data_dir=DATA_DIR\n",
    "    )\n",
    "\n",
    "    updates_faq_config_file(\n",
    "        configs_path=configs_faq,\n",
    "        dataset_reader={'data_path': data['faq']['path']}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10:02:20 INFO: File created at /tmp/tmptuu9vt86/tmp_data.csv\n",
      "10:02:20 INFO: File created at /tmp/tmphqp6ty6p/tmp_data.csv\n",
      "10:02:20 INFO: File created at /tmp/tmpaj9wgtv5/context_data.csv\n",
      "10:02:20 DEBUG:{'context': defaultdict(<class 'str'>, {'path': '/tmp/tmpaj9wgtv5/context_data.csv', 'df': Empty DataFrame\n",
      "Columns: [topic, context]\n",
      "Index: []})}\n",
      "10:02:20 INFO:Data directory created at /tmp/tmp8srz9dtl/data\n",
      "10:02:20 INFO: File created at /tmp/tmp8srz9dtl/data/faq_data.csv\n",
      "10:02:20 INFO: File created at /tmp/tmp8srz9dtl/data/context_data.csv\n"
     ]
    }
   ],
   "source": [
    "#tests\n",
    "import tempfile,logging\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from shutil import rmtree\n",
    "from os import path,popen\n",
    "from unittest.mock import patch\n",
    "\n",
    "\n",
    "def test_set_minimal_faqs_with_more_than_one_question():\n",
    "    with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "        data_file = path.join(tmpdirname, 'tmp_data.csv')\n",
    "        questions = ['a?', 'b?']\n",
    "        answers = ['a', 'b']\n",
    "        df = pd.DataFrame({'Question': questions, 'Answer': answers})\n",
    "        df.to_csv(data_file, index=False)\n",
    "        data = {'df': df, 'path': data_file}\n",
    "        set_minimal_faq_questions(data)\n",
    "\n",
    "        assert data['df'].shape[0] == 2\n",
    "\n",
    "\n",
    "def test_set_minimal_faqs_with_less_than_two_questions():\n",
    "    with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "        data_file = path.join(tmpdirname, 'tmp_data.csv')\n",
    "        questions = ['a?']\n",
    "        answers = ['a']\n",
    "        df = pd.DataFrame({'Question': questions, 'Answer': answers})\n",
    "        df.to_csv(data_file, index=False)\n",
    "        data = {'df': df, 'path': data_file}\n",
    "\n",
    "        assert data['df'].shape[0] == 1\n",
    "\n",
    "        set_minimal_faq_questions(data)\n",
    "\n",
    "        assert data['df'].shape[0] == 3\n",
    "        assert any(\n",
    "            data['df'].Question == 'Is this the Intekglobal Dialog System?'\n",
    "        )\n",
    "\n",
    "\n",
    "def test_set_minimal_contexts():\n",
    "    with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "        data_file = path.join(tmpdirname, 'tmp_data.csv')\n",
    "        data = {'df': pd.DataFrame(), 'path': data_file}\n",
    "        set_minimal_contexts(data)\n",
    "        assert path.isfile(data['path'])\n",
    "        assert all(data['df'].columns == ['topic', 'context'])\n",
    "\n",
    "\n",
    "def test_set_data_dict_no_file():\n",
    "    with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "        data = {'context': defaultdict(str)}\n",
    "        set_data_dict(\n",
    "            file=None,\n",
    "            data=data['context'],\n",
    "            data_dir=tmpdirname,\n",
    "            question_type='context'\n",
    "        )\n",
    "        logging.debug(data)\n",
    "        assert path.isfile(data['context']['path'])\n",
    "\n",
    "\n",
    "@patch('__main__.popen')\n",
    "def test_load_and_prepare_data(mock_popen):\n",
    "    with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "        mock_popen(\"$PWD\").read().strip.side_effect = [tmpdirname]\n",
    "        data = {'context': defaultdict(str), 'faq': defaultdict(str)}\n",
    "\n",
    "        load_and_prepare_data(\n",
    "            context_data_file=None,\n",
    "            faq_data_file=None,\n",
    "            data=data,\n",
    "            configs_faq=None\n",
    "        )\n",
    "        data_dir = path.join(tmpdirname, 'data')\n",
    "        assert path.isdir(data_dir)\n",
    "\n",
    "\n",
    "test_set_minimal_faqs_with_more_than_one_question()\n",
    "test_set_minimal_faqs_with_less_than_two_questions()\n",
    "test_set_minimal_contexts()\n",
    "test_set_data_dict_no_file()\n",
    "test_load_and_prepare_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
