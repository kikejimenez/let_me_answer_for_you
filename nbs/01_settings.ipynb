{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "!pip install -r ../requirements.txt > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "from deeppavlov.core.common.paths import get_settings_path\n",
    "from deeppavlov import configs, build_model, train_model\n",
    "import json\n",
    "from os import path, popen, mkdir\n",
    "from shutil import copyfile\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from pathlib import Path\n",
    "import logging\n",
    "from collections import defaultdict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10:42:33 ERROR: Error Log Active \n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "logging.basicConfig(\n",
    "    #filename='example.log',\n",
    "    format='%(asctime)s %(levelname)s:%(message)s',\n",
    "    level=logging.DEBUG,\n",
    "    datefmt='%I:%M:%S'\n",
    ")\n",
    "\n",
    "logging.info(\"Hello! Welcome to our automated dialog system!\")\n",
    "logging.debug(\" Debug Log Active\")\n",
    "logging.warning(' Warning Log Active')\n",
    "logging.error(' Error Log Active ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def change_log_config():\n",
    "    settings_file = path.join(get_settings_path(), 'log_config.json')\n",
    "    #logs_key = 'disable_existing_loggers'\n",
    "\n",
    "    settings_json = json.load(open(settings_file)) \n",
    "    settings_json['handlers']['file']['level'] = 'ERROR'\n",
    "    settings_json['handlers']['stderr']['level'] = 'ERROR'\n",
    "    settings_json['handlers']['stdout']['level'] = 'ERROR'\n",
    "    settings_json['handlers']['uvicorn_handler']['level'] = 'ERROR'\n",
    "    \n",
    "    settings_json['loggers']['deeppavlov']['level'] ='ERROR'\n",
    "    settings_json['loggers']['deeppavlov']['propagate'] = True\n",
    "    \n",
    "    settings_json['loggers']['uvicorn.access']['level'] ='ERROR'\n",
    "    settings_json['loggers']['uvicorn.access']['propagate'] = True\n",
    "    \n",
    "    settings_json['loggers']['uvicorn.error']['level'] ='ERROR'\n",
    "    settings_json['loggers']['uvicorn.error']['propagate'] = True\n",
    "\n",
    "    #settings_json[logs_key] = False\n",
    "    \n",
    "    json.dump(settings_json, open(settings_file, 'w'))\n",
    "\n",
    "\n",
    "def run_shell_installs():\n",
    "    ''' Run install commands\n",
    "    '''\n",
    "    logging.info(f'..Installing NLP libraries')\n",
    "    change_log_config()\n",
    "\n",
    "    command_strings = (\n",
    "        ' pip install deeppavlov', ' python -m deeppavlov install squad',\n",
    "        ' python -m deeppavlov install squad_bert',\n",
    "        ' python -m deeppavlov install fasttext_avg_autofaq',\n",
    "        ' python -m deeppavlov install fasttext_tfidf_autofaq',\n",
    "        ' python -m deeppavlov install tfidf_autofaq',\n",
    "        ' python -m deeppavlov install tfidf_logreg_autofaq ',\n",
    "        ' python -m deeppavlov install tfidf_logreg_en_faq'\n",
    "    )\n",
    "    for command in command_strings:\n",
    "\n",
    "        logging.debug(command)\n",
    "        logging.debug(popen(command).read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dialog System\n",
    "> Question Answering Automated Dialog System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_shell_installs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def action_over_list_f(arr, v):\n",
    "\n",
    "    k_id, v_id = next(iter(v[0].items()))\n",
    "\n",
    "    for p, a in enumerate(arr):\n",
    "        if k_id in a.keys() and a[k_id] == v_id:\n",
    "            for k_rep, v_rep in v[1].items():\n",
    "                arr[p][k_rep] = v_rep\n",
    "\n",
    "\n",
    "def replacement_f(model_config, **args):\n",
    "    '''Replaces the model config dictionary with new values\n",
    "    '''\n",
    "    for k, v in args.items():\n",
    "        if isinstance(v, dict):\n",
    "            replacement_f(model_config[k], **v)\n",
    "        if isinstance(v, str):\n",
    "            model_config[k] = v\n",
    "        if isinstance(model_config[k], list):\n",
    "            action_over_list_f(model_config[k], v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test action_over_list_f\n",
    "from random import randint\n",
    "\n",
    "\n",
    "def gen_list_keys_for_tests():\n",
    "\n",
    "    str_n = lambda x: f'{x}_{randint(1,10):1}'\n",
    "    gen_dict_list = lambda: {\n",
    "        'id': str_n('id'),\n",
    "        'key1': str_n('v1'),\n",
    "        'key2': str_n('v2'),\n",
    "        'key3': str_n('v3')\n",
    "    }\n",
    "\n",
    "    pipe_list = [gen_dict_list() for _ in range(randint(3, 10))]\n",
    "\n",
    "    rand_id = pipe_list[randint(0, len(pipe_list) - 1)]['id']\n",
    "    rand_key = f'key{randint(1, 3)}' \n",
    "\n",
    "    new_rand_val = str_n('new')\n",
    "    args = {\n",
    "        'chains': {\n",
    "            'pipe': [{\n",
    "                'id': rand_id\n",
    "            }, {\n",
    "               rand_key : new_rand_val\n",
    "            }]\n",
    "        }\n",
    "    }\n",
    "\n",
    "    return pipe_list, rand_id, rand_key, args, new_rand_val\n",
    "\n",
    "\n",
    "def test_action_over_list_f():\n",
    "\n",
    "    pipe_list, rand_id, rand_key, args, new_rand_val = gen_list_keys_for_tests()\n",
    "\n",
    "    assert all(\n",
    "        new_rand_val not in pipe_elem.values() for pipe_elem in pipe_list\n",
    "    )\n",
    "\n",
    "    action_over_list_f(pipe_list, args['chains']['pipe'])\n",
    "\n",
    "    assert any(\n",
    "        rand_key in pipe_elem.keys() and\n",
    "        new_rand_val in pipe_elem.values() for pipe_elem in pipe_list\n",
    "    )\n",
    "\n",
    "\n",
    "def test_replacement_f_list():\n",
    "\n",
    "    pipe_list, rand_id, rand_key, args, new_rand_val = gen_list_keys_for_tests()\n",
    "\n",
    "    mod_conf = {'chains': {'pipe': pipe_list}}\n",
    "\n",
    "    assert all(\n",
    "        new_rand_val not in pipe_elem.values()\n",
    "        for pipe_elem in mod_conf['chains']['pipe']\n",
    "    )\n",
    "\n",
    "    replacement_f(model_config=mod_conf, **args)\n",
    "    assert any(\n",
    "        rand_key in pipe_elem.keys() and\n",
    "        new_rand_val in pipe_elem.values()\n",
    "        for pipe_elem in mod_conf['chains']['pipe']\n",
    "    )\n",
    "\n",
    "\n",
    "def test_replacement_f_val():\n",
    "    args = {'key3': 'newvalue'}\n",
    "    mod_conf = {'key1': 'val1', 'key2': 'val2', 'key3': 'val3'}\n",
    "    replacement_f(model_config=mod_conf, **args)\n",
    "    assert all(\n",
    "        arg_k in mod_conf.keys() and arg_v in mod_conf.values()\n",
    "        for arg_k, arg_v in args.items()\n",
    "    )\n",
    "\n",
    "\n",
    "def test_replacement_f_dict():\n",
    "    args = {'1_key_3': {'2_key_2': 'newvalue'}}\n",
    "    mod_conf = {'1_key_3': {'2_key_2': 'oldvalue'}, '0_key_': '0_val'}\n",
    "    replacement_f(model_config=mod_conf, **args)\n",
    "    assert mod_conf['1_key_3']['2_key_2'] == 'newvalue'\n",
    "\n",
    "\n",
    "test_action_over_list_f()\n",
    "test_replacement_f_list()\n",
    "test_replacement_f_val()\n",
    "test_replacement_f_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def updates_faq_config_file(\n",
    "    configs_path,\n",
    "    **args\n",
    "):\n",
    "    '''Updates deepplavov json config file \n",
    "    '''\n",
    "    #set FAQ data in config file\n",
    "    model_config = json.load(open(configs_path))\n",
    "\n",
    "    if 'data_url' in model_config['dataset_reader']:\n",
    "        del model_config['dataset_reader']['data_url']\n",
    "\n",
    "    replacement_f(model_config=model_config,**args)\n",
    "\n",
    "    json.dump(model_config, open(configs_path, 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test updates_faq_config_file\n",
    "import tempfile\n",
    "from shutil import copyfile\n",
    "\n",
    "\n",
    "def gen_list_keys_for_tests():\n",
    "\n",
    "    str_n = lambda x: f'{x}_{randint(1,10):1}'\n",
    "    gen_dict_list = lambda: {\n",
    "        'id': str_n('id'),\n",
    "        'key1': str_n('v1'),\n",
    "        'key2': str_n('v2'),\n",
    "        'key3': str_n('v3')\n",
    "    }\n",
    "\n",
    "    pipe_list = [gen_dict_list() for _ in range(randint(3, 10))]\n",
    "\n",
    "    rand_id = pipe_list[randint(0, len(pipe_list) - 1)]['id']\n",
    "    rand_key =  f'key{randint(1, 3)}' \n",
    "\n",
    "    new_rand_val = str_n('new')\n",
    "    pipe_dict = {'pipe': [{'id': rand_id}, {rand_key: new_rand_val}]}\n",
    "    args = {'chainer': pipe_dict}\n",
    "\n",
    "    return pipe_list, rand_id, rand_key, args, new_rand_val\n",
    "\n",
    "\n",
    "def test_updates_faq_config_file_update_string():\n",
    "\n",
    "    with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "\n",
    "        tmp_config_file = path.join(tmpdirname, 'tmp_file.json')\n",
    "\n",
    "        copyfile(configs.faq.tfidf_logreg_en_faq, tmp_config_file)\n",
    "\n",
    "        assert path.isfile(tmp_config_file)\n",
    "\n",
    "        updates_faq_config_file(\n",
    "            configs_path=tmp_config_file,\n",
    "            dataset_reader={'data_path': 'fictional_csv_file.csv'}\n",
    "        )\n",
    "\n",
    "        config_json = json.load(open(tmp_config_file))\n",
    "        assert 'data_path' in config_json['dataset_reader']\n",
    "\n",
    "\n",
    "def test_updates_faq_config_file_update_list():\n",
    "\n",
    "    with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "\n",
    "        tmp_config_file = path.join(tmpdirname, 'tmp_file.json')\n",
    "\n",
    "        pipe_list, rand_id, rand_key, args, new_rand_val = gen_list_keys_for_tests(\n",
    "        )\n",
    "        mod_conf = {\n",
    "            'chainer': {\n",
    "                'pipe': pipe_list\n",
    "            },\n",
    "            'dataset_reader': 'dataset_reader_dictionary'\n",
    "        }\n",
    "\n",
    "        json.dump(mod_conf, open(tmp_config_file, 'w'))\n",
    "\n",
    "        assert path.isfile(tmp_config_file)\n",
    "\n",
    "        updates_faq_config_file(configs_path=tmp_config_file, **args)\n",
    "\n",
    "        config_json = json.load(open(tmp_config_file))\n",
    "   \n",
    "        assert any(\n",
    "            rand_key in pipe_elem.keys() and new_rand_val in pipe_elem.values()\n",
    "            for pipe_elem in config_json['chainer']['pipe']\n",
    "        )\n",
    "\n",
    "\n",
    "test_updates_faq_config_file_update_string()\n",
    "test_updates_faq_config_file_update_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def select_faq_responses(faq_model, question):\n",
    "    '''Calls Deeppavlov FAQ model\n",
    "    '''\n",
    "    return faq_model([question])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12.3k/12.3k [00:00<00:00, 39.2MB/s]\n",
      "[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package perluniprops to\n",
      "[nltk_data]     /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package perluniprops is already up-to-date!\n",
      "[nltk_data] Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]     /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package nonbreaking_prefixes is already up-to-date!\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "100%|██████████| 12.3k/12.3k [00:00<00:00, 22.2MB/s]\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "100%|██████████| 12.3k/12.3k [00:00<00:00, 13.6MB/s]\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "#test faq responses\n",
    "import tempfile\n",
    "from shutil import copyfile\n",
    "\n",
    "\n",
    "def gen_mock_csv_file(tmpdirname, faqs):\n",
    "\n",
    "    temp_faq_csv = path.join(tmpdirname, 'tmp_faq.csv')\n",
    "\n",
    "    pd.DataFrame(faqs).to_csv(temp_faq_csv, index=False)\n",
    "\n",
    "    return temp_faq_csv\n",
    "\n",
    "\n",
    "def gen_mock_vocab_answers(tmpdirname, vocabs):\n",
    "\n",
    "    temp_dict_file = path.join(tmpdirname, 'temp_vocab_answers.dict')\n",
    "    vocabs_text = '\\n'.join(\n",
    "        t + '\\t' + str(f) for t, f in zip(vocabs['text'], vocabs['freq'])\n",
    "    )\n",
    "\n",
    "    f = open(temp_dict_file, 'w')\n",
    "    f.write(vocabs_text)\n",
    "    f.close()\n",
    "\n",
    "    return temp_dict_file\n",
    "\n",
    "\n",
    "def gen_faq_config(tmpdirname, vocab_file, faq_file):\n",
    "\n",
    "    temp_configs_faq = path.join(tmpdirname, 'temp_config_faq.json')\n",
    "    copyfile(configs.faq.tfidf_logreg_en_faq, temp_configs_faq)\n",
    "\n",
    "    changes_dict = {'save_path': vocab_file, 'load_path': vocab_file}\n",
    "    id_dict = {'id': 'answers_vocab'}\n",
    "\n",
    "    updates_faq_config_file(\n",
    "        configs_path=temp_configs_faq,\n",
    "        chainer={'pipe': [id_dict, changes_dict]},\n",
    "        dataset_reader={'data_path': faq_file}\n",
    "    )\n",
    "\n",
    "    return temp_configs_faq\n",
    "\n",
    "\n",
    "def test_faq_response_with_minimum_faqs_in_dataframe_fail_case():\n",
    "    with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "\n",
    "        faqs = {\n",
    "            'Question': ['Is Covid erradicated?'],\n",
    "            'Answer': ['Definitely not!']\n",
    "        }\n",
    "\n",
    "        vocabs = {'text': ['This is a vocab example'], 'freq': [1]}\n",
    "\n",
    "        faq_file = gen_mock_csv_file(tmpdirname, faqs)\n",
    "        vocab_file = gen_mock_vocab_answers(tmpdirname, vocabs)\n",
    "\n",
    "        configs_file = gen_faq_config(tmpdirname, vocab_file, faq_file)\n",
    "\n",
    "        try:\n",
    "            select_faq_responses(\n",
    "                question='Is Enrique the prettiest person in town?',\n",
    "                faq_model=train_model(configs_file, download=True)\n",
    "            )\n",
    "            assert False\n",
    "        except ValueError as e:\n",
    "            assert True\n",
    "\n",
    "\n",
    "def test_faq_response_with_minimum_faqs_in_dataframe_success_case():\n",
    "    with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "\n",
    "        faqs = {\n",
    "            'Question': ['Is Covid erradicated?', 'Who is the current POTUS?'],\n",
    "            'Answer': ['Definitely not!', 'Donald Trump']\n",
    "        }\n",
    "\n",
    "        vocabs = {'text': ['This is a vocab example'], 'freq': [1]}\n",
    "\n",
    "        faq_file = gen_mock_csv_file(tmpdirname, faqs)\n",
    "        vocab_file = gen_mock_vocab_answers(tmpdirname, vocabs)\n",
    "\n",
    "        configs_file = gen_faq_config(tmpdirname, vocab_file, faq_file)\n",
    "\n",
    "        assert select_faq_responses(\n",
    "            question='Is Enrique the prettiest person in town?',\n",
    "            faq_model=train_model(configs_file, download=True)\n",
    "        ) == ['Donald Trump']\n",
    "\n",
    "        \n",
    "        \n",
    "def test_faq_response_with_minimum_answers_vocab_success_case():\n",
    "    with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "\n",
    "        faqs = {\n",
    "            'Question': ['Is Covid erradicated?', 'Who is the current POTUS?'],\n",
    "            'Answer': ['Definitely not!', 'Donald Trump']\n",
    "        }\n",
    "\n",
    "        vocabs = {'text': [], 'freq': []}\n",
    "\n",
    "        faq_file = gen_mock_csv_file(tmpdirname, faqs)\n",
    "        vocab_file = gen_mock_vocab_answers(tmpdirname, vocabs)\n",
    "\n",
    "        configs_file = gen_faq_config(tmpdirname, vocab_file, faq_file)\n",
    "\n",
    "        select_faq_responses(\n",
    "            question='Is Enrique the prettiest person in town?',\n",
    "            faq_model=train_model(configs_file, download=True)\n",
    "        ) == ['Donald Trump']\n",
    "\n",
    "test_faq_response_with_minimum_faqs_in_dataframe_fail_case()\n",
    "test_faq_response_with_minimum_faqs_in_dataframe_success_case()\n",
    "test_faq_response_with_minimum_answers_vocab_success_case()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def select_squad_responses(\n",
    "    contexts, squad_model, question, best_results=1\n",
    "):\n",
    "    '''Calls Deeppavlov BERT and RNET Context Question Answering\n",
    "    '''\n",
    "    responses = contexts.context.apply(\n",
    "        lambda context: squad_model([context], [question])\n",
    "    ).values\n",
    "    \n",
    "    logging.debug(f'Responses: {responses}')\n",
    "    top_responses = [\n",
    "        r[0][0] for r in sorted(responses, key=lambda x: -1 * x[2][0])\n",
    "        [:best_results]\n",
    "    ]\n",
    "\n",
    "    logging.debug(f'Top Responses: {top_responses}')\n",
    "    return responses, top_responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test select_squad_responses\n",
    "import tempfile\n",
    "from shutil import copyfile\n",
    "\n",
    "empty = {'topic': [], 'context': []}\n",
    "spacex = {\n",
    "    'topic': ['SpaceX'],\n",
    "    'context':\n",
    "        [\n",
    "            '''Space Exploration Technologies Corp., trading as SpaceX, is an American aerospace manufacturer and space transportation\n",
    "services company headquartered in Hawthorne, California. It was founded in 2002 by Elon Musk with the goal of reducing space \n",
    "transportation costs to enable the colonization of Mars. SpaceX has developed several launch vehicles, the Starlink satellite\n",
    "constellation, and the Dragon spacecraft. It is widely considered among the most successful private spaceflight companies.'''\n",
    "        ]\n",
    "}\n",
    "\n",
    "intekglobal = {\n",
    "    'topic': ['Intekglobal', 'InG'],\n",
    "    'context':\n",
    "        [\n",
    "            'Intekglobal has its headquarters located in TJ',\n",
    "            'Intekglobal is in the north of mexico'\n",
    "        ]\n",
    "}\n",
    "\n",
    "\n",
    "def assert_squad_model(\n",
    "    contexts, squad_model, question, expected_responses, **args\n",
    "):\n",
    "    responses, top_responses = select_squad_responses(\n",
    "        contexts=pd.DataFrame(contexts),\n",
    "        squad_model=squad_model,\n",
    "        question=question,\n",
    "        **args\n",
    "    )\n",
    "    assert top_responses == expected_responses\n",
    "\n",
    "\n",
    "def test_squad_bert():\n",
    "\n",
    "    bert = build_model(configs.squad.squad_bert, download=True)\n",
    "\n",
    "    assert_squad_model(\n",
    "        empty,\n",
    "        bert,\n",
    "        'Is an empty response expected?',\n",
    "        expected_responses=[],\n",
    "        best_results=2\n",
    "    )\n",
    "\n",
    "    assert_squad_model(\n",
    "        spacex, bert, 'Who founded SpaceX?', expected_responses=['Elon Musk']\n",
    "    )\n",
    "\n",
    "    assert_squad_model(\n",
    "        intekglobal,\n",
    "        bert,\n",
    "        'Where is Intekglobal located?',\n",
    "        expected_responses=['north of mexico','TJ'],\n",
    "        best_results=2\n",
    "    )\n",
    "\n",
    "\n",
    "def test_squad_rnet():\n",
    "\n",
    "    bert = build_model(configs.squad.squad, download=True)\n",
    "\n",
    "    assert_squad_model(\n",
    "        empty,\n",
    "        bert,\n",
    "        'Is an empty response expected?',\n",
    "        expected_responses=[],\n",
    "        best_results=5\n",
    "    )\n",
    "\n",
    "    assert_squad_model(\n",
    "        spacex, bert, 'Who founded SpaceX?', expected_responses=['Elon Musk']\n",
    "    )\n",
    "\n",
    "    assert_squad_model(\n",
    "        intekglobal,\n",
    "        bert,\n",
    "        'Where is Intekglobal located?',\n",
    "        expected_responses=['north of mexico','TJ'],\n",
    "        best_results=2\n",
    "    )\n",
    "\n",
    "test_squad_bert()\n",
    "test_squad_rnet()\n",
    "del spacex, empty, intekglobal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def load_qa_models(\n",
    "    config_rnet=configs.squad.squad,\n",
    "    config_bert=configs.squad.squad_bert,\n",
    "    config_tfidf=configs.faq.tfidf_logreg_en_faq,\n",
    "    download=True\n",
    "):\n",
    "    qa_models = {\n",
    "        'squad':\n",
    "            {\n",
    "                'rnet': build_model(config_rnet, download=download),\n",
    "                'bert': build_model(config_bert, download=download)\n",
    "            },\n",
    "        'faq': {\n",
    "            'tfidf': train_model(config_tfidf, download=download)\n",
    "        }\n",
    "    }\n",
    "    return qa_models\n",
    "\n",
    "\n",
    "def format_responses(question, responses):\n",
    "    formatted_response = f'{question}:\\n\\n'\n",
    "    for k, res in enumerate(responses):\n",
    "        formatted_response += f'{k+1}: {res}\\n'\n",
    "    return formatted_response\n",
    "\n",
    "\n",
    "def get_responses(contexts, question, qa_models, nb_squad_results=1):\n",
    "    responses = []\n",
    "    for squad_model in qa_models['squad'].values():\n",
    "        responses.extend(\n",
    "            select_squad_responses(\n",
    "                contexts, squad_model, question, best_results=nb_squad_results\n",
    "            )[1]\n",
    "        )\n",
    "    for faq_model in qa_models['faq'].values():\n",
    "        responses.extend(select_faq_responses(faq_model, question))\n",
    "    return responses, format_responses(\n",
    "        question, set([r for r in responses if r.strip()])\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# test get_responses\n",
    "import tempfile\n",
    "from shutil import copyfile\n",
    "\n",
    "intekglobal_context = {\n",
    "    'topic': ['Intekglobal', 'InG'],\n",
    "    'context':\n",
    "        [\n",
    "            'Intekglobal has its headquarters located in TJ',\n",
    "            'Intekglobal is in the north of mexico'\n",
    "        ]\n",
    "}\n",
    "\n",
    "intekglobal_faqs = {\n",
    "    'Question': ['Is Intekglobal an IT company?', 'Where can I apply?'],\n",
    "    'Answer':\n",
    "        ['Yes it is!', 'Please refer the our website for further information']\n",
    "}\n",
    "\n",
    "\n",
    "def mock_faq_files(tmpdirname, faqs):\n",
    "\n",
    "    faq_files = {\n",
    "        'data': path.join(tmpdirname, 'temp_faq.csv'),\n",
    "        'config': path.join(tmpdirname, 'temp_config_faq.json')\n",
    "    }\n",
    "\n",
    "    pd.DataFrame(faqs).to_csv(faq_files['data'], index=False)\n",
    "    copyfile(configs.faq.tfidf_logreg_en_faq, faq_files['config'])\n",
    "\n",
    "    updates_faq_config_file(\n",
    "        configs_path=faq_files['config'],\n",
    "        dataset_reader={'data_path': faq_files['data']}\n",
    "    )\n",
    "\n",
    "    return faq_files\n",
    "\n",
    "\n",
    "def test_get_intekglobal_responses():\n",
    "    with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "\n",
    "        faq_files = mock_faq_files(tmpdirname, intekglobal_faqs)\n",
    "        qa_models = load_qa_models(\n",
    "            config_tfidf=faq_files['config'], download=False\n",
    "        )\n",
    "\n",
    "        responses, format_responses = get_responses(\n",
    "            pd.DataFrame(intekglobal_context),\n",
    "            'Where is Intekglobal?',\n",
    "            qa_models,\n",
    "            nb_squad_results=2\n",
    "        )\n",
    "\n",
    "        logging.debug(responses)\n",
    "        logging.debug(format_responses)\n",
    "        assert all(\n",
    "            response in ('north of mexico', 'TJ', 'Yes it is!')\n",
    "            for response in responses\n",
    "        )\n",
    "\n",
    "\n",
    "def test_get_responses_with_empty_context():\n",
    "    with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "        min_faqs = {'Question': ['Minimum number of questions?','This is the other question?'], 'Answer': ['Two','yes']}\n",
    "        faq_files = mock_faq_files(tmpdirname, min_faqs)\n",
    "\n",
    "        qa_models = load_qa_models(\n",
    "            config_tfidf=faq_files['config'], download=False\n",
    "        )\n",
    "        empty_context = {'topic': [], 'context': []}\n",
    "\n",
    "        responses, format_responses = get_responses(\n",
    "            pd.DataFrame(empty_context),\n",
    "            'What is the minimun number of FAQ questions',\n",
    "            qa_models,\n",
    "            nb_squad_results=2\n",
    "        )\n",
    "\n",
    "        logging.debug(responses)\n",
    "        logging.debug(format_responses)\n",
    "        assert responses == ['Two']\n",
    "\n",
    "\n",
    "test_get_intekglobal_responses()\n",
    "test_get_responses_with_empty_context()\n",
    "\n",
    "del intekglobal_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_input(text):\n",
    "    '''This redundancy is needed for testing'''\n",
    "    return input(text)\n",
    "\n",
    "\n",
    "def question_response(data, qa_models, num_returned_values_per_squad_model=1):\n",
    "    question = get_input('Introduce question:\\n')\n",
    "\n",
    "    _, formatted_responses = get_responses(\n",
    "        data['context']['df'], question, qa_models, nb_squad_results=1\n",
    "    )\n",
    "    \n",
    "    return question, formatted_responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Test FAQ dialog system's part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import tempfile\n",
    "from unittest.mock import patch\n",
    "from shutil import copyfile\n",
    "from collections import defaultdict\n",
    "\n",
    "def mock_faq_files(tmpdirname, faqs, faq_dic):\n",
    "\n",
    "    faq_dic['path'] = path.join(tmpdirname, 'temp_faq.csv')\n",
    "    faq_dic['config'] = path.join(tmpdirname, 'temp_config_faq.json')\n",
    "    faq_dic['df'] = pd.DataFrame(faqs)\n",
    "    faq_dic['df'].to_csv(faq_dic['path'], index=False)\n",
    "\n",
    "    copyfile(configs.faq.tfidf_logreg_en_faq, faq_dic['config'])\n",
    "\n",
    "    updates_faq_config_file(\n",
    "        configs_path=faq_dic['config'],\n",
    "        dataset_reader={'data_path': faq_dic['path']}\n",
    "    )\n",
    "\n",
    "\n",
    "def mock_context_file(tmpdirname, contexts, context_dic):\n",
    "\n",
    "    context_dic['path'] = path.join(tmpdirname, 'temp_context.csv')\n",
    "    context_dic['df'] = pd.DataFrame(contexts)\n",
    "    context_dic['df'].to_csv(context_dic['path'], index=False)\n",
    "\n",
    "\n",
    "@patch('__main__.get_input')\n",
    "def test_context_response_with_no_updates(mock_input):\n",
    "    mock_input.side_effect = ['Who is Enrique Jimenez?']\n",
    "    data = {'context': defaultdict(str), 'faq': defaultdict(str)}\n",
    "    contexts = {\n",
    "        'context':\n",
    "            [\n",
    "                'Intekglobal has its headquarters located in TJ',\n",
    "                'In Intekglobal we care about you',\n",
    "                '''Enrique Jimenez is one of the smartest minds on the planet, \n",
    "                   he currently works as Intekglobal employee'''\n",
    "            ],\n",
    "        'topic': ['headquarters', 'mission', 'Enrique\\'s biography']\n",
    "    }\n",
    "\n",
    "    faqs = {\n",
    "        'Question':\n",
    "            ['Minimum number of questions?', 'This is the other question?'],\n",
    "        'Answer': ['Two', 'yes']\n",
    "    }\n",
    "\n",
    "    with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "\n",
    "        mock_faq_files(tmpdirname, faqs, data['faq'])\n",
    "        mock_context_file(tmpdirname, contexts, data['context'])\n",
    "\n",
    "        qa_models = load_qa_models(\n",
    "            config_tfidf=data['faq']['config'], download=False\n",
    "        )\n",
    "\n",
    "        question,responses = question_response(data, qa_models)\n",
    "        logging.debug(f'  {question}')\n",
    "        logging.debug(f'  {responses}')\n",
    "        assert 'Who is Enrique Jimenez?' == question\n",
    "        assert 'one of the smartest minds on the planet' in responses\n",
    "\n",
    "test_context_response_with_no_updates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def new_question_answer(data, qa_models):\n",
    "    \n",
    "    question = get_input('Introduce question:\\n')\n",
    "\n",
    "    new_faq = pd.DataFrame(\n",
    "        {\n",
    "            'Question': [question],\n",
    "            'Answer': [get_input('Introduce the answer:\\n')]\n",
    "        }\n",
    "    )\n",
    "    data['faq']['df'] = data['faq']['df'].append(new_faq)\n",
    "    data['faq']['df'].to_csv(data['faq']['path'], index=False)\n",
    "    qa_models['faq']['tfidf'] = train_model(\n",
    "        data['faq']['config'], download=False\n",
    "    )\n",
    "    logging.info('FAQ dataset and model updated..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-075ed7dfea65>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0mtest_new_question_answer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.7/unittest/mock.py\u001b[0m in \u001b[0;36mpatched\u001b[0;34m(*args, **keywargs)\u001b[0m\n\u001b[1;32m   1253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1254\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextra_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1255\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkeywargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1256\u001b[0m             \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1257\u001b[0m                 if (patching not in entered_patchers and\n",
      "\u001b[0;32m<ipython-input-24-075ed7dfea65>\u001b[0m in \u001b[0;36mtest_new_question_answer\u001b[0;34m(mock_input)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mmock_faq_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmpdirname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfaqs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'faq'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         qa_models = load_qa_models(\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0mconfig_tfidf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'faq'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'config'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         )\n\u001b[1;32m     25\u001b[0m         \u001b[0mnew_question_answer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqa_models\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-b11d69a61bed>\u001b[0m in \u001b[0;36mload_qa_models\u001b[0;34m(config_rnet, config_bert, config_tfidf, download)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;34m'squad'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             {\n\u001b[0;32m---> 11\u001b[0;31m                 \u001b[0;34m'rnet'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbuild_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig_rnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m                 \u001b[0;34m'bert'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbuild_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig_bert\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             },\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/deeppavlov/core/commands/infer.py\u001b[0m in \u001b[0;36mbuild_model\u001b[0;34m(config, mode, load_trained, download, serialized)\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0mcomponent_serialized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m         \u001b[0mcomponent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfrom_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomponent_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mserialized\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcomponent_serialized\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'id'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcomponent_config\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/deeppavlov/core/common/params.py\u001b[0m in \u001b[0;36mfrom_params\u001b[0;34m(params, mode, serialized, **kwargs)\u001b[0m\n\u001b[1;32m    104\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mode'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m             \u001b[0mcomponent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m                 \u001b[0m_refs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mconfig_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcomponent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/deeppavlov/core/models/tf_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m                     \u001b[0mwrapped_attr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_graph_wrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m                 \u001b[0msetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapped_attr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m         \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/deeppavlov/core/models/tf_backend.py\u001b[0m in \u001b[0;36m_wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_wrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/deeppavlov/models/squad/squad.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, word_emb, char_emb, context_limit, question_limit, char_limit, train_char_emb, char_hidden_size, encoder_hidden_size, attention_hidden_size, keep_prob, min_learning_rate, noans_token, **kwargs)\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msess_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_optimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/deeppavlov/core/models/tf_backend.py\u001b[0m in \u001b[0;36m_wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_wrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/deeppavlov/models/squad/squad.py\u001b[0m in \u001b[0;36m_init_graph\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariable_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"attention\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m             qc_att = dot_attention(c, q, mask=self.q_mask, att_size=self.attention_hidden_size,\n\u001b[0;32m--> 177\u001b[0;31m                                    keep_prob=self.keep_prob_ph)\n\u001b[0m\u001b[1;32m    178\u001b[0m             rnn = self.GRU(num_layers=1, num_units=self.hidden_size, batch_size=bs,\n\u001b[1;32m    179\u001b[0m                            input_size=qc_att.get_shape().as_list()[-1], keep_prob=self.keep_prob_ph)\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/deeppavlov/models/squad/utils.py\u001b[0m in \u001b[0;36mdot_attention\u001b[0;34m(inputs, memory, mask, att_size, keep_prob, scope)\u001b[0m\n\u001b[1;32m    180\u001b[0m             \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m             \u001b[0md_res\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_prob\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeep_prob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnoise_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mBS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIH\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mMH\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m             \u001b[0mgate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_res\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_bias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mgate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/util/deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    322\u001b[0m               \u001b[0;34m'in a future version'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'after %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m               instructions)\n\u001b[0;32m--> 324\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m     return tf_decorator.make_decorator(\n\u001b[1;32m    326\u001b[0m         \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'deprecated'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/layers/core.py\u001b[0m in \u001b[0;36mdense\u001b[0;34m(inputs, units, activation, use_bias, kernel_initializer, bias_initializer, kernel_regularizer, bias_regularizer, activity_regularizer, kernel_constraint, bias_constraint, trainable, name, reuse)\u001b[0m\n\u001b[1;32m    185\u001b[0m                 \u001b[0m_scope\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m                 _reuse=reuse)\n\u001b[0;32m--> 187\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/util/deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    322\u001b[0m               \u001b[0;34m'in a future version'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'after %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m               instructions)\n\u001b[0;32m--> 324\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m     return tf_decorator.make_decorator(\n\u001b[1;32m    326\u001b[0m         \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'deprecated'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1698\u001b[0m       \u001b[0mOutput\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1699\u001b[0m     \"\"\"\n\u001b[0;32m-> 1700\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1702\u001b[0m   @deprecation.deprecated(\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/layers/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    546\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m       \u001b[0;31m# Actually call layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 548\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    549\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    822\u001b[0m           \u001b[0;31m# Build layer if applicable (if the `build` method has been\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    823\u001b[0m           \u001b[0;31m# overridden).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 824\u001b[0;31m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_build\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    825\u001b[0m           \u001b[0mcast_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    826\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_maybe_build\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2144\u001b[0m         \u001b[0;31m# operations.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2145\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaybe_init_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2146\u001b[0;31m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shapes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2147\u001b[0m       \u001b[0;31m# We must set self.built since user defined build functions are not\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2148\u001b[0m       \u001b[0;31m# constrained to set self.built.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/keras/layers/core.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m   1019\u001b[0m         \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel_constraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1020\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m         trainable=True)\n\u001b[0m\u001b[1;32m   1022\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_bias\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m       self.bias = self.add_weight(\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/layers/base.py\u001b[0m in \u001b[0;36madd_weight\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, trainable, constraint, use_resource, synchronization, aggregation, partitioner, **kwargs)\u001b[0m\n\u001b[1;32m    459\u001b[0m             \u001b[0maggregation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maggregation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m             \u001b[0mgetter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_variable\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 461\u001b[0;31m             **kwargs)\n\u001b[0m\u001b[1;32m    462\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    463\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mregularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36madd_weight\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, trainable, constraint, partitioner, use_resource, synchronization, aggregation, **kwargs)\u001b[0m\n\u001b[1;32m    527\u001b[0m         \u001b[0mcollections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollections_arg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m         \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 529\u001b[0;31m         aggregation=aggregation)\n\u001b[0m\u001b[1;32m    530\u001b[0m     \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrack_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_add_variable_with_custom_getter\u001b[0;34m(self, name, shape, dtype, initializer, getter, overwrite, **kwargs_for_getter)\u001b[0m\n\u001b[1;32m    710\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    711\u001b[0m         \u001b[0minitializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 712\u001b[0;31m         **kwargs_for_getter)\n\u001b[0m\u001b[1;32m    713\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    714\u001b[0m     \u001b[0;31m# If we set an initializer and the variable processed it, tracking will not\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m   1498\u001b[0m       \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1499\u001b[0m       \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1500\u001b[0;31m       aggregation=aggregation)\n\u001b[0m\u001b[1;32m   1501\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1502\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, var_store, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m   1241\u001b[0m           \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1242\u001b[0m           \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1243\u001b[0;31m           aggregation=aggregation)\n\u001b[0m\u001b[1;32m   1244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1245\u001b[0m   def _get_partitioned_variable(self,\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m    565\u001b[0m           \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m           \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 567\u001b[0;31m           aggregation=aggregation)\n\u001b[0m\u001b[1;32m    568\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m   def _get_partitioned_variable(self,\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/ops/variable_scope.py\u001b[0m in \u001b[0;36m_true_getter\u001b[0;34m(name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m    517\u001b[0m           \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m           \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 519\u001b[0;31m           aggregation=aggregation)\n\u001b[0m\u001b[1;32m    520\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m     synchronization, aggregation, trainable = (\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/ops/variable_scope.py\u001b[0m in \u001b[0;36m_get_single_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, partition_info, reuse, trainable, collections, caching_device, validate_shape, use_resource, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0muse_resource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_resource\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m         \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 933\u001b[0;31m         aggregation=aggregation)\n\u001b[0m\u001b[1;32m    934\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_store_eager_variables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/ops/variables.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    256\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mVariableV1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_v1_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_v2_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/ops/variables.py\u001b[0m in \u001b[0;36m_variable_v1_call\u001b[0;34m(cls, initial_value, trainable, collections, validate_shape, caching_device, name, variable_def, dtype, expected_shape, import_scope, constraint, use_resource, synchronization, aggregation, shape)\u001b[0m\n\u001b[1;32m    217\u001b[0m         \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m         \u001b[0maggregation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maggregation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 219\u001b[0;31m         shape=shape)\n\u001b[0m\u001b[1;32m    220\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m   def _variable_v2_call(cls,\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/ops/variables.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(**kwargs)\u001b[0m\n\u001b[1;32m    195\u001b[0m                         shape=None):\n\u001b[1;32m    196\u001b[0m     \u001b[0;34m\"\"\"Call on Variable class. Useful to force the signature.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m     \u001b[0mprevious_getter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdefault_variable_creator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgetter\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creator_stack\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m       \u001b[0mprevious_getter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_make_getter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprevious_getter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mdefault_variable_creator\u001b[0;34m(next_creator, **kwargs)\u001b[0m\n\u001b[1;32m   2517\u001b[0m         \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2518\u001b[0m         \u001b[0maggregation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maggregation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2519\u001b[0;31m         shape=shape)\n\u001b[0m\u001b[1;32m   2520\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/ops/variables.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    260\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_v2_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVariableMetaclass\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/ops/variables.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, initial_value, trainable, collections, validate_shape, caching_device, name, variable_def, dtype, expected_shape, import_scope, constraint, synchronization, aggregation, shape)\u001b[0m\n\u001b[1;32m   1686\u001b[0m           \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1687\u001b[0m           \u001b[0maggregation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maggregation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1688\u001b[0;31m           shape=shape)\n\u001b[0m\u001b[1;32m   1689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1690\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/ops/variables.py\u001b[0m in \u001b[0;36m_init_from_args\u001b[0;34m(self, initial_value, trainable, collections, validate_shape, caching_device, name, dtype, expected_shape, constraint, synchronization, aggregation, shape)\u001b[0m\n\u001b[1;32m   1816\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Initializer\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1817\u001b[0m               self._initial_value = ops.convert_to_tensor(\n\u001b[0;32m-> 1818\u001b[0;31m                   initial_value(), name=\"initial_value\", dtype=dtype)\n\u001b[0m\u001b[1;32m   1819\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1820\u001b[0m                 shape = (\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/ops/variable_scope.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    903\u001b[0m               \u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m               \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 905\u001b[0;31m               partition_info=partition_info)\n\u001b[0m\u001b[1;32m    906\u001b[0m           \u001b[0mvariable_dtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_dtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         elif len(tf_inspect.getargspec(initializer).args) == len(\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/ops/init_ops.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, shape, dtype, partition_info)\u001b[0m\n\u001b[1;32m    531\u001b[0m       \u001b[0mlimit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3.0\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m       return random_ops.random_uniform(\n\u001b[0;32m--> 533\u001b[0;31m           shape, -limit, limit, dtype, seed=self.seed)\n\u001b[0m\u001b[1;32m    534\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mget_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/ops/random_ops.py\u001b[0m in \u001b[0;36mrandom_uniform\u001b[0;34m(shape, minval, maxval, dtype, seed, name)\u001b[0m\n\u001b[1;32m    236\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"random_uniform\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxval\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m     \u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensor_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 238\u001b[0;31m     \u001b[0mminval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mminval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"min\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    239\u001b[0m     \u001b[0mmaxval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaxval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"max\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m     \u001b[0mseed1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom_seed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, preferred_dtype, dtype_hint)\u001b[0m\n\u001b[1;32m   1182\u001b[0m   preferred_dtype = deprecation.deprecated_argument_lookup(\n\u001b[1;32m   1183\u001b[0m       \"dtype_hint\", dtype_hint, \"preferred_dtype\", preferred_dtype)\n\u001b[0;32m-> 1184\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mconvert_to_tensor_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreferred_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor_v2\u001b[0;34m(value, dtype, dtype_hint, name)\u001b[0m\n\u001b[1;32m   1240\u001b[0m       \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m       \u001b[0mpreferred_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype_hint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1242\u001b[0;31m       as_ref=False)\n\u001b[0m\u001b[1;32m   1243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36minternal_convert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, ctx, accepted_result_types)\u001b[0m\n\u001b[1;32m   1295\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1296\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1297\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1299\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/framework/tensor_conversion_registry.py\u001b[0m in \u001b[0;36m_default_conversion_function\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_default_conversion_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mdel\u001b[0m \u001b[0mas_ref\u001b[0m  \u001b[0;31m# Unused.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mconstant_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name)\u001b[0m\n\u001b[1;32m    225\u001b[0m   \"\"\"\n\u001b[1;32m    226\u001b[0m   return _constant_impl(value, dtype, shape, name, verify_shape=False,\n\u001b[0;32m--> 227\u001b[0;31m                         allow_broadcast=True)\n\u001b[0m\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_impl\u001b[0;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    263\u001b[0m       tensor_util.make_tensor_proto(\n\u001b[1;32m    264\u001b[0m           \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverify_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m           allow_broadcast=allow_broadcast))\n\u001b[0m\u001b[1;32m    266\u001b[0m   \u001b[0mdtype_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattr_value_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAttrValue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtensor_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m   const_tensor = g.create_op(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#tests\n",
    "\n",
    "\n",
    "@patch('__main__.get_input')\n",
    "def test_new_question_answer(mock_input):\n",
    "    question = 'What is Intekglobal?'\n",
    "    new_answer = 'Intekglobal is one of the best companies in the world'\n",
    "    mock_input.side_effect = [question, new_answer]\n",
    "\n",
    "    data = {'context': defaultdict(str), 'faq': defaultdict(str)}\n",
    "\n",
    "    faqs = {\n",
    "        'Question': ['Who  owns Tesla Company?', 'Is this is heaven?'],\n",
    "        'Answer': [\n",
    "            'Elon Musk is the owner of Tesla', 'No, it is life on earth'\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "\n",
    "        mock_faq_files(tmpdirname, faqs, data['faq'])\n",
    "        qa_models = load_qa_models(\n",
    "            config_tfidf=data['faq']['config'], download=False\n",
    "        )\n",
    "        new_question_answer(data, qa_models)\n",
    "        updated_faq = pd.read_csv(data['faq']['path'])\n",
    "\n",
    "        assert updated_faq[updated_faq['Answer'] == new_answer].shape[0] == 1\n",
    "\n",
    "\n",
    "test_new_question_answer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "def new_context(data):\n",
    "\n",
    "    new_context = pd.DataFrame(\n",
    "        {\n",
    "            'topic': [get_input('Give context a title:\\n')],\n",
    "            'context': [get_input('Introduce the context:\\n')]\n",
    "        }\n",
    "    )\n",
    "    data['context']['df'] = data['context']['df'].append(new_context)\n",
    "    data['context']['df'].to_csv(data['context']['path'], index=False)\n",
    "\n",
    "    logging.info('contexts dataset updated..')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@patch('__main__.get_input')\n",
    "def test_new_context(mock_input):\n",
    "    data = {'context': defaultdict(str), 'faq': defaultdict(str)}\n",
    "\n",
    "    new_topic = 'AI Tool & Chatbot Development'\n",
    "    new_context_str = '''\n",
    "\n",
    "A chatbot is an important tool for simulating intelligent conversations with humans.\n",
    "Intekglobal chatbots efficiently live message on platforms such as Facebook Messenger, \n",
    "Slack, and Telegram. But chatbots are more than just a cool technology advancement.\n",
    "\n",
    "'''\n",
    "    contexts = {\n",
    "        'context':\n",
    "            [\n",
    "                '''One of the greatest punk rock bands from all the time\n",
    "                is the Ramones.\n",
    "                '''\n",
    "            ],\n",
    "        'topic': ['Ramones']\n",
    "    }\n",
    "    \n",
    "    mock_input.side_effect = [new_topic, new_context_str]\n",
    "    with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "\n",
    "        \n",
    "        logging.debug(str(new_context))\n",
    "        mock_context_file(tmpdirname, contexts, data['context'])\n",
    "        new_context(data)\n",
    "        updated_faq = pd.read_csv(data['context']['path'])\n",
    "\n",
    "        assert updated_faq[updated_faq.topic == new_topic].shape[0] == 1\n",
    "\n",
    "\n",
    "test_new_context()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def set_minimal_faq_questions(data):\n",
    "    if data['df'].shape[0] > 1:\n",
    "        return\n",
    "    minimal_questions = [\n",
    "        'Is this the Intekglobal Dialog System?',\n",
    "        'What is the purpose of these two automated questions?'\n",
    "    ]\n",
    "    minimal_answers = [\n",
    "        'This is the Intekglobal Dialog System', 'To populate the FAQ data file'\n",
    "    ]\n",
    "    minimal_faqs_df = pd.DataFrame(\n",
    "        {\n",
    "            'Question': minimal_questions,\n",
    "            'Answer': minimal_answers\n",
    "        }\n",
    "    )\n",
    "    data['df'] = pd.concat([data['df'], minimal_faqs_df])\n",
    "    data['df'].to_csv(data['path'], index=False)\n",
    "    logging.info(f' File created at {data[\"path\"]}')\n",
    "\n",
    "\n",
    "def set_minimal_contexts(data):\n",
    "    if data['df'].shape[0] > 0:\n",
    "        return\n",
    "\n",
    "    minimal_context_df = pd.DataFrame({'topic': [], 'context': []})\n",
    "    data['df'] = minimal_context_df\n",
    "    data['df'].to_csv(data['path'], index=False)\n",
    "\n",
    "    logging.info(f' File created at {data[\"path\"]}')\n",
    "\n",
    "\n",
    "def set_data_dict(file, data, question_type, data_dir):\n",
    "\n",
    "    data['path'] = file if file is not None else path.join(\n",
    "        data_dir, question_type + '_data.csv'\n",
    "    )\n",
    "\n",
    "    data['df'] = pd.read_csv(data['path']) if path.isfile(data['path']\n",
    "                                                         ) else pd.DataFrame()\n",
    "\n",
    "    if question_type == 'faq':\n",
    "        set_minimal_faq_questions(data)\n",
    "    if question_type == 'context':\n",
    "        set_minimal_contexts(data)\n",
    "\n",
    "\n",
    "def load_and_prepare_data(context_data_file, faq_data_file, data, configs_faq):\n",
    "\n",
    "    PARENT_DIR = popen('$PWD').read().strip()\n",
    "\n",
    "    if faq_data_file or context_data_file is None:\n",
    "        DATA_DIR = path.join(PARENT_DIR, 'data')\n",
    "\n",
    "        if not path.isdir(DATA_DIR):\n",
    "            mkdir(DATA_DIR)\n",
    "            logging.info(f'Data directory created at {DATA_DIR}')\n",
    "\n",
    "    if configs_faq is None:\n",
    "        configs_faq = configs.faq.tfidf_logreg_en_faq\n",
    "\n",
    "    data['faq']['config'] = configs_faq\n",
    "\n",
    "    set_data_dict(\n",
    "        file=faq_data_file,\n",
    "        data=data['faq'],\n",
    "        question_type='faq',\n",
    "        data_dir=DATA_DIR\n",
    "    )\n",
    "    set_data_dict(\n",
    "        file=context_data_file,\n",
    "        data=data['context'],\n",
    "        question_type='context',\n",
    "        data_dir=DATA_DIR\n",
    "    )\n",
    "\n",
    "    updates_faq_config_file(\n",
    "        configs_path=configs_faq,\n",
    "        dataset_reader={'data_path': data['faq']['path']}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-059fa4f8b6c7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0mtest_set_minimal_contexts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0mtest_set_data_dict_no_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m \u001b[0mtest_load_and_prepare_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.7/unittest/mock.py\u001b[0m in \u001b[0;36mpatched\u001b[0;34m(*args, **keywargs)\u001b[0m\n\u001b[1;32m   1253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1254\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextra_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1255\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkeywargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1256\u001b[0m             \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1257\u001b[0m                 if (patching not in entered_patchers and\n",
      "\u001b[0;32m<ipython-input-26-059fa4f8b6c7>\u001b[0m in \u001b[0;36mtest_load_and_prepare_data\u001b[0;34m(mock_popen)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0mdata_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmpdirname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'data'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;31m#config_file = path.join(tmpdirname,'config_faq.json')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#tests\n",
    "import tempfile,logging\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from shutil import rmtree\n",
    "from os import path,popen\n",
    "from unittest.mock import patch\n",
    "#from let_me_answer_for_you.settings import *\n",
    "\n",
    "\n",
    "def test_set_minimal_faqs_with_more_than_one_question():\n",
    "    with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "        data_file = path.join(tmpdirname, 'tmp_data.csv')\n",
    "        questions = ['a?', 'b?']\n",
    "        answers = ['a', 'b']\n",
    "        df = pd.DataFrame({'Question': questions, 'Answer': answers})\n",
    "        df.to_csv(data_file, index=False)\n",
    "        data = {'df': df, 'path': data_file}\n",
    "        set_minimal_faq_questions(data)\n",
    "\n",
    "        assert data['df'].shape[0] == 2\n",
    "\n",
    "\n",
    "def test_set_minimal_faqs_with_less_than_two_questions():\n",
    "    with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "        data_file = path.join(tmpdirname, 'tmp_data.csv')\n",
    "        questions = ['a?']\n",
    "        answers = ['a']\n",
    "        df = pd.DataFrame({'Question': questions, 'Answer': answers})\n",
    "        df.to_csv(data_file, index=False)\n",
    "        data = {'df': df, 'path': data_file}\n",
    "\n",
    "        assert data['df'].shape[0] == 1\n",
    "\n",
    "        set_minimal_faq_questions(data)\n",
    "\n",
    "        assert data['df'].shape[0] == 3\n",
    "        assert any(\n",
    "            data['df'].Question == 'Is this the Intekglobal Dialog System?'\n",
    "        )\n",
    "\n",
    "\n",
    "def test_set_minimal_contexts():\n",
    "    with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "        data_file = path.join(tmpdirname, 'tmp_data.csv')\n",
    "        data = {'df': pd.DataFrame(), 'path': data_file}\n",
    "        set_minimal_contexts(data)\n",
    "        assert path.isfile(data['path'])\n",
    "        assert all(data['df'].columns == ['topic', 'context'])\n",
    "\n",
    "\n",
    "def test_set_data_dict_no_file():\n",
    "    with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "        data = {'context': defaultdict(str)}\n",
    "        set_data_dict(\n",
    "            file=None,\n",
    "            data=data['context'],\n",
    "            data_dir=tmpdirname,\n",
    "            question_type='context'\n",
    "        )\n",
    "        logging.debug(data)\n",
    "        assert path.isfile(data['context']['path'])\n",
    "\n",
    "\n",
    "@patch('__main__.popen')\n",
    "def test_load_and_prepare_data(mock_popen):\n",
    "    with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "        mock_popen(\"$PWD\").read().strip.side_effect = [tmpdirname]\n",
    "        data = {'context': defaultdict(str), 'faq': defaultdict(str)}\n",
    "\n",
    "        load_and_prepare_data(\n",
    "            context_data_file=None,\n",
    "            faq_data_file=None,\n",
    "            data=data,\n",
    "            configs_faq=None\n",
    "        )\n",
    "        data_dir = path.join(tmpdirname, 'data')\n",
    "        #config_file = path.join(tmpdirname,'config_faq.json')\n",
    "        assert path.isdir(data_dir)\n",
    "\n",
    "\n",
    "test_set_minimal_faqs_with_more_than_one_question()\n",
    "test_set_minimal_faqs_with_less_than_two_questions()\n",
    "test_set_minimal_contexts()\n",
    "test_set_data_dict_no_file()\n",
    "test_load_and_prepare_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
