{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp dialog_system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03:53:49 ERROR: Error Log Active \n"
     ]
    }
   ],
   "source": [
    "# export\n",
    "from let_me_answer_for_you.settings import *\n",
    "import logging\n",
    "from collections import defaultdict\n",
    "\n",
    "logging.basicConfig(\n",
    "    #filename='example.log',\n",
    "    format='%(asctime)s %(levelname)s:%(message)s',\n",
    "    level=logging.ERROR,\n",
    "    datefmt='%I:%M:%S'\n",
    ")\n",
    "\n",
    "logging.debug(\" Debug Log Active\")\n",
    "logging.info(\"Hello! Welcome to our automated dialog system!\")\n",
    "logging.warning(' Warning Log Active')\n",
    "logging.error(' Error Log Active ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class DialogSystem:\n",
    "    def __init__(\n",
    "        self, context_data_file=None, faq_data_file=None, configs_faq=None\n",
    "    ):\n",
    "        run_shell_installs()\n",
    "        self.data = {'context': defaultdict(str), 'faq': defaultdict(str)}\n",
    "\n",
    "        load_and_prepare_data(\n",
    "            context_data_file=context_data_file,\n",
    "            faq_data_file=faq_data_file,\n",
    "            configs_faq=configs_faq,\n",
    "            data=self.data\n",
    "        )\n",
    "        self.qa_models = load_qa_models(config_tfidf=self.data['faq']['config'])\n",
    "\n",
    "    def question_answer(self):\n",
    "        question, responses = question_response(\n",
    "            data=self.data,\n",
    "            qa_models=self.qa_models,\n",
    "            num_returned_values_per_squad_model=1\n",
    "        )\n",
    "        print('\\n\\n'+responses)\n",
    "\n",
    "    def new_answer(self):\n",
    "        ...\n",
    "\n",
    "    def new_context(self):\n",
    "        ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0mquestion_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqa_models\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_returned_values_per_squad_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m <no docstring>\n",
       "\u001b[0;31mSource:\u001b[0m   \n",
       "\u001b[0;32mdef\u001b[0m \u001b[0mquestion_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqa_models\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_returned_values_per_squad_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mquestion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Introduce question:\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformatted_responses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_responses\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'context'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'df'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquestion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqa_models\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_squad_results\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mreturn\u001b[0m \u001b[0mquestion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformatted_responses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mFile:\u001b[0m      /wd/nbs/let_me_answer_for_you/settings.py\n",
       "\u001b[0;31mType:\u001b[0m      function\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "question_response??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package perluniprops to\n",
      "[nltk_data]     /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package perluniprops is already up-to-date!\n",
      "[nltk_data] Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]     /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package nonbreaking_prefixes is already up-to-date!\n",
      "100%|██████████| 12.3k/12.3k [00:00<00:00, 3.69MB/s]\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "ds =  DialogSystem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Introduce question:\n",
      " How you doing?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "How you doing?:\n",
      "\n",
      "0: Elon Musk is the CEO of Tesla\n",
      "1: car company left in california\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ds.question_answer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tests\n",
    "import tempfile\n",
    "from unittest.mock import patch\n",
    "from shutil import copyfile\n",
    "from collections import defaultdict\n",
    "\n",
    "def mock_faq_files(tmpdirname, faqs, faq_dic):\n",
    "\n",
    "    faq_dic['path'] = path.join(tmpdirname, 'temp_faq.csv')\n",
    "    faq_dic['config'] = path.join(tmpdirname, 'temp_config_faq.json')\n",
    "    faq_dic['df'] = pd.DataFrame(faqs)\n",
    "    faq_dic['df'].to_csv(faq_dic['path'], index=False)\n",
    "\n",
    "    copyfile(configs.faq.tfidf_logreg_en_faq, faq_dic['config'])\n",
    "\n",
    "    updates_faq_config_file(\n",
    "        configs_path=faq_dic['config'],\n",
    "        dataset_reader={'data_path': faq_dic['path']}\n",
    "    )\n",
    "\n",
    "\n",
    "def mock_context_file(tmpdirname, contexts, context_dic):\n",
    "\n",
    "    context_dic['path'] = path.join(tmpdirname, 'temp_context.csv')\n",
    "    context_dic['df'] = pd.DataFrame(contexts)\n",
    "    context_dic['df'].to_csv(context_dic['path'], index=False)\n",
    "\n",
    "@patch('__main__.get_input')\n",
    "def test_updated_faq_response(mock_input):\n",
    "\n",
    "    new_answer = 'Intekglobal is one of the best companies in the world'\n",
    "    question = 'What is Intekglobal?'\n",
    "    mock_input.side_effect = [question, 'Y', 'N', new_answer, question, 'N']\n",
    "\n",
    "    data = {'context': defaultdict(str), 'faq': defaultdict(str)}\n",
    "    contexts = {\n",
    "        'context':\n",
    "            [\n",
    "                '''One of the greatest punk rock bands from all the time\n",
    "                is the Ramones.\n",
    "                '''\n",
    "            ],\n",
    "        'topic': ['Ramones']\n",
    "    }\n",
    "\n",
    "    faqs = {\n",
    "        'Question':\n",
    "            ['Who was the leading vocal artist of the Ramones?', 'Is this is heaven?'],\n",
    "        'Answer': ['Joey Ramone', 'No, it is life on earth']\n",
    "    }\n",
    "\n",
    "    with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "\n",
    "        mock_faq_files(tmpdirname, faqs, data['faq'])\n",
    "        mock_context_file(tmpdirname, contexts, data['context'])\n",
    "\n",
    "        qa_models = load_qa_models(\n",
    "            config_tfidf=data['faq']['config'], download=False\n",
    "        )\n",
    "        old_responses, old_status = question_response(data, qa_models)\n",
    "        new_responses, new_status = question_response(data, qa_models)\n",
    "\n",
    "        logging.debug(f'Old response:\\n {old_responses}')\n",
    "        logging.debug(f'New response:\\n{new_responses}')\n",
    "        \n",
    "        new_answer not in old_responses\n",
    "        new_answer in new_responses\n",
    "\n",
    "#test_updated_faq_response()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Contex Dialog System's part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "from unittest.mock import patch\n",
    "from shutil import copyfile\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "def mock_faq_files(tmpdirname, faqs, faq_dic):\n",
    "\n",
    "    faq_dic['path'] = path.join(tmpdirname, 'temp_faq.csv')\n",
    "    faq_dic['config'] = path.join(tmpdirname, 'temp_config_faq.json')\n",
    "    faq_dic['df'] = pd.DataFrame(faqs)\n",
    "    faq_dic['df'].to_csv(faq_dic['path'], index=False)\n",
    "\n",
    "    copyfile(configs.faq.tfidf_logreg_en_faq, faq_dic['config'])\n",
    "\n",
    "    updates_faq_config_file(\n",
    "        configs_path=faq_dic['config'],\n",
    "        dataset_reader={'data_path': faq_dic['path']}\n",
    "    )\n",
    "\n",
    "\n",
    "def mock_context_file(tmpdirname, contexts, context_dic):\n",
    "\n",
    "    context_dic['path'] = path.join(tmpdirname, 'temp_context.csv')\n",
    "    context_dic['df'] = pd.DataFrame(contexts)\n",
    "    context_dic['df'].to_csv(context_dic['path'], index=False)\n",
    "\n",
    "\n",
    "@patch('__main__.get_input')\n",
    "def test_updated_context_response(mock_input):\n",
    "    data = {'context': defaultdict(str), 'faq': defaultdict(str)}\n",
    "\n",
    "    contexts = {\n",
    "        'context': ['Intekglobal has its headquarters located in TJ', ],\n",
    "        'topic': ['headquarters']\n",
    "    }\n",
    "\n",
    "    faqs = {\n",
    "        'Question':\n",
    "            ['Minimum number of questions?', 'This is the other question?'],\n",
    "        'Answer': ['Two', 'yes']\n",
    "    }\n",
    "\n",
    "    question = 'What is a Chatbot?'\n",
    "    new_topic = 'AI Tool & Chatbot Development'\n",
    "    new_context = '''\n",
    "\n",
    "A chatbot is an important tool for simulating intelligent conversations with humans.\n",
    "Intekglobal chatbots efficiently live message on platforms such as Facebook Messenger, \n",
    "Slack, and Telegram. Assisting consumers with a variety of purposes and industries. \n",
    "\n",
    "But chatbots are more than just a cool technology advancement. They actually transform the user experience.\n",
    "People want simple and convenient interactions with interface and products.\n",
    "\n",
    "'''\n",
    "\n",
    "    mock_input.side_effect = [\n",
    "        question, 'YES', 'yes', new_topic, new_context, question, 'N'\n",
    "    ]\n",
    "\n",
    "    with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "\n",
    "        mock_faq_files(tmpdirname, faqs, data['faq'])\n",
    "        mock_context_file(tmpdirname, contexts, data['context'])\n",
    "\n",
    "        qa_models = load_qa_models(\n",
    "            config_tfidf=data['faq']['config'], download=False\n",
    "        )\n",
    "\n",
    "        old_responses, old_status = question_response(data, qa_models)\n",
    "        new_responses, new_status = question_response(data, qa_models)\n",
    "\n",
    "        logging.info(f'Old status: {old_status}')\n",
    "        logging.info(f'Old response:\\n {old_responses}')\n",
    "        logging.info(f'New response:\\n{new_responses}')\n",
    "\n",
    "        updated_context = pd.read_csv(data['context']['path'])\n",
    "\n",
    "        assert updated_context[updated_context['context'] == new_context\n",
    "                              ].shape[0] == 1\n",
    "\n",
    "        assert updated_context[updated_context['topic'] == new_topic\n",
    "                              ].shape[0] == 1\n",
    "\n",
    "        assert 'an important tool for simulating intelligent conversations with humans' not in old_responses\n",
    "        assert 'an important tool for simulating intelligent conversations with humans' in new_responses\n",
    "\n",
    "\n",
    "#test_updated_context_response()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def dialog_system(context_data_file=None, faq_data_file=None, configs_faq=None):\n",
    "    '''\n",
    "     Main Dialog System\n",
    "    '''\n",
    "    run_shell_installs()\n",
    "\n",
    "    data = {'context': defaultdict(str), 'faq': defaultdict(str)}\n",
    "\n",
    "    load_and_prepare_data(\n",
    "        context_data_file=context_data_file,\n",
    "        faq_data_file=faq_data_file,\n",
    "        configs_faq=configs_faq,\n",
    "        data=data\n",
    "    )\n",
    "\n",
    "    qa_models = load_qa_models(config_tfidf=data['faq']['config'])\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            responses, _ = question_response(data=data, qa_models=qa_models)\n",
    "        except (KeyboardInterrupt, EOFError, SystemExit):\n",
    "            logging.debug('Goodbye!')\n",
    "            return 'Goodbye!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test  dialog_system()\n",
    "\n",
    "from unittest.mock import patch\n",
    "from let_me_answer_for_you.dialog_system import load_qa_models, load_and_prepare_data\n",
    "from let_me_answer_for_you.dialog_system import question_response,run_shell_installs\n",
    "\n",
    "@patch('__main__.run_shell_installs')\n",
    "@patch('__main__.load_qa_models')\n",
    "@patch('__main__.load_and_prepare_data')\n",
    "@patch('__main__.question_response')\n",
    "def test_main_keyboard_interrupt(\n",
    "    mock_question_response,\n",
    "    mock_pd_read_csv,\n",
    "    mock_load_qa_models,\n",
    "    mock_run_shell_installs,\n",
    "):\n",
    "    mock_question_response.side_effect = [\n",
    "        KeyboardInterrupt(), EOFError(),\n",
    "        SystemExit()\n",
    "    ]\n",
    "    assert 'Goodbye!' == dialog_system()\n",
    "    assert 'Goodbye!' == dialog_system()\n",
    "    assert 'Goodbye!' == dialog_system()\n",
    "\n",
    "\n",
    "test_main_keyboard_interrupt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
