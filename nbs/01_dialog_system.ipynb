{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "!pip install -r ../requirements.txt > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp dialog_system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# export\n",
    "from deeppavlov import configs,build_model,train_model\n",
    "import json\n",
    "from os import path,popen\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s %(levelname)s:%(message)s', level=logging.DEBUG, datefmt='%I:%M:%S')\n",
    "logging.info(\"Hello! Welcome to our automated dialog system!\")\n",
    "logging.debug(\"test: for Debug?\")\n",
    "logging.error(' Error Log Active ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def run_shell_installs():\n",
    "    ''' Run install commands\n",
    "    '''\n",
    "    command_strings = (\n",
    "        ' pip install deeppavlov',\n",
    "        ' python -m deeppavlov install squad',\n",
    "        ' python -m deeppavlov install squad_bert',\n",
    "        ' python -m deeppavlov install fasttext_avg_autofaq',\n",
    "        ' python -m deeppavlov install fasttext_tfidf_autofaq',\n",
    "        ' python -m deeppavlov install tfidf_autofaq',\n",
    "        ' python -m deeppavlov install tfidf_logreg_autofaq ',\n",
    "        ' python -m deeppavlov install tfidf_logreg_en_faq'\n",
    "    )\n",
    "    for command in command_strings:\n",
    "        logging.debug(command)\n",
    "        logging.debug(popen(command).read())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_shell_installs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dialog System\n",
    "> Question Answering Automated Dialog System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def action_over_list_f(arr, v):\n",
    "\n",
    "    k_id, v_id = next(iter(v[0].items()))\n",
    "\n",
    "    for p, a in enumerate(arr):\n",
    "        if k_id in a.keys() and a[k_id] == v_id:\n",
    "            for k_rep, v_rep in v[1].items():\n",
    "                arr[p][k_rep] = v_rep\n",
    "\n",
    "\n",
    "def replacement_f(model_config, **args):\n",
    "    '''Replaces the model config dictionary with new values\n",
    "    '''\n",
    "    for k, v in args.items():\n",
    "        if isinstance(v, dict):\n",
    "            replacement_f(model_config[k], **v)\n",
    "        if isinstance(v, str):\n",
    "            model_config[k] = v\n",
    "        if isinstance(model_config[k], list):\n",
    "            action_over_list_f(model_config[k], v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test action_over_list_f\n",
    "from random import randint\n",
    "\n",
    "\n",
    "def gen_list_keys_for_tests():\n",
    "\n",
    "    str_n = lambda x: f'{x}_{randint(1,10):1}'\n",
    "    gen_dict_list = lambda: {\n",
    "        'id': str_n('id'),\n",
    "        'key1': str_n('v1'),\n",
    "        'key2': str_n('v2'),\n",
    "        'key3': str_n('v3')\n",
    "    }\n",
    "\n",
    "    pipe_list = [gen_dict_list() for _ in range(randint(3, 10))]\n",
    "\n",
    "    rand_id = pipe_list[randint(0, len(pipe_list) - 1)]['id']\n",
    "    rand_key = f'key{randint(1, 3)}' \n",
    "\n",
    "    new_rand_val = str_n('new')\n",
    "    args = {\n",
    "        'chains': {\n",
    "            'pipe': [{\n",
    "                'id': rand_id\n",
    "            }, {\n",
    "               rand_key : new_rand_val\n",
    "            }]\n",
    "        }\n",
    "    }\n",
    "\n",
    "    return pipe_list, rand_id, rand_key, args, new_rand_val\n",
    "\n",
    "\n",
    "def test_action_over_list_f():\n",
    "\n",
    "    pipe_list, rand_id, rand_key, args, new_rand_val = gen_list_keys_for_tests()\n",
    "\n",
    "    assert all(\n",
    "        new_rand_val not in pipe_elem.values() for pipe_elem in pipe_list\n",
    "    )\n",
    "\n",
    "    action_over_list_f(pipe_list, args['chains']['pipe'])\n",
    "\n",
    "    assert any(\n",
    "        rand_key in pipe_elem.keys() and\n",
    "        new_rand_val in pipe_elem.values() for pipe_elem in pipe_list\n",
    "    )\n",
    "\n",
    "\n",
    "def test_replacement_f_list():\n",
    "\n",
    "    pipe_list, rand_id, rand_key, args, new_rand_val = gen_list_keys_for_tests()\n",
    "\n",
    "    mod_conf = {'chains': {'pipe': pipe_list}}\n",
    "\n",
    "    assert all(\n",
    "        new_rand_val not in pipe_elem.values()\n",
    "        for pipe_elem in mod_conf['chains']['pipe']\n",
    "    )\n",
    "\n",
    "    replacement_f(model_config=mod_conf, **args)\n",
    "    assert any(\n",
    "        rand_key in pipe_elem.keys() and\n",
    "        new_rand_val in pipe_elem.values()\n",
    "        for pipe_elem in mod_conf['chains']['pipe']\n",
    "    )\n",
    "\n",
    "\n",
    "def test_replacement_f_val():\n",
    "    args = {'key3': 'newvalue'}\n",
    "    mod_conf = {'key1': 'val1', 'key2': 'val2', 'key3': 'val3'}\n",
    "    replacement_f(model_config=mod_conf, **args)\n",
    "    assert all(\n",
    "        arg_k in mod_conf.keys() and arg_v in mod_conf.values()\n",
    "        for arg_k, arg_v in args.items()\n",
    "    )\n",
    "\n",
    "\n",
    "def test_replacement_f_dict():\n",
    "    args = {'1_key_3': {'2_key_2': 'newvalue'}}\n",
    "    mod_conf = {'1_key_3': {'2_key_2': 'oldvalue'}, '0_key_': '0_val'}\n",
    "    replacement_f(model_config=mod_conf, **args)\n",
    "    assert mod_conf['1_key_3']['2_key_2'] == 'newvalue'\n",
    "\n",
    "\n",
    "test_action_over_list_f()\n",
    "test_replacement_f_list()\n",
    "test_replacement_f_val()\n",
    "test_replacement_f_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def updates_faq_config_file(\n",
    "    configs_path,\n",
    "    **args\n",
    "):\n",
    "    '''Updates deepplavov json config file \n",
    "    '''\n",
    "    #set FAQ data in config file\n",
    "    model_config = json.load(open(configs_path))\n",
    "\n",
    "    if 'data_url' in model_config['dataset_reader']:\n",
    "        del model_config['dataset_reader']['data_url']\n",
    "\n",
    "    replacement_f(model_config=model_config,**args)\n",
    "\n",
    "    json.dump(model_config, open(configs_path, 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test updates_faq_config_file\n",
    "import tempfile\n",
    "from shutil import copyfile\n",
    "\n",
    "\n",
    "def gen_list_keys_for_tests():\n",
    "\n",
    "    str_n = lambda x: f'{x}_{randint(1,10):1}'\n",
    "    gen_dict_list = lambda: {\n",
    "        'id': str_n('id'),\n",
    "        'key1': str_n('v1'),\n",
    "        'key2': str_n('v2'),\n",
    "        'key3': str_n('v3')\n",
    "    }\n",
    "\n",
    "    pipe_list = [gen_dict_list() for _ in range(randint(3, 10))]\n",
    "\n",
    "    rand_id = pipe_list[randint(0, len(pipe_list) - 1)]['id']\n",
    "    rand_key =  f'key{randint(1, 3)}' \n",
    "\n",
    "    new_rand_val = str_n('new')\n",
    "    pipe_dict = {'pipe': [{'id': rand_id}, {rand_key: new_rand_val}]}\n",
    "    args = {'chainer': pipe_dict}\n",
    "\n",
    "    return pipe_list, rand_id, rand_key, args, new_rand_val\n",
    "\n",
    "\n",
    "def test_updates_faq_config_file_update_string():\n",
    "\n",
    "    with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "\n",
    "        tmp_config_file = path.join(tmpdirname, 'tmp_file.json')\n",
    "\n",
    "        copyfile(configs.faq.tfidf_logreg_en_faq, tmp_config_file)\n",
    "\n",
    "        assert path.isfile(tmp_config_file)\n",
    "\n",
    "        updates_faq_config_file(\n",
    "            configs_path=tmp_config_file,\n",
    "            dataset_reader={'data_path': 'fictional_csv_file.csv'}\n",
    "        )\n",
    "\n",
    "        config_json = json.load(open(tmp_config_file))\n",
    "        assert 'data_path' in config_json['dataset_reader']\n",
    "\n",
    "\n",
    "def test_updates_faq_config_file_update_list():\n",
    "\n",
    "    with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "\n",
    "        tmp_config_file = path.join(tmpdirname, 'tmp_file.json')\n",
    "\n",
    "        pipe_list, rand_id, rand_key, args, new_rand_val = gen_list_keys_for_tests(\n",
    "        )\n",
    "        mod_conf = {\n",
    "            'chainer': {\n",
    "                'pipe': pipe_list\n",
    "            },\n",
    "            'dataset_reader': 'dataset_reader_dictionary'\n",
    "        }\n",
    "\n",
    "        json.dump(mod_conf, open(tmp_config_file, 'w'))\n",
    "\n",
    "        assert path.isfile(tmp_config_file)\n",
    "\n",
    "        updates_faq_config_file(configs_path=tmp_config_file, **args)\n",
    "\n",
    "        config_json = json.load(open(tmp_config_file))\n",
    "   \n",
    "        assert any(\n",
    "            rand_key in pipe_elem.keys() and new_rand_val in pipe_elem.values()\n",
    "            for pipe_elem in config_json['chainer']['pipe']\n",
    "        )\n",
    "\n",
    "\n",
    "test_updates_faq_config_file_update_string()\n",
    "test_updates_faq_config_file_update_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def select_faq_responses(faq_model, question):\n",
    "    '''Calls Deeppavlov FAQ model\n",
    "    '''\n",
    "    return faq_model([question])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test faq responses\n",
    "import tempfile\n",
    "from shutil import copyfile\n",
    "\n",
    "\n",
    "def gen_mock_csv_file(tmpdirname, faqs):\n",
    "\n",
    "    temp_faq_csv = path.join(tmpdirname, 'tmp_faq.csv')\n",
    "\n",
    "    pd.DataFrame(faqs).to_csv(temp_faq_csv, index=False)\n",
    "\n",
    "    return temp_faq_csv\n",
    "\n",
    "\n",
    "def gen_mock_vocab_answers(tmpdirname, vocabs):\n",
    "\n",
    "    temp_dict_file = path.join(tmpdirname, 'temp_vocab_answers.dict')\n",
    "    vocabs_text = '\\n'.join(\n",
    "        t + '\\t' + str(f) for t, f in zip(vocabs['text'], vocabs['freq'])\n",
    "    )\n",
    "\n",
    "    f = open(temp_dict_file, 'w')\n",
    "    f.write(vocabs_text)\n",
    "    f.close()\n",
    "\n",
    "    return temp_dict_file\n",
    "\n",
    "\n",
    "def gen_faq_config(tmpdirname, vocab_file, faq_file):\n",
    "\n",
    "    temp_configs_faq = path.join(tmpdirname, 'temp_config_faq.json')\n",
    "    copyfile(configs.faq.tfidf_logreg_en_faq, temp_configs_faq)\n",
    "\n",
    "    changes_dict = {'save_path': vocab_file, 'load_path': vocab_file}\n",
    "    id_dict = {'id': 'answers_vocab'}\n",
    "\n",
    "    updates_faq_config_file(\n",
    "        configs_path=temp_configs_faq,\n",
    "        chainer={'pipe': [id_dict, changes_dict]},\n",
    "        dataset_reader={'data_path': faq_file}\n",
    "    )\n",
    "\n",
    "    return temp_configs_faq\n",
    "\n",
    "\n",
    "def test_faq_response_with_minimum_faqs_in_dataframe_fail_case():\n",
    "    with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "\n",
    "        faqs = {\n",
    "            'Question': ['Is Covid erradicated?'],\n",
    "            'Answer': ['Definitely not!']\n",
    "        }\n",
    "\n",
    "        vocabs = {'text': ['This is a vocab example'], 'freq': [1]}\n",
    "\n",
    "        faq_file = gen_mock_csv_file(tmpdirname, faqs)\n",
    "        vocab_file = gen_mock_vocab_answers(tmpdirname, vocabs)\n",
    "\n",
    "        configs_file = gen_faq_config(tmpdirname, vocab_file, faq_file)\n",
    "\n",
    "        try:\n",
    "            select_faq_responses(\n",
    "                question='Is Enrique the prettiest person in town?',\n",
    "                faq_model=train_model(configs_file, download=True)\n",
    "            )\n",
    "            assert False\n",
    "        except ValueError as e:\n",
    "            assert True\n",
    "\n",
    "\n",
    "def test_faq_response_with_minimum_faqs_in_dataframe_success_case():\n",
    "    with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "\n",
    "        faqs = {\n",
    "            'Question': ['Is Covid erradicated?', 'Who is the current POTUS?'],\n",
    "            'Answer': ['Definitely not!', 'Donald Trump']\n",
    "        }\n",
    "\n",
    "        vocabs = {'text': ['This is a vocab example'], 'freq': [1]}\n",
    "\n",
    "        faq_file = gen_mock_csv_file(tmpdirname, faqs)\n",
    "        vocab_file = gen_mock_vocab_answers(tmpdirname, vocabs)\n",
    "\n",
    "        configs_file = gen_faq_config(tmpdirname, vocab_file, faq_file)\n",
    "\n",
    "        assert select_faq_responses(\n",
    "            question='Is Enrique the prettiest person in town?',\n",
    "            faq_model=train_model(configs_file, download=True)\n",
    "        ) == ['Donald Trump']\n",
    "\n",
    "        \n",
    "        \n",
    "def test_faq_response_with_minimum_answers_vocab_success_case():\n",
    "    with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "\n",
    "        faqs = {\n",
    "            'Question': ['Is Covid erradicated?', 'Who is the current POTUS?'],\n",
    "            'Answer': ['Definitely not!', 'Donald Trump']\n",
    "        }\n",
    "\n",
    "        vocabs = {'text': [], 'freq': []}\n",
    "\n",
    "        faq_file = gen_mock_csv_file(tmpdirname, faqs)\n",
    "        vocab_file = gen_mock_vocab_answers(tmpdirname, vocabs)\n",
    "\n",
    "        configs_file = gen_faq_config(tmpdirname, vocab_file, faq_file)\n",
    "\n",
    "        select_faq_responses(\n",
    "            question='Is Enrique the prettiest person in town?',\n",
    "            faq_model=train_model(configs_file, download=True)\n",
    "        ) == ['Donald Trump']\n",
    "\n",
    "test_faq_response_with_minimum_faqs_in_dataframe_fail_case()\n",
    "test_faq_response_with_minimum_faqs_in_dataframe_success_case()\n",
    "test_faq_response_with_minimum_answers_vocab_success_case()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_squad_responses(\n",
    "    contexts, squad_model, question, best_results=1\n",
    "):\n",
    "    '''Calls Deeppavlov BERT and RNET Context Question Answering\n",
    "    '''\n",
    "    responses = contexts.context.apply(\n",
    "        lambda context: squad_model([context], [question])\n",
    "    ).values\n",
    "    \n",
    "    logging.debug(f'Responses: {responses}')\n",
    "    top_responses = [\n",
    "        r[0][0] for r in sorted(responses, key=lambda x: -1 * x[2][0])\n",
    "        [:best_results]\n",
    "    ]\n",
    "\n",
    "    logging.debug(f'Top Responses: {top_responses}')\n",
    "    return responses, top_responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test select_squad_responses\n",
    "import tempfile\n",
    "from shutil import copyfile\n",
    "\n",
    "empty = {'topic': [], 'context': []}\n",
    "spacex = {\n",
    "    'topic': ['SpaceX'],\n",
    "    'context':\n",
    "        [\n",
    "            '''Space Exploration Technologies Corp., trading as SpaceX, is an American aerospace manufacturer and space transportation\n",
    "services company headquartered in Hawthorne, California. It was founded in 2002 by Elon Musk with the goal of reducing space \n",
    "transportation costs to enable the colonization of Mars. SpaceX has developed several launch vehicles, the Starlink satellite\n",
    "constellation, and the Dragon spacecraft. It is widely considered among the most successful private spaceflight companies.'''\n",
    "        ]\n",
    "}\n",
    "\n",
    "intekglobal = {\n",
    "    'topic': ['Intekglobal', 'InG'],\n",
    "    'context':\n",
    "        [\n",
    "            'Intekglobal has its headquarters located in TJ',\n",
    "            'Intekglobal is in the north of mexico'\n",
    "        ]\n",
    "}\n",
    "\n",
    "\n",
    "def assert_squad_model(\n",
    "    contexts, squad_model, question, expected_responses, **args\n",
    "):\n",
    "    responses, top_responses = select_squad_responses(\n",
    "        contexts=pd.DataFrame(contexts),\n",
    "        squad_model=squad_model,\n",
    "        question=question,\n",
    "        **args\n",
    "    )\n",
    "    assert top_responses == expected_responses\n",
    "\n",
    "\n",
    "def test_squad_bert():\n",
    "\n",
    "    bert = build_model(configs.squad.squad_bert, download=True)\n",
    "\n",
    "    assert_squad_model(\n",
    "        empty,\n",
    "        bert,\n",
    "        'Is an empty response expected?',\n",
    "        expected_responses=[],\n",
    "        best_results=2\n",
    "    )\n",
    "\n",
    "    assert_squad_model(\n",
    "        spacex, bert, 'Who founded SpaceX?', expected_responses=['Elon Musk']\n",
    "    )\n",
    "\n",
    "    assert_squad_model(\n",
    "        intekglobal,\n",
    "        bert,\n",
    "        'Where is Intekglobal located?',\n",
    "        expected_responses=['north of mexico','TJ'],\n",
    "        best_results=2\n",
    "    )\n",
    "\n",
    "\n",
    "def test_squad_rnet():\n",
    "\n",
    "    bert = build_model(configs.squad.squad, download=True)\n",
    "\n",
    "    assert_squad_model(\n",
    "        empty,\n",
    "        bert,\n",
    "        'Is an empty response expected?',\n",
    "        expected_responses=[],\n",
    "        best_results=5\n",
    "    )\n",
    "\n",
    "    assert_squad_model(\n",
    "        spacex, bert, 'Who founded SpaceX?', expected_responses=['Elon Musk']\n",
    "    )\n",
    "\n",
    "    assert_squad_model(\n",
    "        intekglobal,\n",
    "        bert,\n",
    "        'Where is Intekglobal located?',\n",
    "        expected_responses=['north of mexico','TJ'],\n",
    "        best_results=2\n",
    "    )\n",
    "\n",
    "test_squad_bert()\n",
    "test_squad_rnet()\n",
    "del spacex, empty, intekglobal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def load_qa_models(\n",
    "    config_rnet=configs.squad.squad,\n",
    "    config_bert=configs.squad.squad_bert,\n",
    "    config_tfidf=configs.faq.tfidf_logreg_en_faq,\n",
    "    download=True\n",
    "):\n",
    "    qa_models = {\n",
    "        'squad':\n",
    "            {\n",
    "                'rnet': build_model(config_rnet, download=download),\n",
    "                'bert': build_model(config_bert, download=download)\n",
    "            },\n",
    "        'faq': {\n",
    "            'tfidf': train_model(config_tfidf, download=download)\n",
    "        }\n",
    "    }\n",
    "    return qa_models\n",
    "\n",
    "\n",
    "def format_responses(question, responses):\n",
    "    formatted_response = f'{question}:\\n\\n'\n",
    "    for k, res in enumerate(responses):\n",
    "        formatted_response += f'{k}: {res}\\n'\n",
    "    return formatted_response\n",
    "\n",
    "\n",
    "def get_responses(contexts, question, qa_models, nb_squad_results=1):\n",
    "    responses = []\n",
    "    for squad_model in qa_models['squad'].values():\n",
    "        responses.extend(\n",
    "            select_squad_responses(\n",
    "                contexts, squad_model, question, best_results=nb_squad_results\n",
    "            )[1]\n",
    "        )\n",
    "    for faq_model in qa_models['faq'].values():\n",
    "        responses.extend(select_faq_responses(faq_model, question))\n",
    "    return responses, format_responses(\n",
    "        question, set([r for r in responses if r.strip()])\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test get_responses\n",
    "import tempfile\n",
    "from shutil import copyfile\n",
    "\n",
    "intekglobal_context = {\n",
    "    'topic': ['Intekglobal', 'InG'],\n",
    "    'context':\n",
    "        [\n",
    "            'Intekglobal has its headquarters located in TJ',\n",
    "            'Intekglobal is in the north of mexico'\n",
    "        ]\n",
    "}\n",
    "\n",
    "intekglobal_faqs = {\n",
    "    'Question': ['Is Intekglobal an IT company?', 'Where can I apply?'],\n",
    "    'Answer':\n",
    "        ['Yes it is!', 'Please refer the our website for further information']\n",
    "}\n",
    "\n",
    "\n",
    "def mock_faq_files(tmpdirname, faqs):\n",
    "\n",
    "    faq_files = {\n",
    "        'data': path.join(tmpdirname, 'temp_faq.csv'),\n",
    "        'config': path.join(tmpdirname, 'temp_config_faq.json')\n",
    "    }\n",
    "\n",
    "    pd.DataFrame(faqs).to_csv(faq_files['data'], index=False)\n",
    "    copyfile(configs.faq.tfidf_logreg_en_faq, faq_files['config'])\n",
    "\n",
    "    updates_faq_config_file(\n",
    "        configs_path=faq_files['config'],\n",
    "        dataset_reader={'data_path': faq_files['data']}\n",
    "    )\n",
    "\n",
    "    return faq_files\n",
    "\n",
    "\n",
    "def test_get_intekglobal_responses():\n",
    "    with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "\n",
    "        faq_files = mock_faq_files(tmpdirname, intekglobal_faqs)\n",
    "        qa_models = load_qa_models(\n",
    "            config_tfidf=faq_files['config'], download=False\n",
    "        )\n",
    "\n",
    "        responses, format_responses = get_responses(\n",
    "            pd.DataFrame(intekglobal_context),\n",
    "            'Where is Intekglobal?',\n",
    "            qa_models,\n",
    "            nb_squad_results=2\n",
    "        )\n",
    "\n",
    "        logging.debug(responses)\n",
    "        logging.debug(format_responses)\n",
    "        assert all(\n",
    "            response in ('north of mexico', 'TJ', 'Yes it is!')\n",
    "            for response in responses\n",
    "        )\n",
    "        assert ''' Where is Intekglobal?:\n",
    "\n",
    "0: north of mexico\n",
    "1: TJ\n",
    "2: Yes it is!\n",
    "        '''.strip() == format_responses.strip()\n",
    "\n",
    "\n",
    "def test_get_empty_responses():\n",
    "    with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "        empty_faqs = {'Question': [], 'Answer': []}\n",
    "        faq_files = mock_faq_files(tmpdirname, empty_faqs)\n",
    "\n",
    "        qa_models = load_qa_models(\n",
    "            config_tfidf=faq_files['config'], download=False\n",
    "        )\n",
    "        empty_context = {'topic': [], 'context': []}\n",
    "\n",
    "        responses, format_responses = get_responses(\n",
    "            pd.DataFrame(empty_context),\n",
    "            'Where is Intekglobal?',\n",
    "            qa_models,\n",
    "            nb_squad_results=2\n",
    "        )\n",
    "\n",
    "        logging.debug(responses)\n",
    "        logging.debug(format_responses)\n",
    "        #assert all(\n",
    "        #    response in ('north of mexico', 'TJ', 'Yes it is!')\n",
    "        #    for response in responses\n",
    "        #)\n",
    "\n",
    "\n",
    "test_get_intekglobal_responses()\n",
    "test_get_empty_responses()\n",
    "\n",
    "del intekglobal_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_input(text):\n",
    "    '''This redundancy is needed for testing'''\n",
    "    return input(text)\n",
    "\n",
    "\n",
    "def new_answer(question, data, qa_models):\n",
    "\n",
    "    if get_input('Give a better anwser [y/n]?')[0].lower() != 'y':\n",
    "        return 'no data updates..'\n",
    "\n",
    "    if get_input('Give the answer as a context [y/n]?')[0].lower() == 'y':\n",
    "        new_context = pd.DataFrame(\n",
    "            {\n",
    "                'topic': [get_input('Give context a title:\\n')],\n",
    "                'context': [get_input('Introduce the context:\\n')]\n",
    "            }\n",
    "        )\n",
    "        data['context']['df'] = data['context']['df'].append(new_context)\n",
    "        data['context']['df'].to_csv(data['context']['path'])\n",
    "\n",
    "        return 'contexts dataset updated..'\n",
    "    else:\n",
    "        new_faq = pd.DataFrame(\n",
    "            {\n",
    "                'Question': [question],\n",
    "                'Answer': [get_input('Introduce the answer:\\n')]\n",
    "            }\n",
    "        )\n",
    "        data['faq']['df'] = data['faq']['df'].append(new_faq)\n",
    "        data['faq']['df'].to_csv(data['faq']['path'])\n",
    "        qa_models['faq']['tfidf'] = train_model(\n",
    "            configs.faq.tfidf_logreg_en_faq, download=False\n",
    "        )\n",
    "        return 'FAQ dataset and model updated..'\n",
    "\n",
    "\n",
    "def question_response(data, qa_models, num_returned_values_per_squad_model=1):\n",
    "    question = get_input('Introduce question:\\n')\n",
    "\n",
    "\n",
    "    _, formatted_responses = get_responses(\n",
    "            pd.DataFrame(intekglobal_context), 'Where is Intekglobal?',\n",
    "            qa_models,\n",
    "            nb_squad_results=2\n",
    "        )\n",
    "\n",
    "    return formatted_responses, new_answer(question, data, qa_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Test FAQ dialog system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'qa_models' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-7a53bbcc2785>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m \u001b[0mtest_context_response_with_no_updates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0mtest_updating_faq_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/unittest/mock.py\u001b[0m in \u001b[0;36mpatched\u001b[0;34m(*args, **keywargs)\u001b[0m\n\u001b[1;32m   1253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1254\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextra_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1255\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkeywargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1256\u001b[0m             \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1257\u001b[0m                 if (patching not in entered_patchers and\n",
      "\u001b[0;32m<ipython-input-31-7a53bbcc2785>\u001b[0m in \u001b[0;36mtest_context_response_with_no_updates\u001b[0;34m(mock_input)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtempfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTemporaryDirectory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtmpdirname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mcopy_data_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtmpdirname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0mresponses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquestion_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqa_models\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0;34m'no data updates..'\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0;34m'one of the smartest minds on the planet'\u001b[0m  \u001b[0;32min\u001b[0m \u001b[0mresponses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'qa_models' is not defined"
     ]
    }
   ],
   "source": [
    "import tempfile\n",
    "from unittest.mock import patch\n",
    "from shutil import copyfile\n",
    "example_contexts = pd.DataFrame(\n",
    "    {\n",
    "        'context':\n",
    "            [\n",
    "                'Intekglobal has its headquarters located in TJ',\n",
    "                'In Intekglobal we care about you',\n",
    "                'Enrique Jimenez is one of the smartest minds on the planet'\n",
    "            ]\n",
    "    },\n",
    ")\n",
    "\n",
    "data = {\n",
    "    'context': {\n",
    "        'df': example_contexts,\n",
    "        'path': ...\n",
    "    },\n",
    "    'faq': {\n",
    "        'df': ...,\n",
    "        'path': ...\n",
    "    }\n",
    "}\n",
    "\n",
    "FAQ_DATA_FILE = path.join(\n",
    "    popen('dirname $PWD').read().strip(), 'data/faq_data.csv'\n",
    ")\n",
    "\n",
    "def copy_data_files(data, tmpdirname):\n",
    "    data['context']['path'] = path.join(tmpdirname, 'tmp_context.csv')\n",
    "    data['faq']['path'] = path.join(tmpdirname, 'tmp_faq.csv')\n",
    "    data['faq']['df'] = pd.read_csv(FAQ_DATA_FILE)\n",
    "    data['context']['df'].to_csv(data['context']['path'])\n",
    "    copyfile(FAQ_DATA_FILE, data['faq']['path'])\n",
    "\n",
    "\n",
    "@patch('__main__.get_input')\n",
    "def test_context_response_with_no_updates(mock_input):\n",
    "    mock_input.side_effect = ['Who is Enrique Jimenez?', 'N']\n",
    "\n",
    "    with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "        copy_data_files(data, tmpdirname)\n",
    "        responses, status = question_response(data, qa_models)\n",
    "        assert 'no data updates..' == status\n",
    "        assert 'one of the smartest minds on the planet'  in responses\n",
    "\n",
    "\n",
    "@patch('__main__.train_model')\n",
    "@patch('__main__.get_input')\n",
    "def test_updating_faq_dataset(mock_input, mock_train_model):\n",
    "\n",
    "    new_answer = 'Intekglobal is one of the best companies in the world'\n",
    "    mock_input.side_effect = ['What is Intekglobal?', 'Y', 'N', new_answer]\n",
    "    qa_model_faq = qa_models['faq']['tfidf']\n",
    "    try:\n",
    "\n",
    "        with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "            copy_data_files(data, tmpdirname)\n",
    "\n",
    "            assert 'FAQ dataset and model updated..' == question_response(\n",
    "                data, qa_models\n",
    "            )[1]\n",
    "\n",
    "            updated_faq = pd.read_csv(data['faq']['path'])\n",
    "\n",
    "            assert updated_faq[updated_faq['Answer'] == new_answer\n",
    "                              ].shape[0] == 1\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    finally:\n",
    "        qa_models['faq']['tfidf'] = qa_model_faq\n",
    "\n",
    "\n",
    "test_context_response_with_no_updates()\n",
    "test_updating_faq_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "from unittest.mock import patch\n",
    "from shutil import copyfile, copytree\n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "HOME_DIR = popen('echo $HOME').read().strip()\n",
    "\n",
    "example_contexts = pd.DataFrame(\n",
    "    {\n",
    "        'context':\n",
    "            [\n",
    "                'Intekglobal has its headquarters located in TJ',\n",
    "                'In Intekglobal we care about you'\n",
    "            ]\n",
    "    },\n",
    ")\n",
    "\n",
    "data = {\n",
    "    'context': {\n",
    "        'df': example_contexts,\n",
    "        'path': ...\n",
    "    },\n",
    "    'faq': {\n",
    "        'df': ...,\n",
    "        'path': ...\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "FAQ_DATA_FILE = path.join(\n",
    "    popen('dirname $PWD').read().strip(), 'data/faq_data.csv'\n",
    ")\n",
    "\n",
    "CONTEXT_DATA_FILE = path.join(\n",
    "    popen('dirname $PWD').read().strip(), 'data/context_data.csv'\n",
    ")\n",
    "\n",
    "\n",
    "def copy_data_files(data, tmpdirname):\n",
    "    data['context']['path'] = path.join(tmpdirname, 'tmp_context.csv')\n",
    "    data['faq']['path'] = path.join(tmpdirname, 'tmp_faq.csv')\n",
    "    data['faq']['df'] = pd.read_csv(FAQ_DATA_FILE)\n",
    "    copyfile(CONTEXT_DATA_FILE, data['context']['path'])\n",
    "    copyfile(FAQ_DATA_FILE, data['faq']['path'])\n",
    "\n",
    "\n",
    "def modify_configs(data, tmpdirname):\n",
    "\n",
    "    copy_data_files(data, tmpdirname)\n",
    "\n",
    "    tmp_configs_faq = path.join(tmpdirname, 'temp_config_faq.json')\n",
    "    tmp_model_dir = path.join(tmpdirname, 'temp_models_dir')\n",
    "\n",
    "    metadata = json.load(open(configs.faq.tfidf_logreg_en_faq)\n",
    "                        )['metadata']['variables']\n",
    "\n",
    "    models_dir = metadata['MODELS_PATH'].replace(\n",
    "        '{ROOT_PATH}', metadata['ROOT_PATH'].replace('~', HOME_DIR)\n",
    "    )\n",
    "\n",
    "    copytree(models_dir, tmp_model_dir)\n",
    "    copyfile(configs.faq.tfidf_logreg_en_faq, tmp_configs_faq)\n",
    "\n",
    "    configs.faq.tfidf_logreg_en_faq = tmp_configs_faq\n",
    "\n",
    "    updates_faq_config_file(\n",
    "        metadata={'variables': {\n",
    "            'MODELS_PATH': tmp_model_dir\n",
    "        }},\n",
    "        dataset_reader={'data_path': data['faq']['path']}\n",
    "    )\n",
    "    #pprint(json.load(open(configs.faq.tfidf_logreg_en_faq)))\n",
    "    assert path.isdir(tmp_model_dir)\n",
    "    assert path.isfile(configs.faq.tfidf_logreg_en_faq)\n",
    "\n",
    "\n",
    "@patch('__main__.get_input')\n",
    "def test_faq_answer_with_updating(mock_input):\n",
    "\n",
    "    new_answer = 'Intekglobal is one of the best companies in the world'\n",
    "    question = 'What is Intekglobal?'\n",
    "    mock_input.side_effect = [question, 'Y', 'N', new_answer, question, 'N']\n",
    "\n",
    "    original_config_file = configs.faq.tfidf_logreg_en_faq\n",
    "    qa_model_faq = qa_models['faq']['tfidf']\n",
    "\n",
    "    with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "\n",
    "        try:\n",
    "            modify_configs(data, tmpdirname)\n",
    "    \n",
    "            old_responses = question_response(data, qa_models)[0]\n",
    "            new_responses = question_response(data, qa_models)[0]\n",
    "           \n",
    "            logging.info(f'Old response:\\n {old_responses}')\n",
    "            logging.info(f'New response:\\n{new_responses}')\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "        finally:\n",
    "            configs.faq.tfidf_logreg_en_faq = original_config_file\n",
    "            qa_models['faq']['tfidf'] = qa_model_faq\n",
    "\n",
    "        assert new_answer not in old_responses\n",
    "        assert new_answer in new_responses\n",
    "\n",
    "\n",
    "test_faq_answer_with_updating()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Test Context Dialog System\n",
    "import tempfile\n",
    "from unittest.mock import patch\n",
    "from shutil import copyfile, copytree\n",
    "from pprint import pprint\n",
    "\n",
    "example_contexts = pd.DataFrame(\n",
    "    {\n",
    "        'topic': ['Headquarters', 'Mision'],\n",
    "        'context':\n",
    "            [\n",
    "                'Intekglobal has its headquarters located in TJ',\n",
    "                'In Intekglobal we care about you'\n",
    "            ]\n",
    "    }\n",
    ")\n",
    "\n",
    "data = {\n",
    "    'context': {\n",
    "        'df': example_contexts,\n",
    "        'path': ...\n",
    "    },\n",
    "    'faq': {\n",
    "        'df': ...,\n",
    "        'path': ...\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "FAQ_DATA_FILE = path.join(\n",
    "    popen('dirname $PWD').read().strip(), 'data/faq_data.csv'\n",
    ")\n",
    "\n",
    "CONTEXT_DATA_FILE = path.join(\n",
    "    popen('dirname $PWD').read().strip(), 'data/context_data.csv'\n",
    ")\n",
    "\n",
    "\n",
    "def copy_data_files(data, tmpdirname):\n",
    "    data['context']['path'] = path.join(tmpdirname, 'tmp_context.csv')\n",
    "    data['faq']['path'] = path.join(tmpdirname, 'tmp_faq.csv')\n",
    "    data['faq']['df'] = pd.read_csv(FAQ_DATA_FILE)\n",
    "    copyfile(CONTEXT_DATA_FILE, data['context']['path'])\n",
    "    copyfile(FAQ_DATA_FILE, data['faq']['path'])\n",
    "\n",
    "\n",
    "@patch('__main__.get_input')\n",
    "def test_context_new_answer(mock_input):\n",
    "\n",
    "    question = 'What is a Chatbot?'\n",
    "    new_topic = 'AI Tool & Chatbot Development'\n",
    "    new_context = '''\n",
    "\n",
    "A chatbot is an important tool for simulating intelligent conversations with humans.\n",
    "Intekglobal chatbots efficiently live message on platforms such as Facebook Messenger, \n",
    "Slack, and Telegram. Assisting consumers with a variety of purposes and industries. \n",
    "\n",
    "But chatbots are more than just a cool technology advancement. They actually transform the user experience.\n",
    "People want simple and convenient interactions with interface and products.\n",
    "\n",
    "'''\n",
    "\n",
    "    mock_input.side_effect = [\n",
    "        question, 'YES', 'yes', new_topic, new_context, question, 'N'\n",
    "    ]\n",
    "\n",
    "    with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "\n",
    "        try:\n",
    "\n",
    "            copy_data_files(data, tmpdirname)\n",
    "\n",
    "            old_responses = question_response(data, qa_models)[0]\n",
    "            logging.info(f'Old response:\\n {old_responses}')\n",
    "            new_responses = question_response(data, qa_models)[0]\n",
    "            logging.info(f'New response:\\n{new_responses}')\n",
    "        except Exception as e:\n",
    "            print(repr(e))\n",
    "\n",
    "        finally:\n",
    "            logging.info(' Test finished')\n",
    "\n",
    "        updated_context = pd.read_csv(data['context']['path'])\n",
    "\n",
    "        assert updated_context[updated_context['context'] == new_context\n",
    "                              ].shape[0] == 1\n",
    "\n",
    "        assert updated_context[updated_context['topic'] == new_topic\n",
    "                              ].shape[0] == 1\n",
    "        assert 'an important tool for simulating intelligent conversations with humans' not in old_responses\n",
    "        assert 'an important tool for simulating intelligent conversations with humans' in new_responses\n",
    "\n",
    "\n",
    "test_context_new_answer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def dialog_system(context_data_file=None, faq_data_file=None):\n",
    "    '''\n",
    "     Main Dialog System\n",
    "    '''\n",
    "\n",
    "    PARENT_DIR = popen('dirname $PWD').read().strip()\n",
    "    if context_data_file is None:\n",
    "        context_data_file = path.join(PARENT_DIR, 'data/context_data.csv')\n",
    "    if faq_data_file is None:\n",
    "        faq_data_file = path.join(PARENT_DIR, 'data/faq_data.csv')\n",
    "    \n",
    "    run_shell_installs()\n",
    "    updates_faq_config_file(dataset_reader={'data_path': faq_data_file}) \n",
    "    qa_models = load_qa_models()\n",
    "\n",
    "    context = {'df': pd.read_csv(CONTEXT_DATA_FILE), 'path': CONTEXT_DATA_FILE}\n",
    "    faq = {'df': pd.read_csv(FAQ_DATA_FILE), 'path': FAQ_DATA_FILE}\n",
    "\n",
    "    data = {'context': context, 'faq': faq}\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            question_response(data=data, qa_models=qa_models)\n",
    "        except (KeyboardInterrupt, EOFError, SystemExit):\n",
    "            logging.debug('Goodbye!')\n",
    "            return 'Goodbye!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test  dialog_system()\n",
    "\n",
    "from unittest.mock import patch\n",
    "\n",
    "@patch('__main__.run_shell_installs')\n",
    "@patch('__main__.load_qa_models')\n",
    "@patch('__main__.pd.read_csv')\n",
    "@patch('__main__.question_response')\n",
    "def test_main_keyboard_interrupt(\n",
    "    mock_question_response,\n",
    "    mock_pd_read_csv,\n",
    "    mock_load_qa_models,\n",
    "    mock_run_shell_installs,\n",
    "):\n",
    "    mock_question_response.side_effect = [\n",
    "        KeyboardInterrupt(), EOFError(),\n",
    "        SystemExit()\n",
    "    ]\n",
    "    assert 'Goodbye!' == dialog_system()\n",
    "    assert 'Goodbye!' == dialog_system()\n",
    "    assert 'Goodbye!' == dialog_system()\n",
    "\n",
    "\n",
    "test_main_keyboard_interrupt()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
