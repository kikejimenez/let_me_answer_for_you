{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10:11:25 INFO:test: hola\n",
      "10:11:25 ERROR:test: adios\n"
     ]
    }
   ],
   "source": [
    "# export\n",
    "from deeppavlov import configs,build_model,train_model\n",
    "import json\n",
    "from os import path\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s %(levelname)s:%(message)s', level=logging.DEBUG, datefmt='%I:%M:%S')\n",
    "logging.info(\"test: hola\")\n",
    "logging.error('test: adios')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-09 22:11:35.793 INFO in 'deeppavlov.core.common.file'['file'] at line 32: Interpreting 'squad' as '/opt/conda/lib/python3.7/site-packages/deeppavlov/configs/squad/squad.json'\n",
      "2020-06-09 22:11:39.555 INFO in 'deeppavlov.core.common.file'['file'] at line 32: Interpreting 'squad_bert' as '/opt/conda/lib/python3.7/site-packages/deeppavlov/configs/squad/squad_bert.json'\n",
      "  Running command git clone -q https://github.com/deepmipt/bert.git /tmp/pip-req-build-n6xueuk8\n",
      "  Running command git checkout -b feat/multi_gpu --track origin/feat/multi_gpu\n",
      "  Switched to a new branch 'feat/multi_gpu'\n",
      "  Branch 'feat/multi_gpu' set up to track remote branch 'feat/multi_gpu' from 'origin'.\n",
      "2020-06-09 22:11:48.138 INFO in 'deeppavlov.core.common.file'['file'] at line 32: Interpreting 'fasttext_avg_autofaq' as '/opt/conda/lib/python3.7/site-packages/deeppavlov/configs/faq/fasttext_avg_autofaq.json'\n",
      "2020-06-09 22:11:51.207 INFO in 'deeppavlov.core.common.file'['file'] at line 32: Interpreting 'fasttext_tfidf_autofaq' as '/opt/conda/lib/python3.7/site-packages/deeppavlov/configs/faq/fasttext_tfidf_autofaq.json'\n",
      "2020-06-09 22:11:54.305 INFO in 'deeppavlov.core.common.file'['file'] at line 32: Interpreting 'tfidf_autofaq' as '/opt/conda/lib/python3.7/site-packages/deeppavlov/configs/faq/tfidf_autofaq.json'\n",
      "2020-06-09 22:11:54.306 WARNING in 'deeppavlov.utils.pip_wrapper.pip_wrapper'['pip_wrapper'] at line 59: No requirements found in config\n",
      "2020-06-09 22:11:55.987 INFO in 'deeppavlov.core.common.file'['file'] at line 32: Interpreting 'tfidf_logreg_autofaq' as '/opt/conda/lib/python3.7/site-packages/deeppavlov/configs/faq/tfidf_logreg_autofaq.json'\n",
      "2020-06-09 22:11:55.988 WARNING in 'deeppavlov.utils.pip_wrapper.pip_wrapper'['pip_wrapper'] at line 59: No requirements found in config\n",
      "2020-06-09 22:11:57.699 INFO in 'deeppavlov.core.common.file'['file'] at line 32: Interpreting 'tfidf_logreg_en_faq' as '/opt/conda/lib/python3.7/site-packages/deeppavlov/configs/faq/tfidf_logreg_en_faq.json'\n"
     ]
    }
   ],
   "source": [
    "!pip install deeppavlov > /dev/null\n",
    "!python -m deeppavlov install squad > /dev/null\n",
    "!python -m deeppavlov install squad_bert > /dev/null\n",
    "!python -m deeppavlov install fasttext_avg_autofaq > /dev/null\n",
    "!python -m deeppavlov install fasttext_tfidf_autofaq > /dev/null\n",
    "!python -m deeppavlov install tfidf_autofaq > /dev/null\n",
    "!python -m deeppavlov install tfidf_logreg_autofaq > /dev/null\n",
    "!python -m deeppavlov install tfidf_logreg_en_faq > /dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dialog System\n",
    "> Question Answering Automatic System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def replacement_f(model_config,**args):\n",
    "    for k,v in args.items():\n",
    "        if isinstance(v,dict):\n",
    "            replacement_f(model_config[k],**v)\n",
    "        else:\n",
    "            model_config[k] = v\n",
    "            \n",
    "def gen_faq_config_file(**args):\n",
    "\n",
    "    #set FAQ data in config file\n",
    "    model_config = json.load(open(configs.faq.tfidf_logreg_en_faq))\n",
    "    if 'data_url' in model_config['dataset_reader']:\n",
    "        del model_config['dataset_reader']['data_url']\n",
    "    \n",
    "    replacement_f(model_config,**args)\n",
    "\n",
    "    json.dump(model_config, open(configs.faq.tfidf_logreg_en_faq, 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get notebook directory\n",
    "current_dir = !pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "faq_data_file =  path.join(current_dir[0], 'faq_data.csv')\n",
    "gen_faq_config_file(\n",
    "    dataset_reader={'data_path':faq_data_file}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def load_qa_models():\n",
    "    qa_models = {\n",
    "        'squad':\n",
    "            {\n",
    "                'rnet': build_model(configs.squad.squad, download=True),\n",
    "                'bert': build_model(configs.squad.squad_bert, download=True)\n",
    "            },\n",
    "        'faq':\n",
    "            {\n",
    "                'tfidf':\n",
    "                    train_model(configs.faq.tfidf_logreg_en_faq, download=True)\n",
    "            }\n",
    "    }\n",
    "    return qa_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10:12:01 DEBUG:Starting new HTTP connection (1): files.deeppavlov.ai:80\n",
      "10:12:01 DEBUG:http://files.deeppavlov.ai:80 \"GET /embeddings/wiki-news-300d-1M.vec.md5 HTTP/1.1\" 200 56\n",
      "2020-06-09 22:12:19.500 INFO in 'deeppavlov.download'['download'] at line 117: Skipped http://files.deeppavlov.ai/embeddings/wiki-news-300d-1M.vec download because of matching hashes\n",
      "10:12:19 INFO:Skipped http://files.deeppavlov.ai/embeddings/wiki-news-300d-1M.vec download because of matching hashes\n",
      "10:12:19 DEBUG:Starting new HTTP connection (1): files.deeppavlov.ai:80\n",
      "10:12:19 DEBUG:http://files.deeppavlov.ai:80 \"GET /embeddings/wiki-news-300d-1M-char.vec.md5 HTTP/1.1\" 200 61\n",
      "2020-06-09 22:12:20.53 INFO in 'deeppavlov.download'['download'] at line 117: Skipped http://files.deeppavlov.ai/embeddings/wiki-news-300d-1M-char.vec download because of matching hashes\n",
      "10:12:20 INFO:Skipped http://files.deeppavlov.ai/embeddings/wiki-news-300d-1M-char.vec download because of matching hashes\n",
      "10:12:20 DEBUG:Starting new HTTP connection (1): files.deeppavlov.ai:80\n",
      "10:12:20 DEBUG:http://files.deeppavlov.ai:80 \"GET /deeppavlov_data/squad_model_1.4_cpu_compatible.tar.gz.md5 HTTP/1.1\" 200 389\n",
      "2020-06-09 22:12:22.471 INFO in 'deeppavlov.download'['download'] at line 117: Skipped http://files.deeppavlov.ai/deeppavlov_data/squad_model_1.4_cpu_compatible.tar.gz download because of matching hashes\n",
      "10:12:22 INFO:Skipped http://files.deeppavlov.ai/deeppavlov_data/squad_model_1.4_cpu_compatible.tar.gz download because of matching hashes\n",
      "[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package perluniprops to\n",
      "[nltk_data]     /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package perluniprops is already up-to-date!\n",
      "[nltk_data] Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]     /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package nonbreaking_prefixes is already up-to-date!\n",
      "2020-06-09 22:12:24.618 INFO in 'deeppavlov.models.preprocessors.squad_preprocessor'['squad_preprocessor'] at line 310: SquadVocabEmbedder: loading saved tokens vocab from /home/jovyan/.deeppavlov/models/squad_model/emb/vocab_embedder.pckl\n",
      "10:12:24 INFO:SquadVocabEmbedder: loading saved tokens vocab from /home/jovyan/.deeppavlov/models/squad_model/emb/vocab_embedder.pckl\n",
      "2020-06-09 22:12:24.950 INFO in 'deeppavlov.models.preprocessors.squad_preprocessor'['squad_preprocessor'] at line 310: SquadVocabEmbedder: loading saved chars vocab from /home/jovyan/.deeppavlov/models/squad_model/emb/char_vocab_embedder.pckl\n",
      "10:12:24 INFO:SquadVocabEmbedder: loading saved chars vocab from /home/jovyan/.deeppavlov/models/squad_model/emb/char_vocab_embedder.pckl\n",
      "10:12:32 WARNING:From /opt/conda/lib/python3.7/site-packages/deeppavlov/core/models/tf_model.py:37: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "10:12:32 WARNING:From /opt/conda/lib/python3.7/site-packages/deeppavlov/core/models/tf_model.py:222: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "10:12:32 WARNING:From /opt/conda/lib/python3.7/site-packages/deeppavlov/core/models/tf_model.py:222: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "10:12:32 WARNING:From /opt/conda/lib/python3.7/site-packages/deeppavlov/core/models/tf_model.py:193: The name tf.train.AdadeltaOptimizer is deprecated. Please use tf.compat.v1.train.AdadeltaOptimizer instead.\n",
      "\n",
      "10:12:32 WARNING:From /opt/conda/lib/python3.7/site-packages/deeppavlov/core/common/check_gpu.py:29: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "10:12:33 WARNING:From /opt/conda/lib/python3.7/site-packages/deeppavlov/models/squad/squad.py:224: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "10:12:33 WARNING:From /opt/conda/lib/python3.7/site-packages/deeppavlov/models/squad/squad.py:102: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n",
      "10:12:33 WARNING:From /opt/conda/lib/python3.7/site-packages/deeppavlov/models/squad/squad.py:139: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "2020-06-09 22:12:33.753 INFO in 'deeppavlov.core.layers.tf_layers'['tf_layers'] at line 615: \n",
      "Warning! tf.contrib.cudnn_rnn.CudnnCompatibleGRUCell is used. It is okay for inference mode, but if you train your model with this cell it could NOT be used with tf.contrib.cudnn_rnn.CudnnGRUCell later. \n",
      "10:12:33 INFO:\n",
      "Warning! tf.contrib.cudnn_rnn.CudnnCompatibleGRUCell is used. It is okay for inference mode, but if you train your model with this cell it could NOT be used with tf.contrib.cudnn_rnn.CudnnGRUCell later. \n",
      "10:12:33 WARNING:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "10:12:36 WARNING:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/contrib/cudnn_rnn/python/ops/cudnn_rnn_ops.py:122: GRUCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.GRUCell, and will be replaced by that in Tensorflow 2.0.\n",
      "10:12:36 WARNING:From /opt/conda/lib/python3.7/site-packages/deeppavlov/core/layers/tf_layers.py:591: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n",
      "10:12:36 WARNING:From /opt/conda/lib/python3.7/site-packages/deeppavlov/core/layers/tf_layers.py:596: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "10:12:36 WARNING:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/contrib/cudnn_rnn/python/ops/cudnn_rnn_ops.py:133: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.add_weight` method instead.\n",
      "10:12:36 WARNING:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/contrib/cudnn_rnn/python/ops/cudnn_rnn_ops.py:139: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "10:12:36 WARNING:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/contrib/cudnn_rnn/python/ops/cudnn_rnn_ops.py:155: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "10:12:36 WARNING:From /opt/conda/lib/python3.7/site-packages/deeppavlov/core/layers/tf_layers.py:808: calling reverse_sequence (from tensorflow.python.ops.array_ops) with seq_dim is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "seq_dim is deprecated, use seq_axis instead\n",
      "10:12:36 WARNING:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/util/deprecation.py:507: calling reverse_sequence (from tensorflow.python.ops.array_ops) with batch_dim is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "batch_dim is deprecated, use batch_axis instead\n",
      "2020-06-09 22:12:36.600 INFO in 'deeppavlov.core.layers.tf_layers'['tf_layers'] at line 615: \n",
      "Warning! tf.contrib.cudnn_rnn.CudnnCompatibleGRUCell is used. It is okay for inference mode, but if you train your model with this cell it could NOT be used with tf.contrib.cudnn_rnn.CudnnGRUCell later. \n",
      "10:12:36 INFO:\n",
      "Warning! tf.contrib.cudnn_rnn.CudnnCompatibleGRUCell is used. It is okay for inference mode, but if you train your model with this cell it could NOT be used with tf.contrib.cudnn_rnn.CudnnGRUCell later. \n",
      "2020-06-09 22:12:36.813 INFO in 'deeppavlov.core.layers.tf_layers'['tf_layers'] at line 615: \n",
      "Warning! tf.contrib.cudnn_rnn.CudnnCompatibleGRUCell is used. It is okay for inference mode, but if you train your model with this cell it could NOT be used with tf.contrib.cudnn_rnn.CudnnGRUCell later. \n",
      "10:12:36 INFO:\n",
      "Warning! tf.contrib.cudnn_rnn.CudnnCompatibleGRUCell is used. It is okay for inference mode, but if you train your model with this cell it could NOT be used with tf.contrib.cudnn_rnn.CudnnGRUCell later. \n",
      "2020-06-09 22:12:36.950 INFO in 'deeppavlov.core.layers.tf_layers'['tf_layers'] at line 615: \n",
      "Warning! tf.contrib.cudnn_rnn.CudnnCompatibleGRUCell is used. It is okay for inference mode, but if you train your model with this cell it could NOT be used with tf.contrib.cudnn_rnn.CudnnGRUCell later. \n",
      "10:12:36 INFO:\n",
      "Warning! tf.contrib.cudnn_rnn.CudnnCompatibleGRUCell is used. It is okay for inference mode, but if you train your model with this cell it could NOT be used with tf.contrib.cudnn_rnn.CudnnGRUCell later. \n",
      "10:12:37 WARNING:From /opt/conda/lib/python3.7/site-packages/deeppavlov/models/squad/utils.py:87: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "10:12:37 WARNING:From /opt/conda/lib/python3.7/site-packages/deeppavlov/models/squad/utils.py:101: The name tf.AUTO_REUSE is deprecated. Please use tf.compat.v1.AUTO_REUSE instead.\n",
      "\n",
      "10:12:38 WARNING:From /opt/conda/lib/python3.7/site-packages/deeppavlov/models/squad/utils.py:171: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.Dense instead.\n",
      "10:12:38 WARNING:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/layers/core.py:187: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n",
      "10:12:39 WARNING:From /opt/conda/lib/python3.7/site-packages/deeppavlov/models/squad/utils.py:139: The name tf.get_variable_scope is deprecated. Please use tf.compat.v1.get_variable_scope instead.\n",
      "\n",
      "10:12:39 WARNING:From /opt/conda/lib/python3.7/site-packages/deeppavlov/models/squad/squad.py:203: The name tf.matrix_band_part is deprecated. Please use tf.linalg.band_part instead.\n",
      "\n",
      "10:12:39 WARNING:From /opt/conda/lib/python3.7/site-packages/deeppavlov/models/squad/squad.py:212: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "10:12:39 WARNING:From /opt/conda/lib/python3.7/site-packages/deeppavlov/core/models/tf_model.py:232: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n",
      "10:12:39 WARNING:From /opt/conda/lib/python3.7/site-packages/deeppavlov/core/models/tf_model.py:127: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n",
      "\n",
      "10:12:39 WARNING:From /opt/conda/lib/python3.7/site-packages/deeppavlov/core/models/tf_model.py:127: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
      "\n",
      "10:12:46 WARNING:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/ops/clip_ops.py:172: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "10:12:48 WARNING:From /opt/conda/lib/python3.7/site-packages/deeppavlov/models/squad/squad.py:93: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
      "\n",
      "10:12:50 WARNING:From /opt/conda/lib/python3.7/site-packages/deeppavlov/core/models/tf_model.py:50: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "2020-06-09 22:12:50.808 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 51: [loading model from /home/jovyan/.deeppavlov/models/squad_model/model]\n",
      "10:12:50 INFO:[loading model from /home/jovyan/.deeppavlov/models/squad_model/model]\n",
      "10:12:50 WARNING:From /opt/conda/lib/python3.7/site-packages/deeppavlov/core/models/tf_model.py:54: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
      "\n",
      "10:12:50 INFO:Restoring parameters from /home/jovyan/.deeppavlov/models/squad_model/model\n",
      "10:12:51 DEBUG:Starting new HTTP connection (1): files.deeppavlov.ai:80\n",
      "10:12:52 DEBUG:http://files.deeppavlov.ai:80 \"GET /deeppavlov_data/squad_bert.tar.gz.md5 HTTP/1.1\" 200 184\n",
      "2020-06-09 22:12:53.87 INFO in 'deeppavlov.download'['download'] at line 117: Skipped http://files.deeppavlov.ai/deeppavlov_data/squad_bert.tar.gz download because of matching hashes\n",
      "10:12:53 INFO:Skipped http://files.deeppavlov.ai/deeppavlov_data/squad_bert.tar.gz download because of matching hashes\n",
      "10:12:53 DEBUG:Starting new HTTP connection (1): files.deeppavlov.ai:80\n",
      "10:12:53 DEBUG:http://files.deeppavlov.ai:80 \"GET /deeppavlov_data/bert/cased_L-12_H-768_A-12.zip.md5 HTTP/1.1\" 200 386\n",
      "2020-06-09 22:12:58.168 INFO in 'deeppavlov.download'['download'] at line 117: Skipped http://files.deeppavlov.ai/deeppavlov_data/bert/cased_L-12_H-768_A-12.zip download because of matching hashes\n",
      "10:12:58 INFO:Skipped http://files.deeppavlov.ai/deeppavlov_data/bert/cased_L-12_H-768_A-12.zip download because of matching hashes\n",
      "10:12:58 WARNING:From /opt/conda/lib/python3.7/site-packages/bert_dp/tokenization.py:125: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
      "\n",
      "10:12:58 WARNING:From /opt/conda/lib/python3.7/site-packages/bert_dp/modeling.py:499: The name tf.assert_less_equal is deprecated. Please use tf.compat.v1.assert_less_equal instead.\n",
      "\n",
      "10:12:58 WARNING:From /opt/conda/lib/python3.7/site-packages/bert_dp/modeling.py:283: The name tf.erf is deprecated. Please use tf.math.erf instead.\n",
      "\n",
      "2020-06-09 22:13:16.41 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 51: [loading model from /home/jovyan/.deeppavlov/models/squad_bert/model]\n",
      "10:13:16 INFO:[loading model from /home/jovyan/.deeppavlov/models/squad_bert/model]\n",
      "10:13:16 INFO:Restoring parameters from /home/jovyan/.deeppavlov/models/squad_bert/model\n",
      "10:13:17 DEBUG:Starting new HTTP connection (1): files.deeppavlov.ai:80\n",
      "10:13:17 DEBUG:http://files.deeppavlov.ai:80 \"GET /faq/mipt/en_mipt_faq_v4.tar.gz.md5 HTTP/1.1\" 200 189\n",
      "10:13:17 DEBUG:Starting new HTTP connection (1): files.deeppavlov.ai:80\n",
      "10:13:18 DEBUG:http://files.deeppavlov.ai:80 \"GET /faq/mipt/en_mipt_faq_v4.tar.gz HTTP/1.1\" 200 12276\n",
      "2020-06-09 22:13:18.234 INFO in 'deeppavlov.core.data.utils'['utils'] at line 80: Downloading from http://files.deeppavlov.ai/faq/mipt/en_mipt_faq_v4.tar.gz to /home/jovyan/.deeppavlov/models/faq/en_mipt_faq_v4.tar.gz\n",
      "10:13:18 INFO:Downloading from http://files.deeppavlov.ai/faq/mipt/en_mipt_faq_v4.tar.gz to /home/jovyan/.deeppavlov/models/faq/en_mipt_faq_v4.tar.gz\n",
      "100%|██████████| 12.3k/12.3k [00:00<00:00, 5.40MB/s]\n",
      "2020-06-09 22:13:18.251 INFO in 'deeppavlov.core.data.utils'['utils'] at line 242: Extracting /home/jovyan/.deeppavlov/models/faq/en_mipt_faq_v4.tar.gz archive into /home/jovyan/.deeppavlov/models/faq/mipt\n",
      "10:13:18 INFO:Extracting /home/jovyan/.deeppavlov/models/faq/en_mipt_faq_v4.tar.gz archive into /home/jovyan/.deeppavlov/models/faq/mipt\n",
      "2020-06-09 22:13:20.877 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 203: Loading model sklearn.feature_extraction.text:TfidfVectorizer from /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/tfidf.pkl\n",
      "10:13:20 INFO:Loading model sklearn.feature_extraction.text:TfidfVectorizer from /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/tfidf.pkl\n",
      "2020-06-09 22:13:20.882 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 210: Model sklearn.feature_extraction.textTfidfVectorizer loaded  with parameters\n",
      "10:13:20 INFO:Model sklearn.feature_extraction.textTfidfVectorizer loaded  with parameters\n",
      "2020-06-09 22:13:20.886 WARNING in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 216: Fitting of loaded model can not be continued. Model can be fitted from scratch.If one needs to continue fitting, please, look at `warm_start` parameter\n",
      "10:13:20 WARNING:Fitting of loaded model can not be continued. Model can be fitted from scratch.If one needs to continue fitting, please, look at `warm_start` parameter\n",
      "2020-06-09 22:13:20.955 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 109: Fitting model sklearn.feature_extraction.textTfidfVectorizer\n",
      "10:13:20 INFO:Fitting model sklearn.feature_extraction.textTfidfVectorizer\n",
      "2020-06-09 22:13:20.963 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 241: Saving model to /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/tfidf.pkl\n",
      "10:13:20 INFO:Saving model to /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/tfidf.pkl\n",
      "2020-06-09 22:13:20.968 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 115: [loading vocabulary from /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/en_mipt_answers.dict]\n",
      "10:13:20 INFO:[loading vocabulary from /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/en_mipt_answers.dict]\n",
      "2020-06-09 22:13:20.971 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 101: [saving vocabulary to /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/en_mipt_answers.dict]\n",
      "10:13:20 INFO:[saving vocabulary to /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/en_mipt_answers.dict]\n",
      "2020-06-09 22:13:20.974 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 203: Loading model sklearn.linear_model:LogisticRegression from /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/logreg.pkl\n",
      "10:13:20 INFO:Loading model sklearn.linear_model:LogisticRegression from /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/logreg.pkl\n",
      "2020-06-09 22:13:20.977 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 210: Model sklearn.linear_model.logisticLogisticRegression loaded  with parameters\n",
      "10:13:20 INFO:Model sklearn.linear_model.logisticLogisticRegression loaded  with parameters\n",
      "2020-06-09 22:13:20.978 WARNING in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 216: Fitting of loaded model can not be continued. Model can be fitted from scratch.If one needs to continue fitting, please, look at `warm_start` parameter\n",
      "10:13:20 WARNING:Fitting of loaded model can not be continued. Model can be fitted from scratch.If one needs to continue fitting, please, look at `warm_start` parameter\n",
      "2020-06-09 22:13:21.26 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 109: Fitting model sklearn.linear_model.logisticLogisticRegression\n",
      "10:13:21 INFO:Fitting model sklearn.linear_model.logisticLogisticRegression\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "2020-06-09 22:13:21.51 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 241: Saving model to /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/logreg.pkl\n",
      "10:13:21 INFO:Saving model to /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/logreg.pkl\n",
      "2020-06-09 22:13:21.885 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 203: Loading model sklearn.feature_extraction.text:TfidfVectorizer from /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/tfidf.pkl\n",
      "10:13:21 INFO:Loading model sklearn.feature_extraction.text:TfidfVectorizer from /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/tfidf.pkl\n",
      "2020-06-09 22:13:21.887 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 210: Model sklearn.feature_extraction.textTfidfVectorizer loaded  with parameters\n",
      "10:13:21 INFO:Model sklearn.feature_extraction.textTfidfVectorizer loaded  with parameters\n",
      "2020-06-09 22:13:21.888 WARNING in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 216: Fitting of loaded model can not be continued. Model can be fitted from scratch.If one needs to continue fitting, please, look at `warm_start` parameter\n",
      "10:13:21 WARNING:Fitting of loaded model can not be continued. Model can be fitted from scratch.If one needs to continue fitting, please, look at `warm_start` parameter\n",
      "2020-06-09 22:13:21.890 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 115: [loading vocabulary from /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/en_mipt_answers.dict]\n",
      "10:13:21 INFO:[loading vocabulary from /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/en_mipt_answers.dict]\n",
      "2020-06-09 22:13:21.892 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 203: Loading model sklearn.linear_model:LogisticRegression from /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/logreg.pkl\n",
      "10:13:21 INFO:Loading model sklearn.linear_model:LogisticRegression from /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/logreg.pkl\n",
      "2020-06-09 22:13:21.893 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 210: Model sklearn.linear_model.logisticLogisticRegression loaded  with parameters\n",
      "10:13:21 INFO:Model sklearn.linear_model.logisticLogisticRegression loaded  with parameters\n",
      "2020-06-09 22:13:21.893 WARNING in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 216: Fitting of loaded model can not be continued. Model can be fitted from scratch.If one needs to continue fitting, please, look at `warm_start` parameter\n",
      "10:13:21 WARNING:Fitting of loaded model can not be continued. Model can be fitted from scratch.If one needs to continue fitting, please, look at `warm_start` parameter\n",
      "2020-06-09 22:13:22.709 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 203: Loading model sklearn.feature_extraction.text:TfidfVectorizer from /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/tfidf.pkl\n",
      "10:13:22 INFO:Loading model sklearn.feature_extraction.text:TfidfVectorizer from /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/tfidf.pkl\n",
      "2020-06-09 22:13:22.710 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 210: Model sklearn.feature_extraction.textTfidfVectorizer loaded  with parameters\n",
      "10:13:22 INFO:Model sklearn.feature_extraction.textTfidfVectorizer loaded  with parameters\n",
      "2020-06-09 22:13:22.711 WARNING in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 216: Fitting of loaded model can not be continued. Model can be fitted from scratch.If one needs to continue fitting, please, look at `warm_start` parameter\n",
      "10:13:22 WARNING:Fitting of loaded model can not be continued. Model can be fitted from scratch.If one needs to continue fitting, please, look at `warm_start` parameter\n",
      "2020-06-09 22:13:22.713 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 115: [loading vocabulary from /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/en_mipt_answers.dict]\n",
      "10:13:22 INFO:[loading vocabulary from /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/en_mipt_answers.dict]\n",
      "2020-06-09 22:13:22.715 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 203: Loading model sklearn.linear_model:LogisticRegression from /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/logreg.pkl\n",
      "10:13:22 INFO:Loading model sklearn.linear_model:LogisticRegression from /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/logreg.pkl\n",
      "2020-06-09 22:13:22.716 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 210: Model sklearn.linear_model.logisticLogisticRegression loaded  with parameters\n",
      "10:13:22 INFO:Model sklearn.linear_model.logisticLogisticRegression loaded  with parameters\n",
      "2020-06-09 22:13:22.716 WARNING in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 216: Fitting of loaded model can not be continued. Model can be fitted from scratch.If one needs to continue fitting, please, look at `warm_start` parameter\n",
      "10:13:22 WARNING:Fitting of loaded model can not be continued. Model can be fitted from scratch.If one needs to continue fitting, please, look at `warm_start` parameter\n"
     ]
    }
   ],
   "source": [
    "qa_models = load_qa_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def select_squad_responses(\n",
    "    contexts, squad_model, question, num_returned_values=1\n",
    "):\n",
    "    responses = contexts.context.apply(\n",
    "        lambda context: squad_model([context], [question])\n",
    "    ).values\n",
    "    return [\n",
    "        r[0][0] for r in sorted(responses, key=lambda x: -1 * x[2][0])\n",
    "        [:num_returned_values]\n",
    "    ]\n",
    "\n",
    "\n",
    "def select_faq_responses(faq_model, question):\n",
    "    return faq_model([question])[0]\n",
    "\n",
    "\n",
    "def format_responses(question, responses):\n",
    "    formatted_response = f'{question}:\\n\\n'\n",
    "    for k, res in enumerate(responses):\n",
    "        formatted_response += f'{k}: {res}\\n'\n",
    "    return formatted_response\n",
    "\n",
    "\n",
    "def get_responses(\n",
    "    contexts, question, qa_models, num_returned_values_per_squad_model=1\n",
    "):\n",
    "    responses = []\n",
    "    for squad_model in qa_models['squad'].values():\n",
    "        responses.extend(\n",
    "            select_squad_responses(\n",
    "                contexts,\n",
    "                squad_model,\n",
    "                question,\n",
    "                num_returned_values=num_returned_values_per_squad_model\n",
    "            )\n",
    "        )\n",
    "    for faq_model in qa_models['faq'].values():\n",
    "        responses.extend(select_faq_responses(faq_model, question))\n",
    "    return format_responses(question, set([r for r in responses if r.strip()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tests\n",
    "example_contexts = pd.DataFrame(\n",
    "    {\n",
    "        'context':\n",
    "            [\n",
    "                'Intekglobal has its headquarters located in TJ',\n",
    "                'In Intekglobal we care about you',\n",
    "                'Intekglobal is in the north of mexico'\n",
    "            ]\n",
    "    },\n",
    ")\n",
    "\n",
    "assert select_squad_responses(\n",
    "    contexts=example_contexts,\n",
    "    squad_model=qa_models['squad']['bert'],\n",
    "    question='Where is Intekglobal located?'\n",
    ") == ['north of mexico']\n",
    "\n",
    "assert select_faq_responses(\n",
    "    question='Is Enrique Jimenez the prettiest person in town?',\n",
    "    faq_model=qa_models['faq']['tfidf']\n",
    ") == ['Yes']\n",
    "\n",
    "\n",
    "assert get_responses(example_contexts, 'What is Intekglobal?',\n",
    "                     qa_models).strip() == '''\n",
    "What is Intekglobal?:\n",
    "\n",
    "0: north of mexico\n",
    "1: A person.\n",
    "2: its headquarters\n",
    "'''.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_input(text):\n",
    "    '''This redundancy is needed for testing'''\n",
    "    return input(text)\n",
    "\n",
    "\n",
    "def new_answer(question, data, qa_models):\n",
    "\n",
    "    if get_input('Give a better anwser [y/n]?')[0].lower() != 'y':\n",
    "        return 'no data updates..'\n",
    "\n",
    "    if get_input('Give the answer as a context [y/n]?')[0].lower() == 'y':\n",
    "        new_context = pd.DataFrame(\n",
    "            {\n",
    "                'topic': [get_input('Give context a title:\\n')],\n",
    "                'context': [get_input('Introduce the context:\\n')]\n",
    "            }\n",
    "        )\n",
    "        data['context']['df'] = data['context']['df'].append(new_context)\n",
    "        data['context']['df'].to_csv(data['context']['path'])\n",
    "\n",
    "        return 'contexts dataset updated..'\n",
    "    else:\n",
    "        new_faq = pd.DataFrame(\n",
    "            {\n",
    "                'Question': [question],\n",
    "                'Answer': [get_input('Introduce the answer:\\n')]\n",
    "            }\n",
    "        )\n",
    "        data['faq']['df'] = data['faq']['df'].append(new_faq)\n",
    "        data['faq']['df'].to_csv(data['faq']['path'])\n",
    "        qa_models['faq']['tfidf'] = train_model(\n",
    "            configs.faq.tfidf_logreg_en_faq, download=False\n",
    "        )\n",
    "        return 'FAQ dataset and model updated..'\n",
    "\n",
    "\n",
    "def dialog_system(data, qa_models, num_returned_values_per_squad_model=1):\n",
    "    question = get_input('Introduce question:\\n')\n",
    "\n",
    "    responses = get_responses(\n",
    "        data['context']['df'], question, qa_models,\n",
    "        num_returned_values_per_squad_model\n",
    "    )\n",
    "    return responses, new_answer(question, data, qa_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Test FAQ dialog system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "from unittest.mock import patch\n",
    "from shutil import copyfile\n",
    "example_contexts = pd.DataFrame(\n",
    "    {\n",
    "        'context':\n",
    "            [\n",
    "                'Intekglobal has its headquarters located in TJ',\n",
    "                'In Intekglobal we care about you',\n",
    "                'Enrique Jimenez is one of the smartest minds on the planet'\n",
    "            ]\n",
    "    },\n",
    ")\n",
    "\n",
    "data = {\n",
    "    'context': {\n",
    "        'df': example_contexts,\n",
    "        'path': ...\n",
    "    },\n",
    "    'faq': {\n",
    "        'df': ...,\n",
    "        'path': ...\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "def copy_data_files(data, tmpdirname):\n",
    "    data['context']['path'] = path.join(tmpdirname, 'tmp_context.csv')\n",
    "    data['faq']['path'] = path.join(tmpdirname, 'tmp_faq.csv')\n",
    "    data['faq']['df'] = pd.read_csv('./faq_data.csv')\n",
    "    data['context']['df'].to_csv(data['context']['path'])\n",
    "    copyfile('./faq_data.csv', data['faq']['path'])\n",
    "\n",
    "\n",
    "@patch('__main__.get_input')\n",
    "def test_context_response_with_no_updates(mock_input):\n",
    "    mock_input.side_effect = ['Who is Enrique Jimenez?', 'N']\n",
    "\n",
    "    with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "        copy_data_files(data, tmpdirname)\n",
    "        responses, status = dialog_system(data, qa_models)\n",
    "        assert 'no data updates..' == status\n",
    "        assert 'one of the smartest minds on the planet'  in responses\n",
    "\n",
    "\n",
    "@patch('__main__.train_model')\n",
    "@patch('__main__.get_input')\n",
    "def test_updating_faq_dataset(mock_input, mock_train_model):\n",
    "\n",
    "    new_answer = 'Intekglobal is one of the best companies in the world'\n",
    "    mock_input.side_effect = ['What is Intekglobal?', 'Y', 'N', new_answer]\n",
    "    qa_model_faq = qa_models['faq']['tfidf']\n",
    "    try:\n",
    "\n",
    "        with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "            copy_data_files(data, tmpdirname)\n",
    "\n",
    "            assert 'FAQ dataset and model updated..' == dialog_system(\n",
    "                data, qa_models\n",
    "            )[1]\n",
    "\n",
    "            updated_faq = pd.read_csv(data['faq']['path'])\n",
    "\n",
    "            assert updated_faq[updated_faq['Answer'] == new_answer\n",
    "                              ].shape[0] == 1\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    finally:\n",
    "        qa_models['faq']['tfidf'] = qa_model_faq\n",
    "\n",
    "\n",
    "test_context_response_with_no_updates()\n",
    "test_updating_faq_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "HOME_DIR = !echo $HOME\n",
    "HOME_DIR = HOME_DIR[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-06-09 22:36:36.731 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 203: Loading model sklearn.feature_extraction.text:TfidfVectorizer from /tmp/tmp13etuaio/temp_models_dir/faq/mipt/en_mipt_faq_v4/tfidf.pkl\n",
      "10:36:36 INFO:Loading model sklearn.feature_extraction.text:TfidfVectorizer from /tmp/tmp13etuaio/temp_models_dir/faq/mipt/en_mipt_faq_v4/tfidf.pkl\n",
      "2020-06-09 22:36:36.733 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 210: Model sklearn.feature_extraction.textTfidfVectorizer loaded  with parameters\n",
      "10:36:36 INFO:Model sklearn.feature_extraction.textTfidfVectorizer loaded  with parameters\n",
      "2020-06-09 22:36:36.734 WARNING in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 216: Fitting of loaded model can not be continued. Model can be fitted from scratch.If one needs to continue fitting, please, look at `warm_start` parameter\n",
      "10:36:36 WARNING:Fitting of loaded model can not be continued. Model can be fitted from scratch.If one needs to continue fitting, please, look at `warm_start` parameter\n",
      "2020-06-09 22:36:36.805 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 109: Fitting model sklearn.feature_extraction.textTfidfVectorizer\n",
      "10:36:36 INFO:Fitting model sklearn.feature_extraction.textTfidfVectorizer\n",
      "2020-06-09 22:36:36.810 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 241: Saving model to /tmp/tmp13etuaio/temp_models_dir/faq/mipt/en_mipt_faq_v4/tfidf.pkl\n",
      "10:36:36 INFO:Saving model to /tmp/tmp13etuaio/temp_models_dir/faq/mipt/en_mipt_faq_v4/tfidf.pkl\n",
      "2020-06-09 22:36:36.817 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 115: [loading vocabulary from /tmp/tmp13etuaio/temp_models_dir/faq/mipt/en_mipt_faq_v4/en_mipt_answers.dict]\n",
      "10:36:36 INFO:[loading vocabulary from /tmp/tmp13etuaio/temp_models_dir/faq/mipt/en_mipt_faq_v4/en_mipt_answers.dict]\n",
      "2020-06-09 22:36:36.819 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 101: [saving vocabulary to /tmp/tmp13etuaio/temp_models_dir/faq/mipt/en_mipt_faq_v4/en_mipt_answers.dict]\n",
      "10:36:36 INFO:[saving vocabulary to /tmp/tmp13etuaio/temp_models_dir/faq/mipt/en_mipt_faq_v4/en_mipt_answers.dict]\n",
      "2020-06-09 22:36:36.824 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 203: Loading model sklearn.linear_model:LogisticRegression from /tmp/tmp13etuaio/temp_models_dir/faq/mipt/en_mipt_faq_v4/logreg.pkl\n",
      "10:36:36 INFO:Loading model sklearn.linear_model:LogisticRegression from /tmp/tmp13etuaio/temp_models_dir/faq/mipt/en_mipt_faq_v4/logreg.pkl\n",
      "2020-06-09 22:36:36.827 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 210: Model sklearn.linear_model.logisticLogisticRegression loaded  with parameters\n",
      "10:36:36 INFO:Model sklearn.linear_model.logisticLogisticRegression loaded  with parameters\n",
      "2020-06-09 22:36:36.828 WARNING in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 216: Fitting of loaded model can not be continued. Model can be fitted from scratch.If one needs to continue fitting, please, look at `warm_start` parameter\n",
      "10:36:36 WARNING:Fitting of loaded model can not be continued. Model can be fitted from scratch.If one needs to continue fitting, please, look at `warm_start` parameter\n",
      "2020-06-09 22:36:36.868 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 109: Fitting model sklearn.linear_model.logisticLogisticRegression\n",
      "10:36:36 INFO:Fitting model sklearn.linear_model.logisticLogisticRegression\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "2020-06-09 22:36:36.877 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 241: Saving model to /tmp/tmp13etuaio/temp_models_dir/faq/mipt/en_mipt_faq_v4/logreg.pkl\n",
      "10:36:36 INFO:Saving model to /tmp/tmp13etuaio/temp_models_dir/faq/mipt/en_mipt_faq_v4/logreg.pkl\n",
      "2020-06-09 22:36:37.691 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 203: Loading model sklearn.feature_extraction.text:TfidfVectorizer from /tmp/tmp13etuaio/temp_models_dir/faq/mipt/en_mipt_faq_v4/tfidf.pkl\n",
      "10:36:37 INFO:Loading model sklearn.feature_extraction.text:TfidfVectorizer from /tmp/tmp13etuaio/temp_models_dir/faq/mipt/en_mipt_faq_v4/tfidf.pkl\n",
      "2020-06-09 22:36:37.692 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 210: Model sklearn.feature_extraction.textTfidfVectorizer loaded  with parameters\n",
      "10:36:37 INFO:Model sklearn.feature_extraction.textTfidfVectorizer loaded  with parameters\n",
      "2020-06-09 22:36:37.693 WARNING in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 216: Fitting of loaded model can not be continued. Model can be fitted from scratch.If one needs to continue fitting, please, look at `warm_start` parameter\n",
      "10:36:37 WARNING:Fitting of loaded model can not be continued. Model can be fitted from scratch.If one needs to continue fitting, please, look at `warm_start` parameter\n",
      "2020-06-09 22:36:37.695 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 115: [loading vocabulary from /tmp/tmp13etuaio/temp_models_dir/faq/mipt/en_mipt_faq_v4/en_mipt_answers.dict]\n",
      "10:36:37 INFO:[loading vocabulary from /tmp/tmp13etuaio/temp_models_dir/faq/mipt/en_mipt_faq_v4/en_mipt_answers.dict]\n",
      "2020-06-09 22:36:37.697 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 203: Loading model sklearn.linear_model:LogisticRegression from /tmp/tmp13etuaio/temp_models_dir/faq/mipt/en_mipt_faq_v4/logreg.pkl\n",
      "10:36:37 INFO:Loading model sklearn.linear_model:LogisticRegression from /tmp/tmp13etuaio/temp_models_dir/faq/mipt/en_mipt_faq_v4/logreg.pkl\n",
      "2020-06-09 22:36:37.699 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 210: Model sklearn.linear_model.logisticLogisticRegression loaded  with parameters\n",
      "10:36:37 INFO:Model sklearn.linear_model.logisticLogisticRegression loaded  with parameters\n",
      "2020-06-09 22:36:37.700 WARNING in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 216: Fitting of loaded model can not be continued. Model can be fitted from scratch.If one needs to continue fitting, please, look at `warm_start` parameter\n",
      "10:36:37 WARNING:Fitting of loaded model can not be continued. Model can be fitted from scratch.If one needs to continue fitting, please, look at `warm_start` parameter\n",
      "2020-06-09 22:36:38.841 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 203: Loading model sklearn.feature_extraction.text:TfidfVectorizer from /tmp/tmp13etuaio/temp_models_dir/faq/mipt/en_mipt_faq_v4/tfidf.pkl\n",
      "10:36:38 INFO:Loading model sklearn.feature_extraction.text:TfidfVectorizer from /tmp/tmp13etuaio/temp_models_dir/faq/mipt/en_mipt_faq_v4/tfidf.pkl\n",
      "2020-06-09 22:36:38.843 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 210: Model sklearn.feature_extraction.textTfidfVectorizer loaded  with parameters\n",
      "10:36:38 INFO:Model sklearn.feature_extraction.textTfidfVectorizer loaded  with parameters\n",
      "2020-06-09 22:36:38.844 WARNING in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 216: Fitting of loaded model can not be continued. Model can be fitted from scratch.If one needs to continue fitting, please, look at `warm_start` parameter\n",
      "10:36:38 WARNING:Fitting of loaded model can not be continued. Model can be fitted from scratch.If one needs to continue fitting, please, look at `warm_start` parameter\n",
      "2020-06-09 22:36:38.845 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 115: [loading vocabulary from /tmp/tmp13etuaio/temp_models_dir/faq/mipt/en_mipt_faq_v4/en_mipt_answers.dict]\n",
      "10:36:38 INFO:[loading vocabulary from /tmp/tmp13etuaio/temp_models_dir/faq/mipt/en_mipt_faq_v4/en_mipt_answers.dict]\n",
      "2020-06-09 22:36:38.847 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 203: Loading model sklearn.linear_model:LogisticRegression from /tmp/tmp13etuaio/temp_models_dir/faq/mipt/en_mipt_faq_v4/logreg.pkl\n",
      "10:36:38 INFO:Loading model sklearn.linear_model:LogisticRegression from /tmp/tmp13etuaio/temp_models_dir/faq/mipt/en_mipt_faq_v4/logreg.pkl\n",
      "2020-06-09 22:36:38.849 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 210: Model sklearn.linear_model.logisticLogisticRegression loaded  with parameters\n",
      "10:36:38 INFO:Model sklearn.linear_model.logisticLogisticRegression loaded  with parameters\n",
      "2020-06-09 22:36:38.850 WARNING in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 216: Fitting of loaded model can not be continued. Model can be fitted from scratch.If one needs to continue fitting, please, look at `warm_start` parameter\n",
      "10:36:38 WARNING:Fitting of loaded model can not be continued. Model can be fitted from scratch.If one needs to continue fitting, please, look at `warm_start` parameter\n",
      "10:36:40 INFO:Old response:\n",
      " What is Intekglobal?:\n",
      "\n",
      "0: care about you\n",
      "1: A person.\n",
      "2: its headquarters\n",
      "\n",
      "10:36:40 INFO:New response:\n",
      "What is Intekglobal?:\n",
      "\n",
      "0: care about you\n",
      "1: Intekglobal is one of the best companies in the world\n",
      "2: its headquarters\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tempfile\n",
    "from unittest.mock import patch\n",
    "from shutil import copyfile, copytree\n",
    "from pprint import pprint\n",
    "\n",
    "example_contexts = pd.DataFrame(\n",
    "    {\n",
    "        'context':\n",
    "            [\n",
    "                'Intekglobal has its headquarters located in TJ',\n",
    "                'In Intekglobal we care about you'\n",
    "            ]\n",
    "    },\n",
    ")\n",
    "\n",
    "data = {\n",
    "    'context': {\n",
    "        'df': example_contexts,\n",
    "        'path': ...\n",
    "    },\n",
    "    'faq': {\n",
    "        'df': ...,\n",
    "        'path': ...\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "def copy_data_files(data, tmpdirname):\n",
    "    data['context']['path'] = path.join(tmpdirname, 'tmp_context.csv')\n",
    "    data['faq']['path'] = path.join(tmpdirname, 'tmp_faq.csv')\n",
    "    data['faq']['df'] = pd.read_csv('./faq_data.csv')\n",
    "    copyfile('./context_data.csv', data['context']['path'])\n",
    "    copyfile('./faq_data.csv', data['faq']['path'])\n",
    "\n",
    "\n",
    "def modify_configs(data, tmpdirname):\n",
    "\n",
    "    copy_data_files(data, tmpdirname)\n",
    "\n",
    "    tmp_configs_faq = path.join(tmpdirname, 'temp_config_faq.json')\n",
    "    tmp_model_dir = path.join(tmpdirname, 'temp_models_dir')\n",
    "\n",
    "    metadata = json.load(open(configs.faq.tfidf_logreg_en_faq)\n",
    "                        )['metadata']['variables']\n",
    "\n",
    "    models_dir = metadata['MODELS_PATH'].replace(\n",
    "        '{ROOT_PATH}', metadata['ROOT_PATH'].replace('~', HOME_DIR)\n",
    "    )\n",
    "\n",
    "    copytree(models_dir, tmp_model_dir)\n",
    "    copyfile(configs.faq.tfidf_logreg_en_faq, tmp_configs_faq)\n",
    "\n",
    "    configs.faq.tfidf_logreg_en_faq = tmp_configs_faq\n",
    "\n",
    "    gen_faq_config_file(\n",
    "        metadata={'variables': {\n",
    "            'MODELS_PATH': tmp_model_dir\n",
    "        }},\n",
    "        dataset_reader={'data_path': data['faq']['path']}\n",
    "    )\n",
    "    #pprint(json.load(open(configs.faq.tfidf_logreg_en_faq)))\n",
    "    assert path.isdir(tmp_model_dir)\n",
    "    assert path.isfile(configs.faq.tfidf_logreg_en_faq)\n",
    "\n",
    "\n",
    "@patch('__main__.get_input')\n",
    "def test_faq_answer_with_updating(mock_input):\n",
    "\n",
    "    new_answer = 'Intekglobal is one of the best companies in the world'\n",
    "    question = 'What is Intekglobal?'\n",
    "    mock_input.side_effect = [question, 'Y', 'N', new_answer, question, 'N']\n",
    "\n",
    "    original_config_file = configs.faq.tfidf_logreg_en_faq\n",
    "    qa_model_faq = qa_models['faq']['tfidf']\n",
    "\n",
    "    with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "\n",
    "        try:\n",
    "            modify_configs(data, tmpdirname)\n",
    "    \n",
    "            old_responses = dialog_system(data, qa_models)[0]\n",
    "            new_responses = dialog_system(data, qa_models)[0]\n",
    "           \n",
    "            logging.info(f'Old response:\\n {old_responses}')\n",
    "            logging.info(f'New response:\\n{new_responses}')\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "        finally:\n",
    "            configs.faq.tfidf_logreg_en_faq = original_config_file\n",
    "            qa_models['faq']['tfidf'] = qa_model_faq\n",
    "\n",
    "        assert new_answer not in old_responses\n",
    "        assert new_answer in new_responses\n",
    "\n",
    "\n",
    "test_faq_answer_with_updating()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10:37:48 INFO:Old response:\n",
      " What is a Chatbot?:\n",
      "\n",
      "0: A person.\n",
      "1: Intekglobal\n",
      "2: Intekglobal we care about you\n",
      "\n",
      "10:37:51 INFO:New response:\n",
      "What is a Chatbot?:\n",
      "\n",
      "0: an important tool for simulating intelligent conversations with humans\n",
      "1: A person.\n",
      "\n",
      "10:37:51 INFO: Test finished\n"
     ]
    }
   ],
   "source": [
    "##Test Context Dialog System\n",
    "import tempfile\n",
    "from unittest.mock import patch\n",
    "from shutil import copyfile, copytree\n",
    "from pprint import pprint\n",
    "\n",
    "example_contexts = pd.DataFrame(\n",
    "    {\n",
    "        'topic': ['Headquarters', 'Mision'],\n",
    "        'context':\n",
    "            [\n",
    "                'Intekglobal has its headquarters located in TJ',\n",
    "                'In Intekglobal we care about you'\n",
    "            ]\n",
    "    }\n",
    ")\n",
    "\n",
    "data = {\n",
    "    'context': {\n",
    "        'df': example_contexts,\n",
    "        'path': ...\n",
    "    },\n",
    "    'faq': {\n",
    "        'df': ...,\n",
    "        'path': ...\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "def copy_data_files(data, tmpdirname):\n",
    "    data['context']['path'] = path.join(tmpdirname, 'tmp_context.csv')\n",
    "    data['faq']['path'] = path.join(tmpdirname, 'tmp_faq.csv')\n",
    "    data['faq']['df'] = pd.read_csv('./faq_data.csv')\n",
    "    copyfile('./context_data.csv', data['context']['path'])\n",
    "    copyfile('./faq_data.csv', data['faq']['path'])\n",
    "\n",
    "\n",
    "@patch('__main__.get_input')\n",
    "def test_context_new_answer(mock_input):\n",
    "\n",
    "    question = 'What is a Chatbot?'\n",
    "    new_topic = 'AI Tool & Chatbot Development'\n",
    "    new_context = '''\n",
    "\n",
    "A chatbot is an important tool for simulating intelligent conversations with humans.\n",
    "Intekglobal chatbots efficiently live message on platforms such as Facebook Messenger, \n",
    "Slack, and Telegram. Assisting consumers with a variety of purposes and industries. \n",
    "\n",
    "But chatbots are more than just a cool technology advancement. They actually transform the user experience.\n",
    "People want simple and convenient interactions with interface and products.\n",
    "\n",
    "'''\n",
    "\n",
    "    mock_input.side_effect = [\n",
    "        question, 'YES', 'yes', new_topic, new_context, question, 'N'\n",
    "    ]\n",
    "\n",
    "    with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "\n",
    "        try:\n",
    "\n",
    "            copy_data_files(data, tmpdirname)\n",
    "\n",
    "            old_responses = dialog_system(data, qa_models)[0]\n",
    "            logging.info(f'Old response:\\n {old_responses}')\n",
    "            new_responses = dialog_system(data, qa_models)[0]\n",
    "            logging.info(f'New response:\\n{new_responses}')\n",
    "        except Exception as e:\n",
    "            print(repr(e))\n",
    "\n",
    "        finally:\n",
    "            logging.info(' Test finished')\n",
    "\n",
    "        updated_context = pd.read_csv(data['context']['path'])\n",
    "\n",
    "        assert updated_context[updated_context['context'] == new_context\n",
    "                              ].shape[0] == 1\n",
    "\n",
    "        assert updated_context[updated_context['topic'] == new_topic\n",
    "                              ].shape[0] == 1\n",
    "        assert 'an important tool for simulating intelligent conversations with humans' not in old_responses\n",
    "        assert 'an important tool for simulating intelligent conversations with humans' in new_responses\n",
    "\n",
    "\n",
    "test_context_new_answer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "# if _main_..\n",
    "def main():\n",
    "    CONTEXT_DATA_FILE = './context_data.csv'\n",
    "    FAQ_DATA_FILE = './faq_data.csv'\n",
    "    qa_models = load_qa_models()\n",
    "\n",
    "    context = {'df': pd.read_csv(CONTEXT_DATA_FILE), 'path': CONTEXT_DATA_FILE}\n",
    "    faq = {'df': pd.read_csv(FAQ_DATA_FILE), 'path': FAQ_DATA_FILE}\n",
    "\n",
    "    data = {'context': context, 'faq': faq}\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            dialog_system(data=data, qa_models=qa_models)\n",
    "        except (KeyboardInterrupt, EOFError, SystemExit):\n",
    "            logging.info('Goodbye!')\n",
    "            return 'Goodbye!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:50:06 INFO:Goodbye!\n",
      "11:50:06 INFO:Goodbye!\n",
      "11:50:06 INFO:Goodbye!\n"
     ]
    }
   ],
   "source": [
    "# test main()\n",
    "\n",
    "from unittest.mock import patch\n",
    "\n",
    "\n",
    "@patch('__main__.load_qa_models')\n",
    "@patch('__main__.pd.read_csv')\n",
    "@patch('__main__.dialog_system')\n",
    "def test_main_keyboard_interrupt(\n",
    "    mock_dialog_system,\n",
    "    mock_pd_read_csv,\n",
    "    mock_load_qa_models,\n",
    "):\n",
    "    mock_dialog_system.side_effect = [\n",
    "        KeyboardInterrupt(), EOFError(),\n",
    "        SystemExit()\n",
    "    ]\n",
    "    assert 'Goodbye!' == main()\n",
    "    assert 'Goodbye!' == main()\n",
    "    assert 'Goodbye!' == main()\n",
    "\n",
    "\n",
    "test_main_keyboard_interrupt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
