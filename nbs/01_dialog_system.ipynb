{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "!pip install -r ../requirements.txt > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp dialog_system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04:37:56 ERROR: Error Log Active \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# export\n",
    "from deeppavlov import configs,build_model,train_model\n",
    "import json\n",
    "from os import path,popen,mkdir\n",
    "from shutil import copyfile\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import logging\n",
    "from collections import defaultdict \n",
    "\n",
    "logging.basicConfig(format='%(asctime)s %(levelname)s:%(message)s', level=logging.ERROR, datefmt='%I:%M:%S')\n",
    "logging.info(\"Hello! Welcome to our automated dialog system!\")\n",
    "logging.debug(\" Debug Mode Activa\")\n",
    "logging.error(' Error Log Active ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def run_shell_installs():\n",
    "    ''' Run install commands\n",
    "    '''\n",
    "    command_strings = (\n",
    "        ' pip install deeppavlov',\n",
    "        ' python -m deeppavlov install squad',\n",
    "        ' python -m deeppavlov install squad_bert',\n",
    "        ' python -m deeppavlov install fasttext_avg_autofaq',\n",
    "        ' python -m deeppavlov install fasttext_tfidf_autofaq',\n",
    "        ' python -m deeppavlov install tfidf_autofaq',\n",
    "        ' python -m deeppavlov install tfidf_logreg_autofaq ',\n",
    "        ' python -m deeppavlov install tfidf_logreg_en_faq'\n",
    "    )\n",
    "    for command in command_strings:\n",
    "        logging.debug(command)\n",
    "        logging.debug(popen(command).read())\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dialog System\n",
    "> Question Answering Automated Dialog System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_shell_installs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def action_over_list_f(arr, v):\n",
    "\n",
    "    k_id, v_id = next(iter(v[0].items()))\n",
    "\n",
    "    for p, a in enumerate(arr):\n",
    "        if k_id in a.keys() and a[k_id] == v_id:\n",
    "            for k_rep, v_rep in v[1].items():\n",
    "                arr[p][k_rep] = v_rep\n",
    "\n",
    "\n",
    "def replacement_f(model_config, **args):\n",
    "    '''Replaces the model config dictionary with new values\n",
    "    '''\n",
    "    for k, v in args.items():\n",
    "        if isinstance(v, dict):\n",
    "            replacement_f(model_config[k], **v)\n",
    "        if isinstance(v, str):\n",
    "            model_config[k] = v\n",
    "        if isinstance(model_config[k], list):\n",
    "            action_over_list_f(model_config[k], v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test action_over_list_f\n",
    "from random import randint\n",
    "\n",
    "\n",
    "def gen_list_keys_for_tests():\n",
    "\n",
    "    str_n = lambda x: f'{x}_{randint(1,10):1}'\n",
    "    gen_dict_list = lambda: {\n",
    "        'id': str_n('id'),\n",
    "        'key1': str_n('v1'),\n",
    "        'key2': str_n('v2'),\n",
    "        'key3': str_n('v3')\n",
    "    }\n",
    "\n",
    "    pipe_list = [gen_dict_list() for _ in range(randint(3, 10))]\n",
    "\n",
    "    rand_id = pipe_list[randint(0, len(pipe_list) - 1)]['id']\n",
    "    rand_key = f'key{randint(1, 3)}' \n",
    "\n",
    "    new_rand_val = str_n('new')\n",
    "    args = {\n",
    "        'chains': {\n",
    "            'pipe': [{\n",
    "                'id': rand_id\n",
    "            }, {\n",
    "               rand_key : new_rand_val\n",
    "            }]\n",
    "        }\n",
    "    }\n",
    "\n",
    "    return pipe_list, rand_id, rand_key, args, new_rand_val\n",
    "\n",
    "\n",
    "def test_action_over_list_f():\n",
    "\n",
    "    pipe_list, rand_id, rand_key, args, new_rand_val = gen_list_keys_for_tests()\n",
    "\n",
    "    assert all(\n",
    "        new_rand_val not in pipe_elem.values() for pipe_elem in pipe_list\n",
    "    )\n",
    "\n",
    "    action_over_list_f(pipe_list, args['chains']['pipe'])\n",
    "\n",
    "    assert any(\n",
    "        rand_key in pipe_elem.keys() and\n",
    "        new_rand_val in pipe_elem.values() for pipe_elem in pipe_list\n",
    "    )\n",
    "\n",
    "\n",
    "def test_replacement_f_list():\n",
    "\n",
    "    pipe_list, rand_id, rand_key, args, new_rand_val = gen_list_keys_for_tests()\n",
    "\n",
    "    mod_conf = {'chains': {'pipe': pipe_list}}\n",
    "\n",
    "    assert all(\n",
    "        new_rand_val not in pipe_elem.values()\n",
    "        for pipe_elem in mod_conf['chains']['pipe']\n",
    "    )\n",
    "\n",
    "    replacement_f(model_config=mod_conf, **args)\n",
    "    assert any(\n",
    "        rand_key in pipe_elem.keys() and\n",
    "        new_rand_val in pipe_elem.values()\n",
    "        for pipe_elem in mod_conf['chains']['pipe']\n",
    "    )\n",
    "\n",
    "\n",
    "def test_replacement_f_val():\n",
    "    args = {'key3': 'newvalue'}\n",
    "    mod_conf = {'key1': 'val1', 'key2': 'val2', 'key3': 'val3'}\n",
    "    replacement_f(model_config=mod_conf, **args)\n",
    "    assert all(\n",
    "        arg_k in mod_conf.keys() and arg_v in mod_conf.values()\n",
    "        for arg_k, arg_v in args.items()\n",
    "    )\n",
    "\n",
    "\n",
    "def test_replacement_f_dict():\n",
    "    args = {'1_key_3': {'2_key_2': 'newvalue'}}\n",
    "    mod_conf = {'1_key_3': {'2_key_2': 'oldvalue'}, '0_key_': '0_val'}\n",
    "    replacement_f(model_config=mod_conf, **args)\n",
    "    assert mod_conf['1_key_3']['2_key_2'] == 'newvalue'\n",
    "\n",
    "\n",
    "test_action_over_list_f()\n",
    "test_replacement_f_list()\n",
    "test_replacement_f_val()\n",
    "test_replacement_f_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def updates_faq_config_file(\n",
    "    configs_path,\n",
    "    **args\n",
    "):\n",
    "    '''Updates deepplavov json config file \n",
    "    '''\n",
    "    #set FAQ data in config file\n",
    "    model_config = json.load(open(configs_path))\n",
    "\n",
    "    if 'data_url' in model_config['dataset_reader']:\n",
    "        del model_config['dataset_reader']['data_url']\n",
    "\n",
    "    replacement_f(model_config=model_config,**args)\n",
    "\n",
    "    json.dump(model_config, open(configs_path, 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test updates_faq_config_file\n",
    "import tempfile\n",
    "from shutil import copyfile\n",
    "\n",
    "\n",
    "def gen_list_keys_for_tests():\n",
    "\n",
    "    str_n = lambda x: f'{x}_{randint(1,10):1}'\n",
    "    gen_dict_list = lambda: {\n",
    "        'id': str_n('id'),\n",
    "        'key1': str_n('v1'),\n",
    "        'key2': str_n('v2'),\n",
    "        'key3': str_n('v3')\n",
    "    }\n",
    "\n",
    "    pipe_list = [gen_dict_list() for _ in range(randint(3, 10))]\n",
    "\n",
    "    rand_id = pipe_list[randint(0, len(pipe_list) - 1)]['id']\n",
    "    rand_key =  f'key{randint(1, 3)}' \n",
    "\n",
    "    new_rand_val = str_n('new')\n",
    "    pipe_dict = {'pipe': [{'id': rand_id}, {rand_key: new_rand_val}]}\n",
    "    args = {'chainer': pipe_dict}\n",
    "\n",
    "    return pipe_list, rand_id, rand_key, args, new_rand_val\n",
    "\n",
    "\n",
    "def test_updates_faq_config_file_update_string():\n",
    "\n",
    "    with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "\n",
    "        tmp_config_file = path.join(tmpdirname, 'tmp_file.json')\n",
    "\n",
    "        copyfile(configs.faq.tfidf_logreg_en_faq, tmp_config_file)\n",
    "\n",
    "        assert path.isfile(tmp_config_file)\n",
    "\n",
    "        updates_faq_config_file(\n",
    "            configs_path=tmp_config_file,\n",
    "            dataset_reader={'data_path': 'fictional_csv_file.csv'}\n",
    "        )\n",
    "\n",
    "        config_json = json.load(open(tmp_config_file))\n",
    "        assert 'data_path' in config_json['dataset_reader']\n",
    "\n",
    "\n",
    "def test_updates_faq_config_file_update_list():\n",
    "\n",
    "    with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "\n",
    "        tmp_config_file = path.join(tmpdirname, 'tmp_file.json')\n",
    "\n",
    "        pipe_list, rand_id, rand_key, args, new_rand_val = gen_list_keys_for_tests(\n",
    "        )\n",
    "        mod_conf = {\n",
    "            'chainer': {\n",
    "                'pipe': pipe_list\n",
    "            },\n",
    "            'dataset_reader': 'dataset_reader_dictionary'\n",
    "        }\n",
    "\n",
    "        json.dump(mod_conf, open(tmp_config_file, 'w'))\n",
    "\n",
    "        assert path.isfile(tmp_config_file)\n",
    "\n",
    "        updates_faq_config_file(configs_path=tmp_config_file, **args)\n",
    "\n",
    "        config_json = json.load(open(tmp_config_file))\n",
    "   \n",
    "        assert any(\n",
    "            rand_key in pipe_elem.keys() and new_rand_val in pipe_elem.values()\n",
    "            for pipe_elem in config_json['chainer']['pipe']\n",
    "        )\n",
    "\n",
    "\n",
    "test_updates_faq_config_file_update_string()\n",
    "test_updates_faq_config_file_update_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def select_faq_responses(faq_model, question):\n",
    "    '''Calls Deeppavlov FAQ model\n",
    "    '''\n",
    "    return faq_model([question])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-06-18 16:40:05.480 INFO in 'deeppavlov.core.data.utils'['utils'] at line 80: Downloading from http://files.deeppavlov.ai/faq/mipt/en_mipt_faq_v4.tar.gz to /home/jovyan/.deeppavlov/models/faq/en_mipt_faq_v4.tar.gz\n",
      "04:40:05 INFO:Downloading from http://files.deeppavlov.ai/faq/mipt/en_mipt_faq_v4.tar.gz to /home/jovyan/.deeppavlov/models/faq/en_mipt_faq_v4.tar.gz\n",
      "100%|██████████| 12.3k/12.3k [00:00<00:00, 10.8MB/s]\n",
      "2020-06-18 16:40:05.496 INFO in 'deeppavlov.core.data.utils'['utils'] at line 242: Extracting /home/jovyan/.deeppavlov/models/faq/en_mipt_faq_v4.tar.gz archive into /home/jovyan/.deeppavlov/models/faq/mipt\n",
      "04:40:05 INFO:Extracting /home/jovyan/.deeppavlov/models/faq/en_mipt_faq_v4.tar.gz archive into /home/jovyan/.deeppavlov/models/faq/mipt\n",
      "[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package perluniprops to\n",
      "[nltk_data]     /home/jovyan/nltk_data...\n",
      "[nltk_data]   Unzipping misc/perluniprops.zip.\n",
      "[nltk_data] Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]     /home/jovyan/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/nonbreaking_prefixes.zip.\n",
      "2020-06-18 16:40:11.769 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 203: Loading model sklearn.feature_extraction.text:TfidfVectorizer from /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/tfidf.pkl\n",
      "04:40:11 INFO:Loading model sklearn.feature_extraction.text:TfidfVectorizer from /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/tfidf.pkl\n",
      "2020-06-18 16:40:11.774 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 210: Model sklearn.feature_extraction.textTfidfVectorizer loaded  with parameters\n",
      "04:40:11 INFO:Model sklearn.feature_extraction.textTfidfVectorizer loaded  with parameters\n",
      "2020-06-18 16:40:11.776 WARNING in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 216: Fitting of loaded model can not be continued. Model can be fitted from scratch.If one needs to continue fitting, please, look at `warm_start` parameter\n",
      "04:40:11 WARNING:Fitting of loaded model can not be continued. Model can be fitted from scratch.If one needs to continue fitting, please, look at `warm_start` parameter\n",
      "2020-06-18 16:40:11.791 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 109: Fitting model sklearn.feature_extraction.textTfidfVectorizer\n",
      "04:40:11 INFO:Fitting model sklearn.feature_extraction.textTfidfVectorizer\n",
      "2020-06-18 16:40:11.798 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 241: Saving model to /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/tfidf.pkl\n",
      "04:40:11 INFO:Saving model to /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/tfidf.pkl\n",
      "2020-06-18 16:40:11.803 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 115: [loading vocabulary from /tmp/tmp3x4encn3/temp_vocab_answers.dict]\n",
      "04:40:11 INFO:[loading vocabulary from /tmp/tmp3x4encn3/temp_vocab_answers.dict]\n",
      "2020-06-18 16:40:11.806 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 101: [saving vocabulary to /tmp/tmp3x4encn3/temp_vocab_answers.dict]\n",
      "04:40:11 INFO:[saving vocabulary to /tmp/tmp3x4encn3/temp_vocab_answers.dict]\n",
      "2020-06-18 16:40:11.810 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 203: Loading model sklearn.linear_model:LogisticRegression from /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/logreg.pkl\n",
      "04:40:11 INFO:Loading model sklearn.linear_model:LogisticRegression from /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/logreg.pkl\n",
      "2020-06-18 16:40:11.813 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 210: Model sklearn.linear_model.logisticLogisticRegression loaded  with parameters\n",
      "04:40:11 INFO:Model sklearn.linear_model.logisticLogisticRegression loaded  with parameters\n",
      "2020-06-18 16:40:11.816 WARNING in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 216: Fitting of loaded model can not be continued. Model can be fitted from scratch.If one needs to continue fitting, please, look at `warm_start` parameter\n",
      "04:40:11 WARNING:Fitting of loaded model can not be continued. Model can be fitted from scratch.If one needs to continue fitting, please, look at `warm_start` parameter\n",
      "2020-06-18 16:40:11.828 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 109: Fitting model sklearn.linear_model.logisticLogisticRegression\n",
      "04:40:11 INFO:Fitting model sklearn.linear_model.logisticLogisticRegression\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "2020-06-18 16:40:12.658 INFO in 'deeppavlov.core.data.utils'['utils'] at line 80: Downloading from http://files.deeppavlov.ai/faq/mipt/en_mipt_faq_v4.tar.gz to /home/jovyan/.deeppavlov/models/faq/en_mipt_faq_v4.tar.gz\n",
      "04:40:12 INFO:Downloading from http://files.deeppavlov.ai/faq/mipt/en_mipt_faq_v4.tar.gz to /home/jovyan/.deeppavlov/models/faq/en_mipt_faq_v4.tar.gz\n",
      "100%|██████████| 12.3k/12.3k [00:00<00:00, 5.84MB/s]\n",
      "2020-06-18 16:40:12.672 INFO in 'deeppavlov.core.data.utils'['utils'] at line 242: Extracting /home/jovyan/.deeppavlov/models/faq/en_mipt_faq_v4.tar.gz archive into /home/jovyan/.deeppavlov/models/faq/mipt\n",
      "04:40:12 INFO:Extracting /home/jovyan/.deeppavlov/models/faq/en_mipt_faq_v4.tar.gz archive into /home/jovyan/.deeppavlov/models/faq/mipt\n",
      "2020-06-18 16:40:14.11 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 203: Loading model sklearn.feature_extraction.text:TfidfVectorizer from /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/tfidf.pkl\n",
      "04:40:14 INFO:Loading model sklearn.feature_extraction.text:TfidfVectorizer from /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/tfidf.pkl\n",
      "2020-06-18 16:40:14.14 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 210: Model sklearn.feature_extraction.textTfidfVectorizer loaded  with parameters\n",
      "04:40:14 INFO:Model sklearn.feature_extraction.textTfidfVectorizer loaded  with parameters\n",
      "2020-06-18 16:40:14.16 WARNING in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 216: Fitting of loaded model can not be continued. Model can be fitted from scratch.If one needs to continue fitting, please, look at `warm_start` parameter\n",
      "04:40:14 WARNING:Fitting of loaded model can not be continued. Model can be fitted from scratch.If one needs to continue fitting, please, look at `warm_start` parameter\n",
      "2020-06-18 16:40:14.25 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 109: Fitting model sklearn.feature_extraction.textTfidfVectorizer\n",
      "04:40:14 INFO:Fitting model sklearn.feature_extraction.textTfidfVectorizer\n",
      "2020-06-18 16:40:14.30 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 241: Saving model to /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/tfidf.pkl\n",
      "04:40:14 INFO:Saving model to /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/tfidf.pkl\n",
      "2020-06-18 16:40:14.33 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 115: [loading vocabulary from /tmp/tmp__jb3hf0/temp_vocab_answers.dict]\n",
      "04:40:14 INFO:[loading vocabulary from /tmp/tmp__jb3hf0/temp_vocab_answers.dict]\n",
      "2020-06-18 16:40:14.35 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 101: [saving vocabulary to /tmp/tmp__jb3hf0/temp_vocab_answers.dict]\n",
      "04:40:14 INFO:[saving vocabulary to /tmp/tmp__jb3hf0/temp_vocab_answers.dict]\n",
      "2020-06-18 16:40:14.38 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 203: Loading model sklearn.linear_model:LogisticRegression from /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/logreg.pkl\n",
      "04:40:14 INFO:Loading model sklearn.linear_model:LogisticRegression from /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/logreg.pkl\n",
      "2020-06-18 16:40:14.41 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 210: Model sklearn.linear_model.logisticLogisticRegression loaded  with parameters\n",
      "04:40:14 INFO:Model sklearn.linear_model.logisticLogisticRegression loaded  with parameters\n",
      "2020-06-18 16:40:14.42 WARNING in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 216: Fitting of loaded model can not be continued. Model can be fitted from scratch.If one needs to continue fitting, please, look at `warm_start` parameter\n",
      "04:40:14 WARNING:Fitting of loaded model can not be continued. Model can be fitted from scratch.If one needs to continue fitting, please, look at `warm_start` parameter\n",
      "2020-06-18 16:40:14.52 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 109: Fitting model sklearn.linear_model.logisticLogisticRegression\n",
      "04:40:14 INFO:Fitting model sklearn.linear_model.logisticLogisticRegression\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "2020-06-18 16:40:14.55 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 241: Saving model to /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/logreg.pkl\n",
      "04:40:14 INFO:Saving model to /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/logreg.pkl\n",
      "2020-06-18 16:40:15.402 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 203: Loading model sklearn.feature_extraction.text:TfidfVectorizer from /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/tfidf.pkl\n",
      "04:40:15 INFO:Loading model sklearn.feature_extraction.text:TfidfVectorizer from /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/tfidf.pkl\n",
      "2020-06-18 16:40:15.403 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 210: Model sklearn.feature_extraction.textTfidfVectorizer loaded  with parameters\n",
      "04:40:15 INFO:Model sklearn.feature_extraction.textTfidfVectorizer loaded  with parameters\n",
      "2020-06-18 16:40:15.404 WARNING in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 216: Fitting of loaded model can not be continued. Model can be fitted from scratch.If one needs to continue fitting, please, look at `warm_start` parameter\n",
      "04:40:15 WARNING:Fitting of loaded model can not be continued. Model can be fitted from scratch.If one needs to continue fitting, please, look at `warm_start` parameter\n",
      "2020-06-18 16:40:15.406 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 115: [loading vocabulary from /tmp/tmp__jb3hf0/temp_vocab_answers.dict]\n",
      "04:40:15 INFO:[loading vocabulary from /tmp/tmp__jb3hf0/temp_vocab_answers.dict]\n",
      "2020-06-18 16:40:15.407 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 203: Loading model sklearn.linear_model:LogisticRegression from /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/logreg.pkl\n",
      "04:40:15 INFO:Loading model sklearn.linear_model:LogisticRegression from /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/logreg.pkl\n",
      "2020-06-18 16:40:15.409 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 210: Model sklearn.linear_model.logisticLogisticRegression loaded  with parameters\n",
      "04:40:15 INFO:Model sklearn.linear_model.logisticLogisticRegression loaded  with parameters\n",
      "2020-06-18 16:40:15.410 WARNING in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 216: Fitting of loaded model can not be continued. Model can be fitted from scratch.If one needs to continue fitting, please, look at `warm_start` parameter\n",
      "04:40:15 WARNING:Fitting of loaded model can not be continued. Model can be fitted from scratch.If one needs to continue fitting, please, look at `warm_start` parameter\n",
      "2020-06-18 16:40:16.62 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 203: Loading model sklearn.feature_extraction.text:TfidfVectorizer from /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/tfidf.pkl\n",
      "04:40:16 INFO:Loading model sklearn.feature_extraction.text:TfidfVectorizer from /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/tfidf.pkl\n",
      "2020-06-18 16:40:16.64 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 210: Model sklearn.feature_extraction.textTfidfVectorizer loaded  with parameters\n",
      "04:40:16 INFO:Model sklearn.feature_extraction.textTfidfVectorizer loaded  with parameters\n",
      "2020-06-18 16:40:16.65 WARNING in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 216: Fitting of loaded model can not be continued. Model can be fitted from scratch.If one needs to continue fitting, please, look at `warm_start` parameter\n",
      "04:40:16 WARNING:Fitting of loaded model can not be continued. Model can be fitted from scratch.If one needs to continue fitting, please, look at `warm_start` parameter\n",
      "2020-06-18 16:40:16.67 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 115: [loading vocabulary from /tmp/tmp__jb3hf0/temp_vocab_answers.dict]\n",
      "04:40:16 INFO:[loading vocabulary from /tmp/tmp__jb3hf0/temp_vocab_answers.dict]\n",
      "2020-06-18 16:40:16.69 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 203: Loading model sklearn.linear_model:LogisticRegression from /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/logreg.pkl\n",
      "04:40:16 INFO:Loading model sklearn.linear_model:LogisticRegression from /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/logreg.pkl\n",
      "2020-06-18 16:40:16.71 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 210: Model sklearn.linear_model.logisticLogisticRegression loaded  with parameters\n",
      "04:40:16 INFO:Model sklearn.linear_model.logisticLogisticRegression loaded  with parameters\n",
      "2020-06-18 16:40:16.72 WARNING in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 216: Fitting of loaded model can not be continued. Model can be fitted from scratch.If one needs to continue fitting, please, look at `warm_start` parameter\n",
      "04:40:16 WARNING:Fitting of loaded model can not be continued. Model can be fitted from scratch.If one needs to continue fitting, please, look at `warm_start` parameter\n",
      "2020-06-18 16:40:16.884 INFO in 'deeppavlov.core.data.utils'['utils'] at line 80: Downloading from http://files.deeppavlov.ai/faq/mipt/en_mipt_faq_v4.tar.gz to /home/jovyan/.deeppavlov/models/faq/en_mipt_faq_v4.tar.gz\n",
      "04:40:16 INFO:Downloading from http://files.deeppavlov.ai/faq/mipt/en_mipt_faq_v4.tar.gz to /home/jovyan/.deeppavlov/models/faq/en_mipt_faq_v4.tar.gz\n",
      "100%|██████████| 12.3k/12.3k [00:00<00:00, 12.5MB/s]\n",
      "2020-06-18 16:40:16.894 INFO in 'deeppavlov.core.data.utils'['utils'] at line 242: Extracting /home/jovyan/.deeppavlov/models/faq/en_mipt_faq_v4.tar.gz archive into /home/jovyan/.deeppavlov/models/faq/mipt\n",
      "04:40:16 INFO:Extracting /home/jovyan/.deeppavlov/models/faq/en_mipt_faq_v4.tar.gz archive into /home/jovyan/.deeppavlov/models/faq/mipt\n",
      "2020-06-18 16:40:17.619 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 203: Loading model sklearn.feature_extraction.text:TfidfVectorizer from /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/tfidf.pkl\n",
      "04:40:17 INFO:Loading model sklearn.feature_extraction.text:TfidfVectorizer from /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/tfidf.pkl\n",
      "2020-06-18 16:40:17.621 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 210: Model sklearn.feature_extraction.textTfidfVectorizer loaded  with parameters\n",
      "04:40:17 INFO:Model sklearn.feature_extraction.textTfidfVectorizer loaded  with parameters\n",
      "2020-06-18 16:40:17.621 WARNING in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 216: Fitting of loaded model can not be continued. Model can be fitted from scratch.If one needs to continue fitting, please, look at `warm_start` parameter\n",
      "04:40:17 WARNING:Fitting of loaded model can not be continued. Model can be fitted from scratch.If one needs to continue fitting, please, look at `warm_start` parameter\n",
      "2020-06-18 16:40:17.629 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 109: Fitting model sklearn.feature_extraction.textTfidfVectorizer\n",
      "04:40:17 INFO:Fitting model sklearn.feature_extraction.textTfidfVectorizer\n",
      "2020-06-18 16:40:17.631 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 241: Saving model to /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/tfidf.pkl\n",
      "04:40:17 INFO:Saving model to /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/tfidf.pkl\n",
      "2020-06-18 16:40:17.634 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 115: [loading vocabulary from /tmp/tmptsmd_qvt/temp_vocab_answers.dict]\n",
      "04:40:17 INFO:[loading vocabulary from /tmp/tmptsmd_qvt/temp_vocab_answers.dict]\n",
      "2020-06-18 16:40:17.636 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 101: [saving vocabulary to /tmp/tmptsmd_qvt/temp_vocab_answers.dict]\n",
      "04:40:17 INFO:[saving vocabulary to /tmp/tmptsmd_qvt/temp_vocab_answers.dict]\n",
      "2020-06-18 16:40:17.638 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 203: Loading model sklearn.linear_model:LogisticRegression from /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/logreg.pkl\n",
      "04:40:17 INFO:Loading model sklearn.linear_model:LogisticRegression from /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/logreg.pkl\n",
      "2020-06-18 16:40:17.639 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 210: Model sklearn.linear_model.logisticLogisticRegression loaded  with parameters\n",
      "04:40:17 INFO:Model sklearn.linear_model.logisticLogisticRegression loaded  with parameters\n",
      "2020-06-18 16:40:17.640 WARNING in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 216: Fitting of loaded model can not be continued. Model can be fitted from scratch.If one needs to continue fitting, please, look at `warm_start` parameter\n",
      "04:40:17 WARNING:Fitting of loaded model can not be continued. Model can be fitted from scratch.If one needs to continue fitting, please, look at `warm_start` parameter\n",
      "2020-06-18 16:40:17.646 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 109: Fitting model sklearn.linear_model.logisticLogisticRegression\n",
      "04:40:17 INFO:Fitting model sklearn.linear_model.logisticLogisticRegression\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "2020-06-18 16:40:17.648 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 241: Saving model to /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/logreg.pkl\n",
      "04:40:17 INFO:Saving model to /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/logreg.pkl\n",
      "2020-06-18 16:40:18.295 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 203: Loading model sklearn.feature_extraction.text:TfidfVectorizer from /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/tfidf.pkl\n",
      "04:40:18 INFO:Loading model sklearn.feature_extraction.text:TfidfVectorizer from /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/tfidf.pkl\n",
      "2020-06-18 16:40:18.296 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 210: Model sklearn.feature_extraction.textTfidfVectorizer loaded  with parameters\n",
      "04:40:18 INFO:Model sklearn.feature_extraction.textTfidfVectorizer loaded  with parameters\n",
      "2020-06-18 16:40:18.297 WARNING in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 216: Fitting of loaded model can not be continued. Model can be fitted from scratch.If one needs to continue fitting, please, look at `warm_start` parameter\n",
      "04:40:18 WARNING:Fitting of loaded model can not be continued. Model can be fitted from scratch.If one needs to continue fitting, please, look at `warm_start` parameter\n",
      "2020-06-18 16:40:18.299 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 115: [loading vocabulary from /tmp/tmptsmd_qvt/temp_vocab_answers.dict]\n",
      "04:40:18 INFO:[loading vocabulary from /tmp/tmptsmd_qvt/temp_vocab_answers.dict]\n",
      "2020-06-18 16:40:18.301 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 203: Loading model sklearn.linear_model:LogisticRegression from /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/logreg.pkl\n",
      "04:40:18 INFO:Loading model sklearn.linear_model:LogisticRegression from /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/logreg.pkl\n",
      "2020-06-18 16:40:18.303 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 210: Model sklearn.linear_model.logisticLogisticRegression loaded  with parameters\n",
      "04:40:18 INFO:Model sklearn.linear_model.logisticLogisticRegression loaded  with parameters\n",
      "2020-06-18 16:40:18.305 WARNING in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 216: Fitting of loaded model can not be continued. Model can be fitted from scratch.If one needs to continue fitting, please, look at `warm_start` parameter\n",
      "04:40:18 WARNING:Fitting of loaded model can not be continued. Model can be fitted from scratch.If one needs to continue fitting, please, look at `warm_start` parameter\n",
      "2020-06-18 16:40:19.518 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 203: Loading model sklearn.feature_extraction.text:TfidfVectorizer from /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/tfidf.pkl\n",
      "04:40:19 INFO:Loading model sklearn.feature_extraction.text:TfidfVectorizer from /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/tfidf.pkl\n",
      "2020-06-18 16:40:19.520 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 210: Model sklearn.feature_extraction.textTfidfVectorizer loaded  with parameters\n",
      "04:40:19 INFO:Model sklearn.feature_extraction.textTfidfVectorizer loaded  with parameters\n",
      "2020-06-18 16:40:19.522 WARNING in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 216: Fitting of loaded model can not be continued. Model can be fitted from scratch.If one needs to continue fitting, please, look at `warm_start` parameter\n",
      "04:40:19 WARNING:Fitting of loaded model can not be continued. Model can be fitted from scratch.If one needs to continue fitting, please, look at `warm_start` parameter\n",
      "2020-06-18 16:40:19.526 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 115: [loading vocabulary from /tmp/tmptsmd_qvt/temp_vocab_answers.dict]\n",
      "04:40:19 INFO:[loading vocabulary from /tmp/tmptsmd_qvt/temp_vocab_answers.dict]\n",
      "2020-06-18 16:40:19.530 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 203: Loading model sklearn.linear_model:LogisticRegression from /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/logreg.pkl\n",
      "04:40:19 INFO:Loading model sklearn.linear_model:LogisticRegression from /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/logreg.pkl\n",
      "2020-06-18 16:40:19.533 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 210: Model sklearn.linear_model.logisticLogisticRegression loaded  with parameters\n",
      "04:40:19 INFO:Model sklearn.linear_model.logisticLogisticRegression loaded  with parameters\n",
      "2020-06-18 16:40:19.534 WARNING in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 216: Fitting of loaded model can not be continued. Model can be fitted from scratch.If one needs to continue fitting, please, look at `warm_start` parameter\n",
      "04:40:19 WARNING:Fitting of loaded model can not be continued. Model can be fitted from scratch.If one needs to continue fitting, please, look at `warm_start` parameter\n"
     ]
    }
   ],
   "source": [
    "#test faq responses\n",
    "import tempfile\n",
    "from shutil import copyfile\n",
    "\n",
    "\n",
    "def gen_mock_csv_file(tmpdirname, faqs):\n",
    "\n",
    "    temp_faq_csv = path.join(tmpdirname, 'tmp_faq.csv')\n",
    "\n",
    "    pd.DataFrame(faqs).to_csv(temp_faq_csv, index=False)\n",
    "\n",
    "    return temp_faq_csv\n",
    "\n",
    "\n",
    "def gen_mock_vocab_answers(tmpdirname, vocabs):\n",
    "\n",
    "    temp_dict_file = path.join(tmpdirname, 'temp_vocab_answers.dict')\n",
    "    vocabs_text = '\\n'.join(\n",
    "        t + '\\t' + str(f) for t, f in zip(vocabs['text'], vocabs['freq'])\n",
    "    )\n",
    "\n",
    "    f = open(temp_dict_file, 'w')\n",
    "    f.write(vocabs_text)\n",
    "    f.close()\n",
    "\n",
    "    return temp_dict_file\n",
    "\n",
    "\n",
    "def gen_faq_config(tmpdirname, vocab_file, faq_file):\n",
    "\n",
    "    temp_configs_faq = path.join(tmpdirname, 'temp_config_faq.json')\n",
    "    copyfile(configs.faq.tfidf_logreg_en_faq, temp_configs_faq)\n",
    "\n",
    "    changes_dict = {'save_path': vocab_file, 'load_path': vocab_file}\n",
    "    id_dict = {'id': 'answers_vocab'}\n",
    "\n",
    "    updates_faq_config_file(\n",
    "        configs_path=temp_configs_faq,\n",
    "        chainer={'pipe': [id_dict, changes_dict]},\n",
    "        dataset_reader={'data_path': faq_file}\n",
    "    )\n",
    "\n",
    "    return temp_configs_faq\n",
    "\n",
    "\n",
    "def test_faq_response_with_minimum_faqs_in_dataframe_fail_case():\n",
    "    with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "\n",
    "        faqs = {\n",
    "            'Question': ['Is Covid erradicated?'],\n",
    "            'Answer': ['Definitely not!']\n",
    "        }\n",
    "\n",
    "        vocabs = {'text': ['This is a vocab example'], 'freq': [1]}\n",
    "\n",
    "        faq_file = gen_mock_csv_file(tmpdirname, faqs)\n",
    "        vocab_file = gen_mock_vocab_answers(tmpdirname, vocabs)\n",
    "\n",
    "        configs_file = gen_faq_config(tmpdirname, vocab_file, faq_file)\n",
    "\n",
    "        try:\n",
    "            select_faq_responses(\n",
    "                question='Is Enrique the prettiest person in town?',\n",
    "                faq_model=train_model(configs_file, download=True)\n",
    "            )\n",
    "            assert False\n",
    "        except ValueError as e:\n",
    "            assert True\n",
    "\n",
    "\n",
    "def test_faq_response_with_minimum_faqs_in_dataframe_success_case():\n",
    "    with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "\n",
    "        faqs = {\n",
    "            'Question': ['Is Covid erradicated?', 'Who is the current POTUS?'],\n",
    "            'Answer': ['Definitely not!', 'Donald Trump']\n",
    "        }\n",
    "\n",
    "        vocabs = {'text': ['This is a vocab example'], 'freq': [1]}\n",
    "\n",
    "        faq_file = gen_mock_csv_file(tmpdirname, faqs)\n",
    "        vocab_file = gen_mock_vocab_answers(tmpdirname, vocabs)\n",
    "\n",
    "        configs_file = gen_faq_config(tmpdirname, vocab_file, faq_file)\n",
    "\n",
    "        assert select_faq_responses(\n",
    "            question='Is Enrique the prettiest person in town?',\n",
    "            faq_model=train_model(configs_file, download=True)\n",
    "        ) == ['Donald Trump']\n",
    "\n",
    "        \n",
    "        \n",
    "def test_faq_response_with_minimum_answers_vocab_success_case():\n",
    "    with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "\n",
    "        faqs = {\n",
    "            'Question': ['Is Covid erradicated?', 'Who is the current POTUS?'],\n",
    "            'Answer': ['Definitely not!', 'Donald Trump']\n",
    "        }\n",
    "\n",
    "        vocabs = {'text': [], 'freq': []}\n",
    "\n",
    "        faq_file = gen_mock_csv_file(tmpdirname, faqs)\n",
    "        vocab_file = gen_mock_vocab_answers(tmpdirname, vocabs)\n",
    "\n",
    "        configs_file = gen_faq_config(tmpdirname, vocab_file, faq_file)\n",
    "\n",
    "        select_faq_responses(\n",
    "            question='Is Enrique the prettiest person in town?',\n",
    "            faq_model=train_model(configs_file, download=True)\n",
    "        ) == ['Donald Trump']\n",
    "\n",
    "test_faq_response_with_minimum_faqs_in_dataframe_fail_case()\n",
    "test_faq_response_with_minimum_faqs_in_dataframe_success_case()\n",
    "test_faq_response_with_minimum_answers_vocab_success_case()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def select_squad_responses(\n",
    "    contexts, squad_model, question, best_results=1\n",
    "):\n",
    "    '''Calls Deeppavlov BERT and RNET Context Question Answering\n",
    "    '''\n",
    "    responses = contexts.context.apply(\n",
    "        lambda context: squad_model([context], [question])\n",
    "    ).values\n",
    "    \n",
    "    logging.debug(f'Responses: {responses}')\n",
    "    top_responses = [\n",
    "        r[0][0] for r in sorted(responses, key=lambda x: -1 * x[2][0])\n",
    "        [:best_results]\n",
    "    ]\n",
    "\n",
    "    logging.debug(f'Top Responses: {top_responses}')\n",
    "    return responses, top_responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-06-18 16:40:20.504 INFO in 'deeppavlov.core.data.utils'['utils'] at line 80: Downloading from http://files.deeppavlov.ai/deeppavlov_data/bert/cased_L-12_H-768_A-12.zip to /home/jovyan/.deeppavlov/downloads/cased_L-12_H-768_A-12.zip\n",
      "04:40:20 INFO:Downloading from http://files.deeppavlov.ai/deeppavlov_data/bert/cased_L-12_H-768_A-12.zip to /home/jovyan/.deeppavlov/downloads/cased_L-12_H-768_A-12.zip\n",
      " 85%|████████▍ | 343M/404M [01:35<00:13, 4.71MB/s] "
     ]
    }
   ],
   "source": [
    "#test select_squad_responses\n",
    "import tempfile\n",
    "from shutil import copyfile\n",
    "\n",
    "empty = {'topic': [], 'context': []}\n",
    "spacex = {\n",
    "    'topic': ['SpaceX'],\n",
    "    'context':\n",
    "        [\n",
    "            '''Space Exploration Technologies Corp., trading as SpaceX, is an American aerospace manufacturer and space transportation\n",
    "services company headquartered in Hawthorne, California. It was founded in 2002 by Elon Musk with the goal of reducing space \n",
    "transportation costs to enable the colonization of Mars. SpaceX has developed several launch vehicles, the Starlink satellite\n",
    "constellation, and the Dragon spacecraft. It is widely considered among the most successful private spaceflight companies.'''\n",
    "        ]\n",
    "}\n",
    "\n",
    "intekglobal = {\n",
    "    'topic': ['Intekglobal', 'InG'],\n",
    "    'context':\n",
    "        [\n",
    "            'Intekglobal has its headquarters located in TJ',\n",
    "            'Intekglobal is in the north of mexico'\n",
    "        ]\n",
    "}\n",
    "\n",
    "\n",
    "def assert_squad_model(\n",
    "    contexts, squad_model, question, expected_responses, **args\n",
    "):\n",
    "    responses, top_responses = select_squad_responses(\n",
    "        contexts=pd.DataFrame(contexts),\n",
    "        squad_model=squad_model,\n",
    "        question=question,\n",
    "        **args\n",
    "    )\n",
    "    assert top_responses == expected_responses\n",
    "\n",
    "\n",
    "def test_squad_bert():\n",
    "\n",
    "    bert = build_model(configs.squad.squad_bert, download=True)\n",
    "\n",
    "    assert_squad_model(\n",
    "        empty,\n",
    "        bert,\n",
    "        'Is an empty response expected?',\n",
    "        expected_responses=[],\n",
    "        best_results=2\n",
    "    )\n",
    "\n",
    "    assert_squad_model(\n",
    "        spacex, bert, 'Who founded SpaceX?', expected_responses=['Elon Musk']\n",
    "    )\n",
    "\n",
    "    assert_squad_model(\n",
    "        intekglobal,\n",
    "        bert,\n",
    "        'Where is Intekglobal located?',\n",
    "        expected_responses=['north of mexico','TJ'],\n",
    "        best_results=2\n",
    "    )\n",
    "\n",
    "\n",
    "def test_squad_rnet():\n",
    "\n",
    "    bert = build_model(configs.squad.squad, download=True)\n",
    "\n",
    "    assert_squad_model(\n",
    "        empty,\n",
    "        bert,\n",
    "        'Is an empty response expected?',\n",
    "        expected_responses=[],\n",
    "        best_results=5\n",
    "    )\n",
    "\n",
    "    assert_squad_model(\n",
    "        spacex, bert, 'Who founded SpaceX?', expected_responses=['Elon Musk']\n",
    "    )\n",
    "\n",
    "    assert_squad_model(\n",
    "        intekglobal,\n",
    "        bert,\n",
    "        'Where is Intekglobal located?',\n",
    "        expected_responses=['north of mexico','TJ'],\n",
    "        best_results=2\n",
    "    )\n",
    "\n",
    "test_squad_bert()\n",
    "test_squad_rnet()\n",
    "del spacex, empty, intekglobal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def load_qa_models(\n",
    "    config_rnet=configs.squad.squad,\n",
    "    config_bert=configs.squad.squad_bert,\n",
    "    config_tfidf=configs.faq.tfidf_logreg_en_faq,\n",
    "    download=True\n",
    "):\n",
    "    qa_models = {\n",
    "        'squad':\n",
    "            {\n",
    "                'rnet': build_model(config_rnet, download=download),\n",
    "                'bert': build_model(config_bert, download=download)\n",
    "            },\n",
    "        'faq': {\n",
    "            'tfidf': train_model(config_tfidf, download=download)\n",
    "        }\n",
    "    }\n",
    "    return qa_models\n",
    "\n",
    "\n",
    "def format_responses(question, responses):\n",
    "    formatted_response = f'{question}:\\n\\n'\n",
    "    for k, res in enumerate(responses):\n",
    "        formatted_response += f'{k}: {res}\\n'\n",
    "    return formatted_response\n",
    "\n",
    "\n",
    "def get_responses(contexts, question, qa_models, nb_squad_results=1):\n",
    "    responses = []\n",
    "    for squad_model in qa_models['squad'].values():\n",
    "        responses.extend(\n",
    "            select_squad_responses(\n",
    "                contexts, squad_model, question, best_results=nb_squad_results\n",
    "            )[1]\n",
    "        )\n",
    "    for faq_model in qa_models['faq'].values():\n",
    "        responses.extend(select_faq_responses(faq_model, question))\n",
    "    return responses, format_responses(\n",
    "        question, set([r for r in responses if r.strip()])\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test get_responses\n",
    "import tempfile\n",
    "from shutil import copyfile\n",
    "\n",
    "intekglobal_context = {\n",
    "    'topic': ['Intekglobal', 'InG'],\n",
    "    'context':\n",
    "        [\n",
    "            'Intekglobal has its headquarters located in TJ',\n",
    "            'Intekglobal is in the north of mexico'\n",
    "        ]\n",
    "}\n",
    "\n",
    "intekglobal_faqs = {\n",
    "    'Question': ['Is Intekglobal an IT company?', 'Where can I apply?'],\n",
    "    'Answer':\n",
    "        ['Yes it is!', 'Please refer the our website for further information']\n",
    "}\n",
    "\n",
    "\n",
    "def mock_faq_files(tmpdirname, faqs):\n",
    "\n",
    "    faq_files = {\n",
    "        'data': path.join(tmpdirname, 'temp_faq.csv'),\n",
    "        'config': path.join(tmpdirname, 'temp_config_faq.json')\n",
    "    }\n",
    "\n",
    "    pd.DataFrame(faqs).to_csv(faq_files['data'], index=False)\n",
    "    copyfile(configs.faq.tfidf_logreg_en_faq, faq_files['config'])\n",
    "\n",
    "    updates_faq_config_file(\n",
    "        configs_path=faq_files['config'],\n",
    "        dataset_reader={'data_path': faq_files['data']}\n",
    "    )\n",
    "\n",
    "    return faq_files\n",
    "\n",
    "\n",
    "def test_get_intekglobal_responses():\n",
    "    with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "\n",
    "        faq_files = mock_faq_files(tmpdirname, intekglobal_faqs)\n",
    "        qa_models = load_qa_models(\n",
    "            config_tfidf=faq_files['config'], download=False\n",
    "        )\n",
    "\n",
    "        responses, format_responses = get_responses(\n",
    "            pd.DataFrame(intekglobal_context),\n",
    "            'Where is Intekglobal?',\n",
    "            qa_models,\n",
    "            nb_squad_results=2\n",
    "        )\n",
    "\n",
    "        logging.debug(responses)\n",
    "        logging.debug(format_responses)\n",
    "        assert all(\n",
    "            response in ('north of mexico', 'TJ', 'Yes it is!')\n",
    "            for response in responses\n",
    "        )\n",
    "        #assert ''' Where is Intekglobal?:\n",
    "\n",
    "#0: north of mexico\n",
    "#1: Yes it is!\n",
    "#2: TJ\n",
    "#        '''.strip() == format_responses.strip()\n",
    "\n",
    "\n",
    "def test_get_responses_with_empty_context():\n",
    "    with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "        min_faqs = {'Question': ['Minimum number of questions?','This is the other question?'], 'Answer': ['Two','yes']}\n",
    "        faq_files = mock_faq_files(tmpdirname, min_faqs)\n",
    "\n",
    "        qa_models = load_qa_models(\n",
    "            config_tfidf=faq_files['config'], download=False\n",
    "        )\n",
    "        empty_context = {'topic': [], 'context': []}\n",
    "\n",
    "        responses, format_responses = get_responses(\n",
    "            pd.DataFrame(empty_context),\n",
    "            'What is the minimun number of FAQ questions',\n",
    "            qa_models,\n",
    "            nb_squad_results=2\n",
    "        )\n",
    "\n",
    "        logging.debug(responses)\n",
    "        logging.debug(format_responses)\n",
    "        assert responses == ['Two']\n",
    "\n",
    "\n",
    "test_get_intekglobal_responses()\n",
    "test_get_responses_with_empty_context()\n",
    "\n",
    "del intekglobal_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_input(text):\n",
    "    '''This redundancy is needed for testing'''\n",
    "    return input(text)\n",
    "\n",
    "\n",
    "def new_answer(question, data, qa_models):\n",
    "\n",
    "    if get_input('Give a better answer [y/n]?')[0].lower() != 'y':\n",
    "        return 'no data updates..'\n",
    "\n",
    "    if get_input('Give the answer as a context [y/n]?')[0].lower() == 'y':\n",
    "        new_context = pd.DataFrame(\n",
    "            {\n",
    "                'topic': [get_input('Give context a title:\\n')],\n",
    "                'context': [get_input('Introduce the context:\\n')]\n",
    "            }\n",
    "        )\n",
    "        data['context']['df'] = data['context']['df'].append(new_context)\n",
    "        data['context']['df'].to_csv(data['context']['path'], index=False)\n",
    "\n",
    "        return 'contexts dataset updated..'\n",
    "    else:\n",
    "        new_faq = pd.DataFrame(\n",
    "            {\n",
    "                'Question': [question],\n",
    "                'Answer': [get_input('Introduce the answer:\\n')]\n",
    "            }\n",
    "        )\n",
    "        data['faq']['df'] = data['faq']['df'].append(new_faq)\n",
    "        data['faq']['df'].to_csv(data['faq']['path'], index=False)\n",
    "        qa_models['faq']['tfidf'] = train_model(\n",
    "            data['faq']['config'], download=False\n",
    "        )\n",
    "        return 'FAQ dataset and model updated..'\n",
    "\n",
    "\n",
    "def question_response(data, qa_models, num_returned_values_per_squad_model=1):\n",
    "    question = get_input('Introduce question:\\n')\n",
    "\n",
    "    _, formatted_responses = get_responses(\n",
    "        data['context']['df'], question, qa_models, nb_squad_results=2\n",
    "    )\n",
    "    \n",
    "    print(formatted_responses)\n",
    "\n",
    "    return formatted_responses, new_answer(question, data, qa_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Test FAQ dialog system's part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "from unittest.mock import patch\n",
    "from shutil import copyfile\n",
    "from collections import defaultdict\n",
    "\n",
    "def mock_faq_files(tmpdirname, faqs, faq_dic):\n",
    "\n",
    "    faq_dic['path'] = path.join(tmpdirname, 'temp_faq.csv')\n",
    "    faq_dic['config'] = path.join(tmpdirname, 'temp_config_faq.json')\n",
    "    faq_dic['df'] = pd.DataFrame(faqs)\n",
    "    faq_dic['df'].to_csv(faq_dic['path'], index=False)\n",
    "\n",
    "    copyfile(configs.faq.tfidf_logreg_en_faq, faq_dic['config'])\n",
    "\n",
    "    updates_faq_config_file(\n",
    "        configs_path=faq_dic['config'],\n",
    "        dataset_reader={'data_path': faq_dic['path']}\n",
    "    )\n",
    "\n",
    "\n",
    "def mock_context_file(tmpdirname, contexts, context_dic):\n",
    "\n",
    "    context_dic['path'] = path.join(tmpdirname, 'temp_context.csv')\n",
    "    context_dic['df'] = pd.DataFrame(contexts)\n",
    "    context_dic['df'].to_csv(context_dic['path'], index=False)\n",
    "\n",
    "\n",
    "@patch('__main__.get_input')\n",
    "def test_context_response_with_no_updates(mock_input):\n",
    "    mock_input.side_effect = ['Who is Enrique Jimenez?', 'N']\n",
    "    data = {'context': defaultdict(str), 'faq': defaultdict(str)}\n",
    "    contexts = {\n",
    "        'context':\n",
    "            [\n",
    "                'Intekglobal has its headquarters located in TJ',\n",
    "                'In Intekglobal we care about you',\n",
    "                '''Enrique Jimenez is one of the smartest minds on the planet, \n",
    "                   he currently works as Intekglobal employee'''\n",
    "            ],\n",
    "        'topic': ['headquarters', 'mission', 'Enrique\\'s biography']\n",
    "    }\n",
    "\n",
    "    faqs = {\n",
    "        'Question':\n",
    "            ['Minimum number of questions?', 'This is the other question?'],\n",
    "        'Answer': ['Two', 'yes']\n",
    "    }\n",
    "\n",
    "    with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "\n",
    "        mock_faq_files(tmpdirname, faqs, data['faq'])\n",
    "        mock_context_file(tmpdirname, contexts, data['context'])\n",
    "\n",
    "        qa_models = load_qa_models(\n",
    "            config_tfidf=data['faq']['config'], download=False\n",
    "        )\n",
    "\n",
    "        responses, status = question_response(data, qa_models)\n",
    "        logging.debug(f'  {status}')\n",
    "        logging.debug(f'  {responses}')\n",
    "        assert 'no data updates..' == status\n",
    "        assert 'one of the smartest minds on the planet' in responses\n",
    "\n",
    "@patch('__main__.get_input')\n",
    "def test_updating_faq_dataset(mock_input):\n",
    "\n",
    "    new_answer = 'Intekglobal is one of the best companies in the world'\n",
    "    mock_input.side_effect = ['What is Intekglobal?', 'Y', 'N', new_answer]\n",
    "\n",
    "    data = {'context': defaultdict(str), 'faq': defaultdict(str)}\n",
    "    contexts = {\n",
    "        'context':\n",
    "            [\n",
    "                '''Tesla do important AI research. \n",
    "                The goal is to make an automatic pilot.\n",
    "                '''\n",
    "            ],\n",
    "        'topic': ['Tesla AI']\n",
    "    }\n",
    "\n",
    "    faqs = {\n",
    "        'Question':\n",
    "            ['Who  owns Tesla Company?', 'Is this is heaven?'],\n",
    "        'Answer': ['Elon Musk is the owner of Tesla', 'No, it is life on earth']\n",
    "    }\n",
    "\n",
    "    with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "\n",
    "        mock_faq_files(tmpdirname, faqs, data['faq'])\n",
    "        mock_context_file(tmpdirname, contexts, data['context'])\n",
    "\n",
    "        qa_models = load_qa_models(\n",
    "            config_tfidf=data['faq']['config'], download=False\n",
    "        )\n",
    "\n",
    "        responses, status = question_response(data, qa_models)\n",
    "        \n",
    "        logging.debug(f'  {status}')\n",
    "        logging.debug(f'  {responses}')\n",
    "        \n",
    "        assert 'FAQ dataset and model updated..' == status\n",
    "\n",
    "        updated_faq = pd.read_csv(data['faq']['path'])\n",
    "\n",
    "        assert updated_faq[updated_faq['Answer'] == new_answer\n",
    "                              ].shape[0] == 1\n",
    "\n",
    "        \n",
    "test_context_response_with_no_updates()\n",
    "test_updating_faq_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tests\n",
    "import tempfile\n",
    "from unittest.mock import patch\n",
    "from shutil import copyfile\n",
    "from collections import defaultdict\n",
    "\n",
    "def mock_faq_files(tmpdirname, faqs, faq_dic):\n",
    "\n",
    "    faq_dic['path'] = path.join(tmpdirname, 'temp_faq.csv')\n",
    "    faq_dic['config'] = path.join(tmpdirname, 'temp_config_faq.json')\n",
    "    faq_dic['df'] = pd.DataFrame(faqs)\n",
    "    faq_dic['df'].to_csv(faq_dic['path'], index=False)\n",
    "\n",
    "    copyfile(configs.faq.tfidf_logreg_en_faq, faq_dic['config'])\n",
    "\n",
    "    updates_faq_config_file(\n",
    "        configs_path=faq_dic['config'],\n",
    "        dataset_reader={'data_path': faq_dic['path']}\n",
    "    )\n",
    "\n",
    "\n",
    "def mock_context_file(tmpdirname, contexts, context_dic):\n",
    "\n",
    "    context_dic['path'] = path.join(tmpdirname, 'temp_context.csv')\n",
    "    context_dic['df'] = pd.DataFrame(contexts)\n",
    "    context_dic['df'].to_csv(context_dic['path'], index=False)\n",
    "\n",
    "@patch('__main__.get_input')\n",
    "def test_updated_faq_response(mock_input):\n",
    "\n",
    "    new_answer = 'Intekglobal is one of the best companies in the world'\n",
    "    question = 'What is Intekglobal?'\n",
    "    mock_input.side_effect = [question, 'Y', 'N', new_answer, question, 'N']\n",
    "\n",
    "    data = {'context': defaultdict(str), 'faq': defaultdict(str)}\n",
    "    contexts = {\n",
    "        'context':\n",
    "            [\n",
    "                '''One of the greatest punk rock bands from all the time\n",
    "                is the Ramones.\n",
    "                '''\n",
    "            ],\n",
    "        'topic': ['Ramones']\n",
    "    }\n",
    "\n",
    "    faqs = {\n",
    "        'Question':\n",
    "            ['Who was the leading vocal artist of the Ramones?', 'Is this is heaven?'],\n",
    "        'Answer': ['Joey Ramone', 'No, it is life on earth']\n",
    "    }\n",
    "\n",
    "    with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "\n",
    "        mock_faq_files(tmpdirname, faqs, data['faq'])\n",
    "        mock_context_file(tmpdirname, contexts, data['context'])\n",
    "\n",
    "        qa_models = load_qa_models(\n",
    "            config_tfidf=data['faq']['config'], download=False\n",
    "        )\n",
    "        old_responses, old_status = question_response(data, qa_models)\n",
    "        new_responses, new_status = question_response(data, qa_models)\n",
    "\n",
    "        logging.debug(f'Old response:\\n {old_responses}')\n",
    "        logging.debug(f'New response:\\n{new_responses}')\n",
    "        \n",
    "        new_answer not in old_responses\n",
    "        new_answer in new_responses\n",
    "\n",
    "test_updated_faq_response()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Contex Dialog System's part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "from unittest.mock import patch\n",
    "from shutil import copyfile\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "def mock_faq_files(tmpdirname, faqs, faq_dic):\n",
    "\n",
    "    faq_dic['path'] = path.join(tmpdirname, 'temp_faq.csv')\n",
    "    faq_dic['config'] = path.join(tmpdirname, 'temp_config_faq.json')\n",
    "    faq_dic['df'] = pd.DataFrame(faqs)\n",
    "    faq_dic['df'].to_csv(faq_dic['path'], index=False)\n",
    "\n",
    "    copyfile(configs.faq.tfidf_logreg_en_faq, faq_dic['config'])\n",
    "\n",
    "    updates_faq_config_file(\n",
    "        configs_path=faq_dic['config'],\n",
    "        dataset_reader={'data_path': faq_dic['path']}\n",
    "    )\n",
    "\n",
    "\n",
    "def mock_context_file(tmpdirname, contexts, context_dic):\n",
    "\n",
    "    context_dic['path'] = path.join(tmpdirname, 'temp_context.csv')\n",
    "    context_dic['df'] = pd.DataFrame(contexts)\n",
    "    context_dic['df'].to_csv(context_dic['path'], index=False)\n",
    "\n",
    "\n",
    "@patch('__main__.get_input')\n",
    "def test_updated_context_response(mock_input):\n",
    "    data = {'context': defaultdict(str), 'faq': defaultdict(str)}\n",
    "\n",
    "    contexts = {\n",
    "        'context': ['Intekglobal has its headquarters located in TJ', ],\n",
    "        'topic': ['headquarters']\n",
    "    }\n",
    "\n",
    "    faqs = {\n",
    "        'Question':\n",
    "            ['Minimum number of questions?', 'This is the other question?'],\n",
    "        'Answer': ['Two', 'yes']\n",
    "    }\n",
    "\n",
    "    question = 'What is a Chatbot?'\n",
    "    new_topic = 'AI Tool & Chatbot Development'\n",
    "    new_context = '''\n",
    "\n",
    "A chatbot is an important tool for simulating intelligent conversations with humans.\n",
    "Intekglobal chatbots efficiently live message on platforms such as Facebook Messenger, \n",
    "Slack, and Telegram. Assisting consumers with a variety of purposes and industries. \n",
    "\n",
    "But chatbots are more than just a cool technology advancement. They actually transform the user experience.\n",
    "People want simple and convenient interactions with interface and products.\n",
    "\n",
    "'''\n",
    "\n",
    "    mock_input.side_effect = [\n",
    "        question, 'YES', 'yes', new_topic, new_context, question, 'N'\n",
    "    ]\n",
    "\n",
    "    with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "\n",
    "        mock_faq_files(tmpdirname, faqs, data['faq'])\n",
    "        mock_context_file(tmpdirname, contexts, data['context'])\n",
    "\n",
    "        qa_models = load_qa_models(\n",
    "            config_tfidf=data['faq']['config'], download=False\n",
    "        )\n",
    "\n",
    "        old_responses, old_status = question_response(data, qa_models)\n",
    "        new_responses, new_status = question_response(data, qa_models)\n",
    "\n",
    "        logging.info(f'Old status: {old_status}')\n",
    "        logging.info(f'Old response:\\n {old_responses}')\n",
    "        logging.info(f'New response:\\n{new_responses}')\n",
    "\n",
    "        updated_context = pd.read_csv(data['context']['path'])\n",
    "\n",
    "        assert updated_context[updated_context['context'] == new_context\n",
    "                              ].shape[0] == 1\n",
    "\n",
    "        assert updated_context[updated_context['topic'] == new_topic\n",
    "                              ].shape[0] == 1\n",
    "\n",
    "        assert 'an important tool for simulating intelligent conversations with humans' not in old_responses\n",
    "        assert 'an important tool for simulating intelligent conversations with humans' in new_responses\n",
    "\n",
    "\n",
    "test_updated_context_response()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def set_minimal_faq_questions(data):\n",
    "    if data['df'].shape[0] > 1:\n",
    "        return\n",
    "    minimal_questions = [\n",
    "        'Is this the Intekglobal Dialog System?',\n",
    "        'What is the purpose of these two automated questions?'\n",
    "    ]\n",
    "    minimal_answers = [\n",
    "        'This is the Intekglobal Dialog System', 'To populate the FAQ data file'\n",
    "    ]\n",
    "    minimal_faqs_df = pd.DataFrame(\n",
    "        {\n",
    "            'Question': minimal_questions,\n",
    "            'Answer': minimal_answers\n",
    "        }\n",
    "    )\n",
    "    data['df'] = pd.concat([data['df'], minimal_faqs_df])\n",
    "    data['df'].to_csv(data['path'], index=False)\n",
    "    logging.info(f' File created at {data[\"path\"]}')\n",
    "\n",
    "\n",
    "def set_minimal_contexts(data):\n",
    "    if data['df'].shape[0] > 0:\n",
    "        return\n",
    "\n",
    "    minimal_context_df = pd.DataFrame({'topic': [], 'context': []})\n",
    "    data['df'] = minimal_context_df\n",
    "    data['df'].to_csv(data['path'], index=False)\n",
    "\n",
    "    logging.info(f' File created at {data[\"path\"]}')\n",
    "\n",
    "\n",
    "def set_data_dict(file, data, question_type, data_dir):\n",
    "\n",
    "    data['path'] = file if file is not None else path.join(\n",
    "        data_dir, question_type + '_data.csv'\n",
    "    )\n",
    "\n",
    "    data['df'] = pd.read_csv(data['path']) if path.isfile(data['path']\n",
    "                                                         ) else pd.DataFrame()\n",
    "\n",
    "    if question_type == 'faq':\n",
    "        set_minimal_faq_questions(data)\n",
    "    if question_type == 'context':\n",
    "        set_minimal_contexts(data)\n",
    "\n",
    "\n",
    "def load_and_prepare_data(context_data_file, faq_data_file, data, configs_faq):\n",
    "\n",
    "    PARENT_DIR = popen('$PWD').read().strip()\n",
    "\n",
    "    if faq_data_file or context_data_file is None:\n",
    "        DATA_DIR = path.join(PARENT_DIR, 'data')\n",
    "\n",
    "        if not path.isdir(DATA_DIR):\n",
    "            mkdir(DATA_DIR)\n",
    "            logging.info(f'Data directory created at {DATA_DIR}')\n",
    "\n",
    "    if configs_faq is None:\n",
    "        configs_faq = path.join(PARENT_DIR, 'config_faq.json')\n",
    "        copyfile(configs.faq.tfidf_logreg_en_faq, configs_faq)\n",
    "\n",
    "    data['faq']['config'] = configs_faq\n",
    "\n",
    "    set_data_dict(\n",
    "        file=faq_data_file,\n",
    "        data=data['faq'],\n",
    "        question_type='faq',\n",
    "        data_dir=DATA_DIR\n",
    "    )\n",
    "    set_data_dict(\n",
    "        file=context_data_file,\n",
    "        data=data['context'],\n",
    "        question_type='context',\n",
    "        data_dir=DATA_DIR\n",
    "    )\n",
    "\n",
    "    updates_faq_config_file(\n",
    "        configs_path=configs_faq,\n",
    "        dataset_reader={'data_path': data['faq']['path']}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tests\n",
    "import tempfile\n",
    "from collections import defaultdict\n",
    "from shutil import rmtree\n",
    "from unittest.mock import patch\n",
    "from let_me_answer_for_you.dialog_system import updates_faq_config_file\n",
    "\n",
    "\n",
    "def test_set_minimal_faqs_with_more_than_one_question():\n",
    "    with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "        data_file = path.join(tmpdirname, 'tmp_data.csv')\n",
    "        questions = ['a?', 'b?']\n",
    "        answers = ['a', 'b']\n",
    "        df = pd.DataFrame({'Question': questions, 'Answer': answers})\n",
    "        df.to_csv(data_file, index=False)\n",
    "        data = {'df': df, 'path': data_file}\n",
    "        set_minimal_faq_questions(data)\n",
    "\n",
    "        assert data['df'].shape[0] == 2\n",
    "\n",
    "\n",
    "def test_set_minimal_faqs_with_less_than_two_questions():\n",
    "    with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "        data_file = path.join(tmpdirname, 'tmp_data.csv')\n",
    "        questions = ['a?']\n",
    "        answers = ['a']\n",
    "        df = pd.DataFrame({'Question': questions, 'Answer': answers})\n",
    "        df.to_csv(data_file, index=False)\n",
    "        data = {'df': df, 'path': data_file}\n",
    "\n",
    "        assert data['df'].shape[0] == 1\n",
    "\n",
    "        set_minimal_faq_questions(data)\n",
    "\n",
    "        assert data['df'].shape[0] == 3\n",
    "        assert any(\n",
    "            data['df'].Question == 'Is this the Intekglobal Dialog System?'\n",
    "        )\n",
    "\n",
    "\n",
    "def test_set_minimal_contexts():\n",
    "    with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "        data_file = path.join(tmpdirname, 'tmp_data.csv')\n",
    "        data = {'df': pd.DataFrame(), 'path': data_file}\n",
    "        set_minimal_contexts(data)\n",
    "        assert path.isfile(data['path'])\n",
    "        assert all(data['df'].columns == ['topic', 'context'])\n",
    "\n",
    "\n",
    "def test_set_data_dict_no_file():\n",
    "    with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "        data = {'context': defaultdict(str)}\n",
    "        set_data_dict(\n",
    "            file=None,\n",
    "            data=data['context'],\n",
    "            data_dir=tmpdirname,\n",
    "            question_type='context'\n",
    "        )\n",
    "        logging.debug(data)\n",
    "        assert path.isfile(data['context']['path'])\n",
    "\n",
    "\n",
    "@patch('__main__.popen')\n",
    "def test_load_and_prepare_data(mock_popen):\n",
    "    with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "        mock_popen(\"$PWD\").read().strip.side_effect = [tmpdirname]\n",
    "        #tmp_config_file = path.join(tmpdirname, 'tmp_file.json')\n",
    "        #copyfile(configs.faq.tfidf_logreg_en_faq, tmp_config_file)\n",
    "        data = {'context': defaultdict(str), 'faq': defaultdict(str)}\n",
    "\n",
    "        load_and_prepare_data(\n",
    "            context_data_file=None,\n",
    "            faq_data_file=None,\n",
    "            data=data,\n",
    "            configs_faq=None\n",
    "        )\n",
    "        data_dir = path.join(tmpdirname, 'data')\n",
    "        config_file = path.join(tmpdirname,'config_faq.json')\n",
    "        assert path.isdir(data_dir)\n",
    "        assert path.isfile(config_file)\n",
    "\n",
    "\n",
    "test_set_minimal_faqs_with_more_than_one_question()\n",
    "test_set_minimal_faqs_with_less_than_two_questions()\n",
    "test_set_minimal_contexts()\n",
    "test_set_data_dict_no_file()\n",
    "test_load_and_prepare_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def dialog_system(context_data_file=None, faq_data_file=None, configs_faq=None):\n",
    "    '''\n",
    "     Main Dialog System\n",
    "    '''\n",
    "    run_shell_installs()\n",
    "\n",
    "    data = {'context': defaultdict(str), 'faq': defaultdict(str)}\n",
    "\n",
    "    load_and_prepare_data(\n",
    "        context_data_file=context_data_file,\n",
    "        faq_data_file=faq_data_file,\n",
    "        configs_faq=configs_faq,\n",
    "        data=data\n",
    "    )\n",
    "\n",
    "    qa_models = load_qa_models(config_tfidf=data['faq']['config'])\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            responses, _ = question_response(data=data, qa_models=qa_models)\n",
    "        except (KeyboardInterrupt, EOFError, SystemExit):\n",
    "            logging.debug('Goodbye!')\n",
    "            return 'Goodbye!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test  dialog_system()\n",
    "\n",
    "from unittest.mock import patch\n",
    "from let_me_answer_for_you.dialog_system import load_qa_models, load_and_prepare_data\n",
    "from let_me_answer_for_you.dialog_system import question_response,run_shell_installs\n",
    "\n",
    "@patch('__main__.run_shell_installs')\n",
    "@patch('__main__.load_qa_models')\n",
    "@patch('__main__.load_and_prepare_data')\n",
    "@patch('__main__.question_response')\n",
    "def test_main_keyboard_interrupt(\n",
    "    mock_question_response,\n",
    "    mock_pd_read_csv,\n",
    "    mock_load_qa_models,\n",
    "    mock_run_shell_installs,\n",
    "):\n",
    "    mock_question_response.side_effect = [\n",
    "        KeyboardInterrupt(), EOFError(),\n",
    "        SystemExit()\n",
    "    ]\n",
    "    assert 'Goodbye!' == dialog_system()\n",
    "    assert 'Goodbye!' == dialog_system()\n",
    "    assert 'Goodbye!' == dialog_system()\n",
    "\n",
    "\n",
    "test_main_keyboard_interrupt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
