{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "!pip install -r ../requirements.txt > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp dialog_system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "09:27:12 INFO:Hello! Welcome to our automated dialog system!\n",
      "09:27:12 DEBUG:test: for Debug?\n",
      "09:27:12 ERROR: Error Log Active \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# export\n",
    "from deeppavlov import configs,build_model,train_model\n",
    "import json\n",
    "from os import path,popen\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s %(levelname)s:%(message)s', level=logging.DEBUG, datefmt='%I:%M:%S')\n",
    "logging.info(\"Hello! Welcome to our automated dialog system!\")\n",
    "logging.debug(\"test: for Debug?\")\n",
    "logging.error(' Error Log Active ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def run_shell_installs():\n",
    "    ''' Run install commands\n",
    "    '''\n",
    "    command_strings = (\n",
    "        ' pip install deeppavlov',\n",
    "        ' python -m deeppavlov install squad',\n",
    "        ' python -m deeppavlov install squad_bert',\n",
    "        ' python -m deeppavlov install fasttext_avg_autofaq',\n",
    "        ' python -m deeppavlov install fasttext_tfidf_autofaq',\n",
    "        ' python -m deeppavlov install tfidf_autofaq',\n",
    "        ' python -m deeppavlov install tfidf_logreg_autofaq ',\n",
    "        ' python -m deeppavlov install tfidf_logreg_en_faq'\n",
    "    )\n",
    "    for command in command_strings:\n",
    "        logging.debug(command)\n",
    "        logging.debug(popen(command).read())\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dialog System\n",
    "> Question Answering Automated Dialog System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "09:27:12 DEBUG: pip install deeppavlov\n",
      "09:27:13 DEBUG:Requirement already satisfied: deeppavlov in /opt/conda/lib/python3.7/site-packages (0.10.0)\n",
      "Requirement already satisfied: ruamel.yaml==0.15.100 in /opt/conda/lib/python3.7/site-packages (from deeppavlov) (0.15.100)\n",
      "Requirement already satisfied: scipy==1.4.1 in /opt/conda/lib/python3.7/site-packages (from deeppavlov) (1.4.1)\n",
      "Requirement already satisfied: numpy==1.18.0 in /opt/conda/lib/python3.7/site-packages (from deeppavlov) (1.18.0)\n",
      "Requirement already satisfied: pydantic==1.3 in /opt/conda/lib/python3.7/site-packages (from deeppavlov) (1.3)\n",
      "Requirement already satisfied: pytelegrambotapi==3.6.7 in /opt/conda/lib/python3.7/site-packages (from deeppavlov) (3.6.7)\n",
      "Requirement already satisfied: pandas==0.25.3 in /opt/conda/lib/python3.7/site-packages (from deeppavlov) (0.25.3)\n",
      "Requirement already satisfied: tqdm==4.41.1 in /opt/conda/lib/python3.7/site-packages (from deeppavlov) (4.41.1)\n",
      "Requirement already satisfied: pyopenssl==19.1.0 in /opt/conda/lib/python3.7/site-packages (from deeppavlov) (19.1.0)\n",
      "Requirement already satisfied: fastapi==0.47.1 in /opt/conda/lib/python3.7/site-packages (from deeppavlov) (0.47.1)\n",
      "Requirement already satisfied: h5py==2.10.0 in /opt/conda/lib/python3.7/site-packages (from deeppavlov) (2.10.0)\n",
      "Requirement already satisfied: requests==2.22.0 in /opt/conda/lib/python3.7/site-packages (from deeppavlov) (2.22.0)\n",
      "Requirement already satisfied: scikit-learn==0.21.2 in /opt/conda/lib/python3.7/site-packages (from deeppavlov) (0.21.2)\n",
      "Requirement already satisfied: uvicorn==0.11.1 in /opt/conda/lib/python3.7/site-packages (from deeppavlov) (0.11.1)\n",
      "Requirement already satisfied: pymorphy2-dicts-ru in /opt/conda/lib/python3.7/site-packages (from deeppavlov) (2.4.404381.4453942)\n",
      "Requirement already satisfied: nltk==3.4.5 in /opt/conda/lib/python3.7/site-packages (from deeppavlov) (3.4.5)\n",
      "Requirement already satisfied: overrides==2.7.0 in /opt/conda/lib/python3.7/site-packages (from deeppavlov) (2.7.0)\n",
      "Requirement already satisfied: Cython==0.29.14 in /opt/conda/lib/python3.7/site-packages (from deeppavlov) (0.29.14)\n",
      "Requirement already satisfied: pymorphy2==0.8 in /opt/conda/lib/python3.7/site-packages (from deeppavlov) (0.8)\n",
      "Requirement already satisfied: sacremoses==0.0.35 in /opt/conda/lib/python3.7/site-packages (from deeppavlov) (0.0.35)\n",
      "Requirement already satisfied: rusenttokenize==0.0.5 in /opt/conda/lib/python3.7/site-packages (from deeppavlov) (0.0.5)\n",
      "Requirement already satisfied: aio-pika==6.4.1 in /opt/conda/lib/python3.7/site-packages (from deeppavlov) (6.4.1)\n",
      "Requirement already satisfied: pytz==2019.1 in /opt/conda/lib/python3.7/site-packages (from deeppavlov) (2019.1)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from pytelegrambotapi==3.6.7->deeppavlov) (1.14.0)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /opt/conda/lib/python3.7/site-packages (from pandas==0.25.3->deeppavlov) (2.8.1)\n",
      "Requirement already satisfied: cryptography>=2.8 in /opt/conda/lib/python3.7/site-packages (from pyopenssl==19.1.0->deeppavlov) (2.8)\n",
      "Requirement already satisfied: starlette<=0.12.9,>=0.12.9 in /opt/conda/lib/python3.7/site-packages (from fastapi==0.47.1->deeppavlov) (0.12.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests==2.22.0->deeppavlov) (2019.11.28)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests==2.22.0->deeppavlov) (3.0.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests==2.22.0->deeppavlov) (2.8)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests==2.22.0->deeppavlov) (1.25.7)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn==0.21.2->deeppavlov) (0.15.1)\n",
      "Requirement already satisfied: httptools==0.0.13; sys_platform != \"win32\" and sys_platform != \"cygwin\" and platform_python_implementation != \"pypy\" in /opt/conda/lib/python3.7/site-packages (from uvicorn==0.11.1->deeppavlov) (0.0.13)\n",
      "Requirement already satisfied: websockets==8.* in /opt/conda/lib/python3.7/site-packages (from uvicorn==0.11.1->deeppavlov) (8.1)\n",
      "Requirement already satisfied: click==7.* in /opt/conda/lib/python3.7/site-packages (from uvicorn==0.11.1->deeppavlov) (7.1.2)\n",
      "Requirement already satisfied: h11<0.10,>=0.8 in /opt/conda/lib/python3.7/site-packages (from uvicorn==0.11.1->deeppavlov) (0.9.0)\n",
      "Requirement already satisfied: uvloop>=0.14.0; sys_platform != \"win32\" and sys_platform != \"cygwin\" and platform_python_implementation != \"pypy\" in /opt/conda/lib/python3.7/site-packages (from uvicorn==0.11.1->deeppavlov) (0.14.0)\n",
      "Requirement already satisfied: docopt>=0.6 in /opt/conda/lib/python3.7/site-packages (from pymorphy2==0.8->deeppavlov) (0.6.2)\n",
      "Requirement already satisfied: pymorphy2-dicts<3.0,>=2.4 in /opt/conda/lib/python3.7/site-packages (from pymorphy2==0.8->deeppavlov) (2.4.393442.3710985)\n",
      "Requirement already satisfied: dawg-python>=0.7 in /opt/conda/lib/python3.7/site-packages (from pymorphy2==0.8->deeppavlov) (0.7.2)\n",
      "Requirement already satisfied: aiormq<4,>=3.2.0 in /opt/conda/lib/python3.7/site-packages (from aio-pika==6.4.1->deeppavlov) (3.2.2)\n",
      "Requirement already satisfied: yarl in /opt/conda/lib/python3.7/site-packages (from aio-pika==6.4.1->deeppavlov) (1.4.2)\n",
      "Requirement already satisfied: cffi!=1.11.3,>=1.8 in /opt/conda/lib/python3.7/site-packages (from cryptography>=2.8->pyopenssl==19.1.0->deeppavlov) (1.14.0)\n",
      "Requirement already satisfied: pamqp==2.3.0 in /opt/conda/lib/python3.7/site-packages (from aiormq<4,>=3.2.0->aio-pika==6.4.1->deeppavlov) (2.3.0)\n",
      "Requirement already satisfied: multidict>=4.0 in /opt/conda/lib/python3.7/site-packages (from yarl->aio-pika==6.4.1->deeppavlov) (4.7.6)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.7/site-packages (from cffi!=1.11.3,>=1.8->cryptography>=2.8->pyopenssl==19.1.0->deeppavlov) (2.20)\n",
      "\n",
      "09:27:13 DEBUG: python -m deeppavlov install squad\n",
      "09:27:52 DEBUG:Collecting tensorflow==1.15.2\n",
      "  Downloading tensorflow-1.15.2-cp37-cp37m-manylinux2010_x86_64.whl (110.5 MB)\n",
      "Requirement already satisfied: tensorboard<1.16.0,>=1.15.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.15.2) (1.15.0)\n",
      "Requirement already satisfied: keras-applications>=1.0.8 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.15.2) (1.0.8)\n",
      "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.15.2) (0.34.2)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.15.2) (0.9.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.16.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.15.2) (1.18.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.15.2) (3.2.1)\n",
      "Requirement already satisfied: protobuf>=3.6.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.15.2) (3.12.2)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.15.2) (1.1.0)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.15.2) (1.29.0)\n",
      "Requirement already satisfied: tensorflow-estimator==1.15.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.15.2) (1.15.1)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.15.2) (1.12.1)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.15.2) (1.1.2)\n",
      "Requirement already satisfied: six>=1.10.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.15.2) (1.14.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.6 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.15.2) (0.2.0)\n",
      "Requirement already satisfied: astor>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.15.2) (0.8.1)\n",
      "Requirement already satisfied: gast==0.2.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.15.2) (0.2.2)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (46.0.0.post20200311)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.7/site-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (3.2.2)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /opt/conda/lib/python3.7/site-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (1.0.1)\n",
      "Requirement already satisfied: h5py in /opt/conda/lib/python3.7/site-packages (from keras-applications>=1.0.8->tensorflow==1.15.2) (2.10.0)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /opt/conda/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (1.5.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (3.1.0)\n",
      "Installing collected packages: tensorflow\n",
      "  Attempting uninstall: tensorflow\n",
      "    Found existing installation: tensorflow 1.15.3\n",
      "    Uninstalling tensorflow-1.15.3:\n",
      "      Successfully uninstalled tensorflow-1.15.3\n",
      "Successfully installed tensorflow-1.15.2\n",
      "\n",
      "09:27:52 DEBUG: python -m deeppavlov install squad_bert\n",
      "09:27:58 DEBUG:Collecting git+https://github.com/deepmipt/bert.git@feat/multi_gpu\n",
      "  Cloning https://github.com/deepmipt/bert.git (to revision feat/multi_gpu) to /tmp/pip-req-build-td3nwfzk\n",
      "Building wheels for collected packages: bert-dp\n",
      "  Building wheel for bert-dp (setup.py): started\n",
      "  Building wheel for bert-dp (setup.py): finished with status 'done'\n",
      "  Created wheel for bert-dp: filename=bert_dp-1.0-py3-none-any.whl size=23580 sha256=0a6956172d8fb8c68b7768230dbb8643aac7562e8efe6926ccae1716dd4c1038\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-97ljdzeu/wheels/44/29/b2/ee614cb7f97ba5c2d220029eaede3af4b74331ad31d6e2f4eb\n",
      "Successfully built bert-dp\n",
      "Installing collected packages: bert-dp\n",
      "Successfully installed bert-dp-1.0\n",
      "Requirement already satisfied: tensorflow==1.15.2 in /opt/conda/lib/python3.7/site-packages (1.15.2)\n",
      "Requirement already satisfied: google-pasta>=0.1.6 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.15.2) (0.2.0)\n",
      "Requirement already satisfied: astor>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.15.2) (0.8.1)\n",
      "Requirement already satisfied: tensorflow-estimator==1.15.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.15.2) (1.15.1)\n",
      "Requirement already satisfied: keras-applications>=1.0.8 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.15.2) (1.0.8)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.15.2) (1.1.0)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.15.2) (1.29.0)\n",
      "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.15.2) (0.34.2)\n",
      "Requirement already satisfied: gast==0.2.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.15.2) (0.2.2)\n",
      "Requirement already satisfied: numpy<2.0,>=1.16.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.15.2) (1.18.0)\n",
      "Requirement already satisfied: six>=1.10.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.15.2) (1.14.0)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.15.2) (0.9.0)\n",
      "Requirement already satisfied: tensorboard<1.16.0,>=1.15.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.15.2) (1.15.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.15.2) (1.1.2)\n",
      "Requirement already satisfied: protobuf>=3.6.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.15.2) (3.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.15.2) (1.12.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.15.2) (3.2.1)\n",
      "Requirement already satisfied: h5py in /opt/conda/lib/python3.7/site-packages (from keras-applications>=1.0.8->tensorflow==1.15.2) (2.10.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /opt/conda/lib/python3.7/site-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (1.0.1)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (46.0.0.post20200311)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.7/site-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (3.2.2)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /opt/conda/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (1.5.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (3.1.0)\n",
      "\n",
      "09:27:58 DEBUG: python -m deeppavlov install fasttext_avg_autofaq\n",
      "09:28:46 DEBUG:Collecting fasttext==0.9.1\n",
      "  Downloading fasttext-0.9.1.tar.gz (57 kB)\n",
      "Requirement already satisfied: pybind11>=2.2 in /opt/conda/lib/python3.7/site-packages (from fasttext==0.9.1) (2.5.0)\n",
      "Requirement already satisfied: setuptools>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from fasttext==0.9.1) (46.0.0.post20200311)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from fasttext==0.9.1) (1.18.0)\n",
      "Building wheels for collected packages: fasttext\n",
      "  Building wheel for fasttext (setup.py): started\n",
      "  Building wheel for fasttext (setup.py): finished with status 'done'\n",
      "  Created wheel for fasttext: filename=fasttext-0.9.1-cp37-cp37m-linux_x86_64.whl size=2482009 sha256=448ac8205e21e06adcdd34c7f2a1d4ae0bf459e4e213ba30227ea7f9342d68ba\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/b2/5b/4b/9c582c778bb93aaad8fc855d5e79f49eae34f59e363a22c422\n",
      "Successfully built fasttext\n",
      "Installing collected packages: fasttext\n",
      "Successfully installed fasttext-0.9.1\n",
      "\n",
      "09:28:46 DEBUG: python -m deeppavlov install fasttext_tfidf_autofaq\n",
      "09:28:48 DEBUG:Requirement already satisfied: fasttext==0.9.1 in /opt/conda/lib/python3.7/site-packages (0.9.1)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from fasttext==0.9.1) (1.18.0)\n",
      "Requirement already satisfied: setuptools>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from fasttext==0.9.1) (46.0.0.post20200311)\n",
      "Requirement already satisfied: pybind11>=2.2 in /opt/conda/lib/python3.7/site-packages (from fasttext==0.9.1) (2.5.0)\n",
      "\n",
      "09:28:48 DEBUG: python -m deeppavlov install tfidf_autofaq\n",
      "09:28:49 DEBUG:\n",
      "09:28:49 DEBUG: python -m deeppavlov install tfidf_logreg_autofaq \n",
      "09:28:51 DEBUG:\n",
      "09:28:51 DEBUG: python -m deeppavlov install tfidf_logreg_en_faq\n",
      "09:29:11 DEBUG:Collecting spacy==2.2.3\n",
      "  Downloading spacy-2.2.3-cp37-cp37m-manylinux1_x86_64.whl (10.4 MB)\n",
      "Collecting cymem<2.1.0,>=2.0.2\n",
      "  Downloading cymem-2.0.3-cp37-cp37m-manylinux1_x86_64.whl (32 kB)\n",
      "Collecting catalogue<1.1.0,>=0.0.7\n",
      "  Downloading catalogue-1.0.0-py2.py3-none-any.whl (7.7 kB)\n",
      "Collecting plac<1.2.0,>=0.9.6\n",
      "  Downloading plac-1.1.3-py2.py3-none-any.whl (20 kB)\n",
      "Collecting blis<0.5.0,>=0.4.0\n",
      "  Downloading blis-0.4.1-cp37-cp37m-manylinux1_x86_64.whl (3.7 MB)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/conda/lib/python3.7/site-packages (from spacy==2.2.3) (2.22.0)\n",
      "Collecting preshed<3.1.0,>=3.0.2\n",
      "  Downloading preshed-3.0.2-cp37-cp37m-manylinux1_x86_64.whl (118 kB)\n",
      "Collecting thinc<7.4.0,>=7.3.0\n",
      "  Downloading thinc-7.3.1-cp37-cp37m-manylinux1_x86_64.whl (2.2 MB)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from spacy==2.2.3) (46.0.0.post20200311)\n",
      "Collecting wasabi<1.1.0,>=0.4.0\n",
      "  Downloading wasabi-0.6.0-py3-none-any.whl (20 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0\n",
      "  Downloading murmurhash-1.0.2-cp37-cp37m-manylinux1_x86_64.whl (19 kB)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /opt/conda/lib/python3.7/site-packages (from spacy==2.2.3) (1.18.0)\n",
      "Collecting srsly<1.1.0,>=0.1.0\n",
      "  Downloading srsly-1.0.2-cp37-cp37m-manylinux1_x86_64.whl (185 kB)\n",
      "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /opt/conda/lib/python3.7/site-packages (from catalogue<1.1.0,>=0.0.7->spacy==2.2.3) (1.5.0)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy==2.2.3) (1.25.7)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy==2.2.3) (2.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy==2.2.3) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy==2.2.3) (2019.11.28)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /opt/conda/lib/python3.7/site-packages (from thinc<7.4.0,>=7.3.0->spacy==2.2.3) (4.41.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy==2.2.3) (3.1.0)\n",
      "Installing collected packages: cymem, catalogue, plac, blis, murmurhash, preshed, wasabi, srsly, thinc, spacy\n",
      "Successfully installed blis-0.4.1 catalogue-1.0.0 cymem-2.0.3 murmurhash-1.0.2 plac-1.1.3 preshed-3.0.2 spacy-2.2.3 srsly-1.0.2 thinc-7.3.1 wasabi-0.6.0\n",
      "Collecting en_core_web_sm==2.2.5\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz (12.0 MB)\n",
      "Requirement already satisfied: spacy>=2.2.2 in /opt/conda/lib/python3.7/site-packages (from en_core_web_sm==2.2.5) (2.2.3)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (46.0.0.post20200311)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.6.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /opt/conda/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.18.0)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /opt/conda/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
      "Requirement already satisfied: thinc<7.4.0,>=7.3.0 in /opt/conda/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.3.1)\n",
      "Requirement already satisfied: srsly<1.1.0,>=0.1.0 in /opt/conda/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /opt/conda/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/conda/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.22.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.3)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.2)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /opt/conda/lib/python3.7/site-packages (from thinc<7.4.0,>=7.3.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (4.41.1)\n",
      "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /opt/conda/lib/python3.7/site-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2019.11.28)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.25.7)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.8)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.1.0)\n",
      "Building wheels for collected packages: en-core-web-sm\n",
      "  Building wheel for en-core-web-sm (setup.py): started\n",
      "  Building wheel for en-core-web-sm (setup.py): finished with status 'done'\n",
      "  Created wheel for en-core-web-sm: filename=en_core_web_sm-2.2.5-py3-none-any.whl size=12011738 sha256=60a8d920258131e85f671aed1c0d6bf2afd9a8e88a4181832a56d0fef9380200\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/51/19/da/a3885266a3c241aff0ad2eb674ae058fd34a4870fef1c0a5a0\n",
      "Successfully built en-core-web-sm\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-2.2.5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "run_shell_installs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def action_over_list_f(arr, v):\n",
    "\n",
    "    k_id, v_id = next(iter(v[0].items()))\n",
    "\n",
    "    for p, a in enumerate(arr):\n",
    "        if k_id in a.keys() and a[k_id] == v_id:\n",
    "            for k_rep, v_rep in v[1].items():\n",
    "                arr[p][k_rep] = v_rep\n",
    "\n",
    "\n",
    "def replacement_f(model_config, **args):\n",
    "    '''Replaces the model config dictionary with new values\n",
    "    '''\n",
    "    for k, v in args.items():\n",
    "        if isinstance(v, dict):\n",
    "            replacement_f(model_config[k], **v)\n",
    "        if isinstance(v, str):\n",
    "            model_config[k] = v\n",
    "        if isinstance(model_config[k], list):\n",
    "            action_over_list_f(model_config[k], v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test action_over_list_f\n",
    "from random import randint\n",
    "\n",
    "\n",
    "def gen_list_keys_for_tests():\n",
    "\n",
    "    str_n = lambda x: f'{x}_{randint(1,10):1}'\n",
    "    gen_dict_list = lambda: {\n",
    "        'id': str_n('id'),\n",
    "        'key1': str_n('v1'),\n",
    "        'key2': str_n('v2'),\n",
    "        'key3': str_n('v3')\n",
    "    }\n",
    "\n",
    "    pipe_list = [gen_dict_list() for _ in range(randint(3, 10))]\n",
    "\n",
    "    rand_id = pipe_list[randint(0, len(pipe_list) - 1)]['id']\n",
    "    rand_key = f'key{randint(1, 3)}' \n",
    "\n",
    "    new_rand_val = str_n('new')\n",
    "    args = {\n",
    "        'chains': {\n",
    "            'pipe': [{\n",
    "                'id': rand_id\n",
    "            }, {\n",
    "               rand_key : new_rand_val\n",
    "            }]\n",
    "        }\n",
    "    }\n",
    "\n",
    "    return pipe_list, rand_id, rand_key, args, new_rand_val\n",
    "\n",
    "\n",
    "def test_action_over_list_f():\n",
    "\n",
    "    pipe_list, rand_id, rand_key, args, new_rand_val = gen_list_keys_for_tests()\n",
    "\n",
    "    assert all(\n",
    "        new_rand_val not in pipe_elem.values() for pipe_elem in pipe_list\n",
    "    )\n",
    "\n",
    "    action_over_list_f(pipe_list, args['chains']['pipe'])\n",
    "\n",
    "    assert any(\n",
    "        rand_key in pipe_elem.keys() and\n",
    "        new_rand_val in pipe_elem.values() for pipe_elem in pipe_list\n",
    "    )\n",
    "\n",
    "\n",
    "def test_replacement_f_list():\n",
    "\n",
    "    pipe_list, rand_id, rand_key, args, new_rand_val = gen_list_keys_for_tests()\n",
    "\n",
    "    mod_conf = {'chains': {'pipe': pipe_list}}\n",
    "\n",
    "    assert all(\n",
    "        new_rand_val not in pipe_elem.values()\n",
    "        for pipe_elem in mod_conf['chains']['pipe']\n",
    "    )\n",
    "\n",
    "    replacement_f(model_config=mod_conf, **args)\n",
    "    assert any(\n",
    "        rand_key in pipe_elem.keys() and\n",
    "        new_rand_val in pipe_elem.values()\n",
    "        for pipe_elem in mod_conf['chains']['pipe']\n",
    "    )\n",
    "\n",
    "\n",
    "def test_replacement_f_val():\n",
    "    args = {'key3': 'newvalue'}\n",
    "    mod_conf = {'key1': 'val1', 'key2': 'val2', 'key3': 'val3'}\n",
    "    replacement_f(model_config=mod_conf, **args)\n",
    "    assert all(\n",
    "        arg_k in mod_conf.keys() and arg_v in mod_conf.values()\n",
    "        for arg_k, arg_v in args.items()\n",
    "    )\n",
    "\n",
    "\n",
    "def test_replacement_f_dict():\n",
    "    args = {'1_key_3': {'2_key_2': 'newvalue'}}\n",
    "    mod_conf = {'1_key_3': {'2_key_2': 'oldvalue'}, '0_key_': '0_val'}\n",
    "    replacement_f(model_config=mod_conf, **args)\n",
    "    assert mod_conf['1_key_3']['2_key_2'] == 'newvalue'\n",
    "\n",
    "\n",
    "test_action_over_list_f()\n",
    "test_replacement_f_list()\n",
    "test_replacement_f_val()\n",
    "test_replacement_f_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def updates_faq_config_file(\n",
    "    configs_path,\n",
    "    **args\n",
    "):\n",
    "    '''Updates deepplavov json config file \n",
    "    '''\n",
    "    #set FAQ data in config file\n",
    "    model_config = json.load(open(configs_path))\n",
    "\n",
    "    if 'data_url' in model_config['dataset_reader']:\n",
    "        del model_config['dataset_reader']['data_url']\n",
    "\n",
    "    replacement_f(model_config=model_config,**args)\n",
    "\n",
    "    json.dump(model_config, open(configs_path, 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test updates_faq_config_file\n",
    "import tempfile\n",
    "from shutil import copyfile\n",
    "\n",
    "\n",
    "def gen_list_keys_for_tests():\n",
    "\n",
    "    str_n = lambda x: f'{x}_{randint(1,10):1}'\n",
    "    gen_dict_list = lambda: {\n",
    "        'id': str_n('id'),\n",
    "        'key1': str_n('v1'),\n",
    "        'key2': str_n('v2'),\n",
    "        'key3': str_n('v3')\n",
    "    }\n",
    "\n",
    "    pipe_list = [gen_dict_list() for _ in range(randint(3, 10))]\n",
    "\n",
    "    rand_id = pipe_list[randint(0, len(pipe_list) - 1)]['id']\n",
    "    rand_key =  f'key{randint(1, 3)}' \n",
    "\n",
    "    new_rand_val = str_n('new')\n",
    "    pipe_dict = {'pipe': [{'id': rand_id}, {rand_key: new_rand_val}]}\n",
    "    args = {'chainer': pipe_dict}\n",
    "\n",
    "    return pipe_list, rand_id, rand_key, args, new_rand_val\n",
    "\n",
    "\n",
    "def test_updates_faq_config_file_update_string():\n",
    "\n",
    "    with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "\n",
    "        tmp_config_file = path.join(tmpdirname, 'tmp_file.json')\n",
    "\n",
    "        copyfile(configs.faq.tfidf_logreg_en_faq, tmp_config_file)\n",
    "\n",
    "        assert path.isfile(tmp_config_file)\n",
    "\n",
    "        updates_faq_config_file(\n",
    "            configs_path=tmp_config_file,\n",
    "            dataset_reader={'data_path': 'fictional_csv_file.csv'}\n",
    "        )\n",
    "\n",
    "        config_json = json.load(open(tmp_config_file))\n",
    "        assert 'data_path' in config_json['dataset_reader']\n",
    "\n",
    "\n",
    "def test_updates_faq_config_file_update_list():\n",
    "\n",
    "    with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "\n",
    "        tmp_config_file = path.join(tmpdirname, 'tmp_file.json')\n",
    "\n",
    "        pipe_list, rand_id, rand_key, args, new_rand_val = gen_list_keys_for_tests(\n",
    "        )\n",
    "        mod_conf = {\n",
    "            'chainer': {\n",
    "                'pipe': pipe_list\n",
    "            },\n",
    "            'dataset_reader': 'dataset_reader_dictionary'\n",
    "        }\n",
    "\n",
    "        json.dump(mod_conf, open(tmp_config_file, 'w'))\n",
    "\n",
    "        assert path.isfile(tmp_config_file)\n",
    "\n",
    "        updates_faq_config_file(configs_path=tmp_config_file, **args)\n",
    "\n",
    "        config_json = json.load(open(tmp_config_file))\n",
    "   \n",
    "        assert any(\n",
    "            rand_key in pipe_elem.keys() and new_rand_val in pipe_elem.values()\n",
    "            for pipe_elem in config_json['chainer']['pipe']\n",
    "        )\n",
    "\n",
    "\n",
    "test_updates_faq_config_file_update_string()\n",
    "test_updates_faq_config_file_update_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def select_faq_responses(faq_model, question):\n",
    "    '''Calls Deeppavlov FAQ model\n",
    "    '''\n",
    "    return faq_model([question])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "09:29:11 DEBUG:Starting new HTTP connection (1): files.deeppavlov.ai:80\n",
      "09:29:12 DEBUG:http://files.deeppavlov.ai:80 \"GET /faq/mipt/en_mipt_faq_v4.tar.gz.md5 HTTP/1.1\" 200 189\n",
      "09:29:12 DEBUG:Starting new HTTP connection (1): files.deeppavlov.ai:80\n",
      "09:29:12 DEBUG:http://files.deeppavlov.ai:80 \"GET /faq/mipt/en_mipt_faq_v4.tar.gz HTTP/1.1\" 200 12276\n",
      "2020-06-15 21:29:12.832 INFO in 'deeppavlov.core.data.utils'['utils'] at line 80: Downloading from http://files.deeppavlov.ai/faq/mipt/en_mipt_faq_v4.tar.gz to /home/jovyan/.deeppavlov/models/faq/en_mipt_faq_v4.tar.gz\n",
      "09:29:12 INFO:Downloading from http://files.deeppavlov.ai/faq/mipt/en_mipt_faq_v4.tar.gz to /home/jovyan/.deeppavlov/models/faq/en_mipt_faq_v4.tar.gz\n",
      "100%|██████████| 12.3k/12.3k [00:00<00:00, 20.8MB/s]\n",
      "2020-06-15 21:29:12.846 INFO in 'deeppavlov.core.data.utils'['utils'] at line 242: Extracting /home/jovyan/.deeppavlov/models/faq/en_mipt_faq_v4.tar.gz archive into /home/jovyan/.deeppavlov/models/faq/mipt\n",
      "09:29:12 INFO:Extracting /home/jovyan/.deeppavlov/models/faq/en_mipt_faq_v4.tar.gz archive into /home/jovyan/.deeppavlov/models/faq/mipt\n",
      "[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package perluniprops to\n",
      "[nltk_data]     /home/jovyan/nltk_data...\n",
      "[nltk_data]   Unzipping misc/perluniprops.zip.\n",
      "[nltk_data] Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]     /home/jovyan/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/nonbreaking_prefixes.zip.\n",
      "2020-06-15 21:29:19.275 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 203: Loading model sklearn.feature_extraction.text:TfidfVectorizer from /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/tfidf.pkl\n",
      "09:29:19 INFO:Loading model sklearn.feature_extraction.text:TfidfVectorizer from /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/tfidf.pkl\n",
      "2020-06-15 21:29:19.277 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 210: Model sklearn.feature_extraction.textTfidfVectorizer loaded  with parameters\n",
      "09:29:19 INFO:Model sklearn.feature_extraction.textTfidfVectorizer loaded  with parameters\n",
      "2020-06-15 21:29:19.278 WARNING in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 216: Fitting of loaded model can not be continued. Model can be fitted from scratch.If one needs to continue fitting, please, look at `warm_start` parameter\n",
      "09:29:19 WARNING:Fitting of loaded model can not be continued. Model can be fitted from scratch.If one needs to continue fitting, please, look at `warm_start` parameter\n",
      "2020-06-15 21:29:19.284 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 109: Fitting model sklearn.feature_extraction.textTfidfVectorizer\n",
      "09:29:19 INFO:Fitting model sklearn.feature_extraction.textTfidfVectorizer\n",
      "2020-06-15 21:29:19.288 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 241: Saving model to /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/tfidf.pkl\n",
      "09:29:19 INFO:Saving model to /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/tfidf.pkl\n",
      "2020-06-15 21:29:19.290 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 115: [loading vocabulary from /tmp/tmpxf7dago4/temp_vocab_answers.dict]\n",
      "09:29:19 INFO:[loading vocabulary from /tmp/tmpxf7dago4/temp_vocab_answers.dict]\n",
      "2020-06-15 21:29:19.292 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 101: [saving vocabulary to /tmp/tmpxf7dago4/temp_vocab_answers.dict]\n",
      "09:29:19 INFO:[saving vocabulary to /tmp/tmpxf7dago4/temp_vocab_answers.dict]\n",
      "2020-06-15 21:29:19.293 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 203: Loading model sklearn.linear_model:LogisticRegression from /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/logreg.pkl\n",
      "09:29:19 INFO:Loading model sklearn.linear_model:LogisticRegression from /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/logreg.pkl\n",
      "2020-06-15 21:29:19.295 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 210: Model sklearn.linear_model.logisticLogisticRegression loaded  with parameters\n",
      "09:29:19 INFO:Model sklearn.linear_model.logisticLogisticRegression loaded  with parameters\n",
      "2020-06-15 21:29:19.296 WARNING in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 216: Fitting of loaded model can not be continued. Model can be fitted from scratch.If one needs to continue fitting, please, look at `warm_start` parameter\n",
      "09:29:19 WARNING:Fitting of loaded model can not be continued. Model can be fitted from scratch.If one needs to continue fitting, please, look at `warm_start` parameter\n",
      "2020-06-15 21:29:19.301 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 109: Fitting model sklearn.linear_model.logisticLogisticRegression\n",
      "09:29:19 INFO:Fitting model sklearn.linear_model.logisticLogisticRegression\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "09:29:19 DEBUG:Starting new HTTP connection (1): files.deeppavlov.ai:80\n",
      "09:29:19 DEBUG:http://files.deeppavlov.ai:80 \"GET /faq/mipt/en_mipt_faq_v4.tar.gz.md5 HTTP/1.1\" 200 189\n",
      "09:29:19 DEBUG:Starting new HTTP connection (1): files.deeppavlov.ai:80\n",
      "09:29:20 DEBUG:http://files.deeppavlov.ai:80 \"GET /faq/mipt/en_mipt_faq_v4.tar.gz HTTP/1.1\" 200 12276\n",
      "2020-06-15 21:29:20.123 INFO in 'deeppavlov.core.data.utils'['utils'] at line 80: Downloading from http://files.deeppavlov.ai/faq/mipt/en_mipt_faq_v4.tar.gz to /home/jovyan/.deeppavlov/models/faq/en_mipt_faq_v4.tar.gz\n",
      "09:29:20 INFO:Downloading from http://files.deeppavlov.ai/faq/mipt/en_mipt_faq_v4.tar.gz to /home/jovyan/.deeppavlov/models/faq/en_mipt_faq_v4.tar.gz\n",
      "100%|██████████| 12.3k/12.3k [00:00<00:00, 8.89MB/s]\n",
      "2020-06-15 21:29:20.131 INFO in 'deeppavlov.core.data.utils'['utils'] at line 242: Extracting /home/jovyan/.deeppavlov/models/faq/en_mipt_faq_v4.tar.gz archive into /home/jovyan/.deeppavlov/models/faq/mipt\n",
      "09:29:20 INFO:Extracting /home/jovyan/.deeppavlov/models/faq/en_mipt_faq_v4.tar.gz archive into /home/jovyan/.deeppavlov/models/faq/mipt\n",
      "2020-06-15 21:29:20.794 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 203: Loading model sklearn.feature_extraction.text:TfidfVectorizer from /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/tfidf.pkl\n",
      "09:29:20 INFO:Loading model sklearn.feature_extraction.text:TfidfVectorizer from /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/tfidf.pkl\n",
      "2020-06-15 21:29:20.795 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 210: Model sklearn.feature_extraction.textTfidfVectorizer loaded  with parameters\n",
      "09:29:20 INFO:Model sklearn.feature_extraction.textTfidfVectorizer loaded  with parameters\n",
      "2020-06-15 21:29:20.796 WARNING in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 216: Fitting of loaded model can not be continued. Model can be fitted from scratch.If one needs to continue fitting, please, look at `warm_start` parameter\n",
      "09:29:20 WARNING:Fitting of loaded model can not be continued. Model can be fitted from scratch.If one needs to continue fitting, please, look at `warm_start` parameter\n",
      "2020-06-15 21:29:20.801 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 109: Fitting model sklearn.feature_extraction.textTfidfVectorizer\n",
      "09:29:20 INFO:Fitting model sklearn.feature_extraction.textTfidfVectorizer\n",
      "2020-06-15 21:29:20.803 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 241: Saving model to /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/tfidf.pkl\n",
      "09:29:20 INFO:Saving model to /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/tfidf.pkl\n",
      "2020-06-15 21:29:20.805 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 115: [loading vocabulary from /tmp/tmpoavoi8r9/temp_vocab_answers.dict]\n",
      "09:29:20 INFO:[loading vocabulary from /tmp/tmpoavoi8r9/temp_vocab_answers.dict]\n",
      "2020-06-15 21:29:20.806 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 101: [saving vocabulary to /tmp/tmpoavoi8r9/temp_vocab_answers.dict]\n",
      "09:29:20 INFO:[saving vocabulary to /tmp/tmpoavoi8r9/temp_vocab_answers.dict]\n",
      "2020-06-15 21:29:20.808 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 203: Loading model sklearn.linear_model:LogisticRegression from /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/logreg.pkl\n",
      "09:29:20 INFO:Loading model sklearn.linear_model:LogisticRegression from /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/logreg.pkl\n",
      "2020-06-15 21:29:20.809 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 210: Model sklearn.linear_model.logisticLogisticRegression loaded  with parameters\n",
      "09:29:20 INFO:Model sklearn.linear_model.logisticLogisticRegression loaded  with parameters\n",
      "2020-06-15 21:29:20.810 WARNING in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 216: Fitting of loaded model can not be continued. Model can be fitted from scratch.If one needs to continue fitting, please, look at `warm_start` parameter\n",
      "09:29:20 WARNING:Fitting of loaded model can not be continued. Model can be fitted from scratch.If one needs to continue fitting, please, look at `warm_start` parameter\n",
      "2020-06-15 21:29:20.815 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 109: Fitting model sklearn.linear_model.logisticLogisticRegression\n",
      "09:29:20 INFO:Fitting model sklearn.linear_model.logisticLogisticRegression\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "2020-06-15 21:29:20.817 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 241: Saving model to /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/logreg.pkl\n",
      "09:29:20 INFO:Saving model to /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/logreg.pkl\n",
      "2020-06-15 21:29:21.718 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 203: Loading model sklearn.feature_extraction.text:TfidfVectorizer from /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/tfidf.pkl\n",
      "09:29:21 INFO:Loading model sklearn.feature_extraction.text:TfidfVectorizer from /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/tfidf.pkl\n",
      "2020-06-15 21:29:21.719 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 210: Model sklearn.feature_extraction.textTfidfVectorizer loaded  with parameters\n",
      "09:29:21 INFO:Model sklearn.feature_extraction.textTfidfVectorizer loaded  with parameters\n",
      "2020-06-15 21:29:21.720 WARNING in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 216: Fitting of loaded model can not be continued. Model can be fitted from scratch.If one needs to continue fitting, please, look at `warm_start` parameter\n",
      "09:29:21 WARNING:Fitting of loaded model can not be continued. Model can be fitted from scratch.If one needs to continue fitting, please, look at `warm_start` parameter\n",
      "2020-06-15 21:29:21.721 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 115: [loading vocabulary from /tmp/tmpoavoi8r9/temp_vocab_answers.dict]\n",
      "09:29:21 INFO:[loading vocabulary from /tmp/tmpoavoi8r9/temp_vocab_answers.dict]\n",
      "2020-06-15 21:29:21.723 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 203: Loading model sklearn.linear_model:LogisticRegression from /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/logreg.pkl\n",
      "09:29:21 INFO:Loading model sklearn.linear_model:LogisticRegression from /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/logreg.pkl\n",
      "2020-06-15 21:29:21.724 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 210: Model sklearn.linear_model.logisticLogisticRegression loaded  with parameters\n",
      "09:29:21 INFO:Model sklearn.linear_model.logisticLogisticRegression loaded  with parameters\n",
      "2020-06-15 21:29:21.725 WARNING in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 216: Fitting of loaded model can not be continued. Model can be fitted from scratch.If one needs to continue fitting, please, look at `warm_start` parameter\n",
      "09:29:21 WARNING:Fitting of loaded model can not be continued. Model can be fitted from scratch.If one needs to continue fitting, please, look at `warm_start` parameter\n",
      "2020-06-15 21:29:22.526 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 203: Loading model sklearn.feature_extraction.text:TfidfVectorizer from /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/tfidf.pkl\n",
      "09:29:22 INFO:Loading model sklearn.feature_extraction.text:TfidfVectorizer from /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/tfidf.pkl\n",
      "2020-06-15 21:29:22.527 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 210: Model sklearn.feature_extraction.textTfidfVectorizer loaded  with parameters\n",
      "09:29:22 INFO:Model sklearn.feature_extraction.textTfidfVectorizer loaded  with parameters\n",
      "2020-06-15 21:29:22.528 WARNING in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 216: Fitting of loaded model can not be continued. Model can be fitted from scratch.If one needs to continue fitting, please, look at `warm_start` parameter\n",
      "09:29:22 WARNING:Fitting of loaded model can not be continued. Model can be fitted from scratch.If one needs to continue fitting, please, look at `warm_start` parameter\n",
      "2020-06-15 21:29:22.530 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 115: [loading vocabulary from /tmp/tmpoavoi8r9/temp_vocab_answers.dict]\n",
      "09:29:22 INFO:[loading vocabulary from /tmp/tmpoavoi8r9/temp_vocab_answers.dict]\n",
      "2020-06-15 21:29:22.532 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 203: Loading model sklearn.linear_model:LogisticRegression from /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/logreg.pkl\n",
      "09:29:22 INFO:Loading model sklearn.linear_model:LogisticRegression from /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/logreg.pkl\n",
      "2020-06-15 21:29:22.534 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 210: Model sklearn.linear_model.logisticLogisticRegression loaded  with parameters\n",
      "09:29:22 INFO:Model sklearn.linear_model.logisticLogisticRegression loaded  with parameters\n",
      "2020-06-15 21:29:22.535 WARNING in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 216: Fitting of loaded model can not be continued. Model can be fitted from scratch.If one needs to continue fitting, please, look at `warm_start` parameter\n",
      "09:29:22 WARNING:Fitting of loaded model can not be continued. Model can be fitted from scratch.If one needs to continue fitting, please, look at `warm_start` parameter\n",
      "09:29:22 DEBUG:Starting new HTTP connection (1): files.deeppavlov.ai:80\n",
      "09:29:23 DEBUG:http://files.deeppavlov.ai:80 \"GET /faq/mipt/en_mipt_faq_v4.tar.gz.md5 HTTP/1.1\" 200 189\n",
      "09:29:23 DEBUG:Starting new HTTP connection (1): files.deeppavlov.ai:80\n",
      "09:29:24 DEBUG:http://files.deeppavlov.ai:80 \"GET /faq/mipt/en_mipt_faq_v4.tar.gz HTTP/1.1\" 200 12276\n",
      "2020-06-15 21:29:24.238 INFO in 'deeppavlov.core.data.utils'['utils'] at line 80: Downloading from http://files.deeppavlov.ai/faq/mipt/en_mipt_faq_v4.tar.gz to /home/jovyan/.deeppavlov/models/faq/en_mipt_faq_v4.tar.gz\n",
      "09:29:24 INFO:Downloading from http://files.deeppavlov.ai/faq/mipt/en_mipt_faq_v4.tar.gz to /home/jovyan/.deeppavlov/models/faq/en_mipt_faq_v4.tar.gz\n",
      "100%|██████████| 12.3k/12.3k [00:00<00:00, 32.4MB/s]\n",
      "2020-06-15 21:29:24.242 INFO in 'deeppavlov.core.data.utils'['utils'] at line 242: Extracting /home/jovyan/.deeppavlov/models/faq/en_mipt_faq_v4.tar.gz archive into /home/jovyan/.deeppavlov/models/faq/mipt\n",
      "09:29:24 INFO:Extracting /home/jovyan/.deeppavlov/models/faq/en_mipt_faq_v4.tar.gz archive into /home/jovyan/.deeppavlov/models/faq/mipt\n",
      "2020-06-15 21:29:25.23 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 203: Loading model sklearn.feature_extraction.text:TfidfVectorizer from /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/tfidf.pkl\n",
      "09:29:25 INFO:Loading model sklearn.feature_extraction.text:TfidfVectorizer from /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/tfidf.pkl\n",
      "2020-06-15 21:29:25.24 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 210: Model sklearn.feature_extraction.textTfidfVectorizer loaded  with parameters\n",
      "09:29:25 INFO:Model sklearn.feature_extraction.textTfidfVectorizer loaded  with parameters\n",
      "2020-06-15 21:29:25.25 WARNING in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 216: Fitting of loaded model can not be continued. Model can be fitted from scratch.If one needs to continue fitting, please, look at `warm_start` parameter\n",
      "09:29:25 WARNING:Fitting of loaded model can not be continued. Model can be fitted from scratch.If one needs to continue fitting, please, look at `warm_start` parameter\n",
      "2020-06-15 21:29:25.31 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 109: Fitting model sklearn.feature_extraction.textTfidfVectorizer\n",
      "09:29:25 INFO:Fitting model sklearn.feature_extraction.textTfidfVectorizer\n",
      "2020-06-15 21:29:25.33 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 241: Saving model to /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/tfidf.pkl\n",
      "09:29:25 INFO:Saving model to /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/tfidf.pkl\n",
      "2020-06-15 21:29:25.35 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 115: [loading vocabulary from /tmp/tmpkg0ytxyu/temp_vocab_answers.dict]\n",
      "09:29:25 INFO:[loading vocabulary from /tmp/tmpkg0ytxyu/temp_vocab_answers.dict]\n",
      "2020-06-15 21:29:25.37 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 101: [saving vocabulary to /tmp/tmpkg0ytxyu/temp_vocab_answers.dict]\n",
      "09:29:25 INFO:[saving vocabulary to /tmp/tmpkg0ytxyu/temp_vocab_answers.dict]\n",
      "2020-06-15 21:29:25.39 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 203: Loading model sklearn.linear_model:LogisticRegression from /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/logreg.pkl\n",
      "09:29:25 INFO:Loading model sklearn.linear_model:LogisticRegression from /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/logreg.pkl\n",
      "2020-06-15 21:29:25.40 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 210: Model sklearn.linear_model.logisticLogisticRegression loaded  with parameters\n",
      "09:29:25 INFO:Model sklearn.linear_model.logisticLogisticRegression loaded  with parameters\n",
      "2020-06-15 21:29:25.41 WARNING in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 216: Fitting of loaded model can not be continued. Model can be fitted from scratch.If one needs to continue fitting, please, look at `warm_start` parameter\n",
      "09:29:25 WARNING:Fitting of loaded model can not be continued. Model can be fitted from scratch.If one needs to continue fitting, please, look at `warm_start` parameter\n",
      "2020-06-15 21:29:25.49 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 109: Fitting model sklearn.linear_model.logisticLogisticRegression\n",
      "09:29:25 INFO:Fitting model sklearn.linear_model.logisticLogisticRegression\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "2020-06-15 21:29:25.51 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 241: Saving model to /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/logreg.pkl\n",
      "09:29:25 INFO:Saving model to /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/logreg.pkl\n",
      "2020-06-15 21:29:25.721 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 203: Loading model sklearn.feature_extraction.text:TfidfVectorizer from /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/tfidf.pkl\n",
      "09:29:25 INFO:Loading model sklearn.feature_extraction.text:TfidfVectorizer from /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/tfidf.pkl\n",
      "2020-06-15 21:29:25.722 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 210: Model sklearn.feature_extraction.textTfidfVectorizer loaded  with parameters\n",
      "09:29:25 INFO:Model sklearn.feature_extraction.textTfidfVectorizer loaded  with parameters\n",
      "2020-06-15 21:29:25.723 WARNING in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 216: Fitting of loaded model can not be continued. Model can be fitted from scratch.If one needs to continue fitting, please, look at `warm_start` parameter\n",
      "09:29:25 WARNING:Fitting of loaded model can not be continued. Model can be fitted from scratch.If one needs to continue fitting, please, look at `warm_start` parameter\n",
      "2020-06-15 21:29:25.724 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 115: [loading vocabulary from /tmp/tmpkg0ytxyu/temp_vocab_answers.dict]\n",
      "09:29:25 INFO:[loading vocabulary from /tmp/tmpkg0ytxyu/temp_vocab_answers.dict]\n",
      "2020-06-15 21:29:25.726 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 203: Loading model sklearn.linear_model:LogisticRegression from /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/logreg.pkl\n",
      "09:29:25 INFO:Loading model sklearn.linear_model:LogisticRegression from /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/logreg.pkl\n",
      "2020-06-15 21:29:25.727 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 210: Model sklearn.linear_model.logisticLogisticRegression loaded  with parameters\n",
      "09:29:25 INFO:Model sklearn.linear_model.logisticLogisticRegression loaded  with parameters\n",
      "2020-06-15 21:29:25.727 WARNING in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 216: Fitting of loaded model can not be continued. Model can be fitted from scratch.If one needs to continue fitting, please, look at `warm_start` parameter\n",
      "09:29:25 WARNING:Fitting of loaded model can not be continued. Model can be fitted from scratch.If one needs to continue fitting, please, look at `warm_start` parameter\n",
      "2020-06-15 21:29:26.444 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 203: Loading model sklearn.feature_extraction.text:TfidfVectorizer from /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/tfidf.pkl\n",
      "09:29:26 INFO:Loading model sklearn.feature_extraction.text:TfidfVectorizer from /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/tfidf.pkl\n",
      "2020-06-15 21:29:26.445 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 210: Model sklearn.feature_extraction.textTfidfVectorizer loaded  with parameters\n",
      "09:29:26 INFO:Model sklearn.feature_extraction.textTfidfVectorizer loaded  with parameters\n",
      "2020-06-15 21:29:26.446 WARNING in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 216: Fitting of loaded model can not be continued. Model can be fitted from scratch.If one needs to continue fitting, please, look at `warm_start` parameter\n",
      "09:29:26 WARNING:Fitting of loaded model can not be continued. Model can be fitted from scratch.If one needs to continue fitting, please, look at `warm_start` parameter\n",
      "2020-06-15 21:29:26.448 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 115: [loading vocabulary from /tmp/tmpkg0ytxyu/temp_vocab_answers.dict]\n",
      "09:29:26 INFO:[loading vocabulary from /tmp/tmpkg0ytxyu/temp_vocab_answers.dict]\n",
      "2020-06-15 21:29:26.449 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 203: Loading model sklearn.linear_model:LogisticRegression from /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/logreg.pkl\n",
      "09:29:26 INFO:Loading model sklearn.linear_model:LogisticRegression from /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/logreg.pkl\n",
      "2020-06-15 21:29:26.450 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 210: Model sklearn.linear_model.logisticLogisticRegression loaded  with parameters\n",
      "09:29:26 INFO:Model sklearn.linear_model.logisticLogisticRegression loaded  with parameters\n",
      "2020-06-15 21:29:26.451 WARNING in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 216: Fitting of loaded model can not be continued. Model can be fitted from scratch.If one needs to continue fitting, please, look at `warm_start` parameter\n",
      "09:29:26 WARNING:Fitting of loaded model can not be continued. Model can be fitted from scratch.If one needs to continue fitting, please, look at `warm_start` parameter\n"
     ]
    }
   ],
   "source": [
    "#test faq responses\n",
    "import tempfile\n",
    "from shutil import copyfile\n",
    "\n",
    "\n",
    "def gen_mock_csv_file(tmpdirname, faqs):\n",
    "\n",
    "    temp_faq_csv = path.join(tmpdirname, 'tmp_faq.csv')\n",
    "\n",
    "    pd.DataFrame(faqs).to_csv(temp_faq_csv, index=False)\n",
    "\n",
    "    return temp_faq_csv\n",
    "\n",
    "\n",
    "def gen_mock_vocab_answers(tmpdirname, vocabs):\n",
    "\n",
    "    temp_dict_file = path.join(tmpdirname, 'temp_vocab_answers.dict')\n",
    "    vocabs_text = '\\n'.join(\n",
    "        t + '\\t' + str(f) for t, f in zip(vocabs['text'], vocabs['freq'])\n",
    "    )\n",
    "\n",
    "    f = open(temp_dict_file, 'w')\n",
    "    f.write(vocabs_text)\n",
    "    f.close()\n",
    "\n",
    "    return temp_dict_file\n",
    "\n",
    "\n",
    "def gen_faq_config(tmpdirname, vocab_file, faq_file):\n",
    "\n",
    "    temp_configs_faq = path.join(tmpdirname, 'temp_config_faq.json')\n",
    "    copyfile(configs.faq.tfidf_logreg_en_faq, temp_configs_faq)\n",
    "\n",
    "    changes_dict = {'save_path': vocab_file, 'load_path': vocab_file}\n",
    "    id_dict = {'id': 'answers_vocab'}\n",
    "\n",
    "    updates_faq_config_file(\n",
    "        configs_path=temp_configs_faq,\n",
    "        chainer={'pipe': [id_dict, changes_dict]},\n",
    "        dataset_reader={'data_path': faq_file}\n",
    "    )\n",
    "\n",
    "    return temp_configs_faq\n",
    "\n",
    "\n",
    "def test_faq_response_with_minimum_faqs_in_dataframe_fail_case():\n",
    "    with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "\n",
    "        faqs = {\n",
    "            'Question': ['Is Covid erradicated?'],\n",
    "            'Answer': ['Definitely not!']\n",
    "        }\n",
    "\n",
    "        vocabs = {'text': ['This is a vocab example'], 'freq': [1]}\n",
    "\n",
    "        faq_file = gen_mock_csv_file(tmpdirname, faqs)\n",
    "        vocab_file = gen_mock_vocab_answers(tmpdirname, vocabs)\n",
    "\n",
    "        configs_file = gen_faq_config(tmpdirname, vocab_file, faq_file)\n",
    "\n",
    "        try:\n",
    "            select_faq_responses(\n",
    "                question='Is Enrique the prettiest person in town?',\n",
    "                faq_model=train_model(configs_file, download=True)\n",
    "            )\n",
    "            assert False\n",
    "        except ValueError as e:\n",
    "            assert True\n",
    "\n",
    "\n",
    "def test_faq_response_with_minimum_faqs_in_dataframe_success_case():\n",
    "    with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "\n",
    "        faqs = {\n",
    "            'Question': ['Is Covid erradicated?', 'Who is the current POTUS?'],\n",
    "            'Answer': ['Definitely not!', 'Donald Trump']\n",
    "        }\n",
    "\n",
    "        vocabs = {'text': ['This is a vocab example'], 'freq': [1]}\n",
    "\n",
    "        faq_file = gen_mock_csv_file(tmpdirname, faqs)\n",
    "        vocab_file = gen_mock_vocab_answers(tmpdirname, vocabs)\n",
    "\n",
    "        configs_file = gen_faq_config(tmpdirname, vocab_file, faq_file)\n",
    "\n",
    "        assert select_faq_responses(\n",
    "            question='Is Enrique the prettiest person in town?',\n",
    "            faq_model=train_model(configs_file, download=True)\n",
    "        ) == ['Donald Trump']\n",
    "\n",
    "        \n",
    "        \n",
    "def test_faq_response_with_minimum_answers_vocab_success_case():\n",
    "    with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "\n",
    "        faqs = {\n",
    "            'Question': ['Is Covid erradicated?', 'Who is the current POTUS?'],\n",
    "            'Answer': ['Definitely not!', 'Donald Trump']\n",
    "        }\n",
    "\n",
    "        vocabs = {'text': [], 'freq': []}\n",
    "\n",
    "        faq_file = gen_mock_csv_file(tmpdirname, faqs)\n",
    "        vocab_file = gen_mock_vocab_answers(tmpdirname, vocabs)\n",
    "\n",
    "        configs_file = gen_faq_config(tmpdirname, vocab_file, faq_file)\n",
    "\n",
    "        select_faq_responses(\n",
    "            question='Is Enrique the prettiest person in town?',\n",
    "            faq_model=train_model(configs_file, download=True)\n",
    "        ) == ['Donald Trump']\n",
    "\n",
    "test_faq_response_with_minimum_faqs_in_dataframe_fail_case()\n",
    "test_faq_response_with_minimum_faqs_in_dataframe_success_case()\n",
    "test_faq_response_with_minimum_answers_vocab_success_case()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_squad_responses(\n",
    "    contexts, squad_model, question, best_results=1\n",
    "):\n",
    "    '''Calls Deeppavlov BERT and RNET Context Question Answering\n",
    "    '''\n",
    "    responses = contexts.context.apply(\n",
    "        lambda context: squad_model([context], [question])\n",
    "    ).values\n",
    "    \n",
    "    logging.debug(f'Responses: {responses}')\n",
    "    top_responses = [\n",
    "        r[0][0] for r in sorted(responses, key=lambda x: -1 * x[2][0])\n",
    "        [:best_results]\n",
    "    ]\n",
    "\n",
    "    logging.debug(f'Top Responses: {top_responses}')\n",
    "    return responses, top_responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10:00:02 DEBUG:Starting new HTTP connection (1): files.deeppavlov.ai:80\n",
      "10:00:03 DEBUG:http://files.deeppavlov.ai:80 \"GET /deeppavlov_data/bert/cased_L-12_H-768_A-12.zip.md5 HTTP/1.1\" 200 386\n",
      "2020-06-15 22:00:03.821 INFO in 'deeppavlov.download'['download'] at line 117: Skipped http://files.deeppavlov.ai/deeppavlov_data/bert/cased_L-12_H-768_A-12.zip download because of matching hashes\n",
      "10:00:03 INFO:Skipped http://files.deeppavlov.ai/deeppavlov_data/bert/cased_L-12_H-768_A-12.zip download because of matching hashes\n",
      "10:00:03 DEBUG:Starting new HTTP connection (1): files.deeppavlov.ai:80\n",
      "10:00:04 DEBUG:http://files.deeppavlov.ai:80 \"GET /deeppavlov_data/squad_bert.tar.gz.md5 HTTP/1.1\" 200 184\n",
      "2020-06-15 22:00:04.941 INFO in 'deeppavlov.download'['download'] at line 117: Skipped http://files.deeppavlov.ai/deeppavlov_data/squad_bert.tar.gz download because of matching hashes\n",
      "10:00:04 INFO:Skipped http://files.deeppavlov.ai/deeppavlov_data/squad_bert.tar.gz download because of matching hashes\n",
      "2020-06-15 22:00:21.401 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 51: [loading model from /home/jovyan/.deeppavlov/models/squad_bert/model]\n",
      "10:00:21 INFO:[loading model from /home/jovyan/.deeppavlov/models/squad_bert/model]\n",
      "10:00:21 INFO:Restoring parameters from /home/jovyan/.deeppavlov/models/squad_bert/model\n",
      "10:00:23 DEBUG:Responses: []\n",
      "10:00:23 DEBUG:Top Responses: []\n",
      "10:00:24 DEBUG:Responses: [list([['Elon Musk'], [203], [50257280.0]])]\n",
      "10:00:24 DEBUG:Top Responses: ['Elon Musk']\n",
      "10:00:26 DEBUG:Responses: [list([['TJ'], [44], [6978.86279296875]])\n",
      " list([['north of mexico'], [22], [81567.328125]])]\n",
      "10:00:26 DEBUG:Top Responses: ['north of mexico', 'TJ']\n",
      "10:00:26 DEBUG:Starting new HTTP connection (1): files.deeppavlov.ai:80\n",
      "10:00:26 DEBUG:http://files.deeppavlov.ai:80 \"GET /embeddings/wiki-news-300d-1M.vec.md5 HTTP/1.1\" 200 56\n",
      "10:00:26 DEBUG:Starting new HTTP connection (1): files.deeppavlov.ai:80\n",
      "10:00:26 DEBUG:http://files.deeppavlov.ai:80 \"GET /embeddings/wiki-news-300d-1M.vec HTTP/1.1\" 200 2260102345\n",
      "2020-06-15 22:00:26.888 INFO in 'deeppavlov.core.data.utils'['utils'] at line 80: Downloading from http://files.deeppavlov.ai/embeddings/wiki-news-300d-1M.vec to /home/jovyan/.deeppavlov/downloads/embeddings/wiki-news-300d-1M.vec\n",
      "10:00:26 INFO:Downloading from http://files.deeppavlov.ai/embeddings/wiki-news-300d-1M.vec to /home/jovyan/.deeppavlov/downloads/embeddings/wiki-news-300d-1M.vec\n",
      "2020-06-15 22:00:26.893 WARNING in 'deeppavlov.core.data.utils'['utils'] at line 89: Found a partial download /home/jovyan/.deeppavlov/downloads/embeddings/wiki-news-300d-1M.vec.part\n",
      "10:00:26 WARNING:Found a partial download /home/jovyan/.deeppavlov/downloads/embeddings/wiki-news-300d-1M.vec.part\n",
      " 73%|███████▎  | 1.66G/2.26G [00:00<?, ?B/s]2020-06-15 22:00:26.899 WARNING in 'deeppavlov.core.data.utils'['utils'] at line 93: Download stopped abruptly, trying to resume from 1656553472 to reach 2260102345\n",
      "10:00:26 WARNING:Download stopped abruptly, trying to resume from 1656553472 to reach 2260102345\n",
      "10:00:26 DEBUG:Starting new HTTP connection (1): files.deeppavlov.ai:80\n",
      "10:00:27 DEBUG:http://files.deeppavlov.ai:80 \"GET /embeddings/wiki-news-300d-1M.vec HTTP/1.1\" 206 603548873\n",
      "100%|██████████| 2.26G/2.26G [02:03<00:00, 18.3MB/s]  \n",
      "10:02:30 DEBUG:Starting new HTTP connection (1): files.deeppavlov.ai:80\n",
      "10:02:31 DEBUG:http://files.deeppavlov.ai:80 \"GET /deeppavlov_data/squad_model_1.4_cpu_compatible.tar.gz.md5 HTTP/1.1\" 200 389\n",
      "10:02:31 DEBUG:Starting new HTTP connection (1): files.deeppavlov.ai:80\n",
      "10:02:31 DEBUG:http://files.deeppavlov.ai:80 \"GET /deeppavlov_data/squad_model_1.4_cpu_compatible.tar.gz HTTP/1.1\" 200 222203159\n",
      "2020-06-15 22:02:31.612 INFO in 'deeppavlov.core.data.utils'['utils'] at line 80: Downloading from http://files.deeppavlov.ai/deeppavlov_data/squad_model_1.4_cpu_compatible.tar.gz to /home/jovyan/.deeppavlov/squad_model_1.4_cpu_compatible.tar.gz\n",
      "10:02:31 INFO:Downloading from http://files.deeppavlov.ai/deeppavlov_data/squad_model_1.4_cpu_compatible.tar.gz to /home/jovyan/.deeppavlov/squad_model_1.4_cpu_compatible.tar.gz\n",
      "100%|██████████| 222M/222M [00:48<00:00, 4.58MB/s] \n",
      "2020-06-15 22:03:20.106 INFO in 'deeppavlov.core.data.utils'['utils'] at line 242: Extracting /home/jovyan/.deeppavlov/squad_model_1.4_cpu_compatible.tar.gz archive into /home/jovyan/.deeppavlov/models\n",
      "10:03:20 INFO:Extracting /home/jovyan/.deeppavlov/squad_model_1.4_cpu_compatible.tar.gz archive into /home/jovyan/.deeppavlov/models\n",
      "10:03:24 DEBUG:Starting new HTTP connection (1): files.deeppavlov.ai:80\n",
      "10:03:24 DEBUG:http://files.deeppavlov.ai:80 \"GET /embeddings/wiki-news-300d-1M-char.vec.md5 HTTP/1.1\" 200 61\n",
      "10:03:24 DEBUG:Starting new HTTP connection (1): files.deeppavlov.ai:80\n",
      "10:03:25 DEBUG:http://files.deeppavlov.ai:80 \"GET /embeddings/wiki-news-300d-1M-char.vec HTTP/1.1\" 200 7733456\n",
      "2020-06-15 22:03:25.12 INFO in 'deeppavlov.core.data.utils'['utils'] at line 80: Downloading from http://files.deeppavlov.ai/embeddings/wiki-news-300d-1M-char.vec to /home/jovyan/.deeppavlov/downloads/embeddings/wiki-news-300d-1M-char.vec\n",
      "10:03:25 INFO:Downloading from http://files.deeppavlov.ai/embeddings/wiki-news-300d-1M-char.vec to /home/jovyan/.deeppavlov/downloads/embeddings/wiki-news-300d-1M-char.vec\n",
      "100%|██████████| 7.73M/7.73M [00:02<00:00, 2.77MB/s]\n",
      "2020-06-15 22:03:27.818 INFO in 'deeppavlov.models.preprocessors.squad_preprocessor'['squad_preprocessor'] at line 310: SquadVocabEmbedder: loading saved tokens vocab from /home/jovyan/.deeppavlov/models/squad_model/emb/vocab_embedder.pckl\n",
      "10:03:27 INFO:SquadVocabEmbedder: loading saved tokens vocab from /home/jovyan/.deeppavlov/models/squad_model/emb/vocab_embedder.pckl\n",
      "2020-06-15 22:03:28.64 INFO in 'deeppavlov.models.preprocessors.squad_preprocessor'['squad_preprocessor'] at line 310: SquadVocabEmbedder: loading saved chars vocab from /home/jovyan/.deeppavlov/models/squad_model/emb/char_vocab_embedder.pckl\n",
      "10:03:28 INFO:SquadVocabEmbedder: loading saved chars vocab from /home/jovyan/.deeppavlov/models/squad_model/emb/char_vocab_embedder.pckl\n",
      "2020-06-15 22:03:28.668 INFO in 'deeppavlov.core.layers.tf_layers'['tf_layers'] at line 615: \n",
      "Warning! tf.contrib.cudnn_rnn.CudnnCompatibleGRUCell is used. It is okay for inference mode, but if you train your model with this cell it could NOT be used with tf.contrib.cudnn_rnn.CudnnGRUCell later. \n",
      "10:03:28 INFO:\n",
      "Warning! tf.contrib.cudnn_rnn.CudnnCompatibleGRUCell is used. It is okay for inference mode, but if you train your model with this cell it could NOT be used with tf.contrib.cudnn_rnn.CudnnGRUCell later. \n",
      "10:03:28 WARNING:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/contrib/cudnn_rnn/python/ops/cudnn_rnn_ops.py:122: GRUCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.GRUCell, and will be replaced by that in Tensorflow 2.0.\n",
      "10:03:28 WARNING:From /opt/conda/lib/python3.7/site-packages/deeppavlov/core/layers/tf_layers.py:591: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n",
      "10:03:28 WARNING:From /opt/conda/lib/python3.7/site-packages/deeppavlov/core/layers/tf_layers.py:596: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "10:03:28 WARNING:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/contrib/cudnn_rnn/python/ops/cudnn_rnn_ops.py:133: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.add_weight` method instead.\n",
      "10:03:28 WARNING:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/contrib/cudnn_rnn/python/ops/cudnn_rnn_ops.py:139: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "10:03:28 WARNING:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/contrib/cudnn_rnn/python/ops/cudnn_rnn_ops.py:155: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "10:03:28 WARNING:From /opt/conda/lib/python3.7/site-packages/deeppavlov/core/layers/tf_layers.py:808: calling reverse_sequence (from tensorflow.python.ops.array_ops) with seq_dim is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "seq_dim is deprecated, use seq_axis instead\n",
      "10:03:28 WARNING:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/util/deprecation.py:507: calling reverse_sequence (from tensorflow.python.ops.array_ops) with batch_dim is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "batch_dim is deprecated, use batch_axis instead\n",
      "2020-06-15 22:03:28.792 INFO in 'deeppavlov.core.layers.tf_layers'['tf_layers'] at line 615: \n",
      "Warning! tf.contrib.cudnn_rnn.CudnnCompatibleGRUCell is used. It is okay for inference mode, but if you train your model with this cell it could NOT be used with tf.contrib.cudnn_rnn.CudnnGRUCell later. \n",
      "10:03:28 INFO:\n",
      "Warning! tf.contrib.cudnn_rnn.CudnnCompatibleGRUCell is used. It is okay for inference mode, but if you train your model with this cell it could NOT be used with tf.contrib.cudnn_rnn.CudnnGRUCell later. \n",
      "2020-06-15 22:03:28.917 INFO in 'deeppavlov.core.layers.tf_layers'['tf_layers'] at line 615: \n",
      "Warning! tf.contrib.cudnn_rnn.CudnnCompatibleGRUCell is used. It is okay for inference mode, but if you train your model with this cell it could NOT be used with tf.contrib.cudnn_rnn.CudnnGRUCell later. \n",
      "10:03:28 INFO:\n",
      "Warning! tf.contrib.cudnn_rnn.CudnnCompatibleGRUCell is used. It is okay for inference mode, but if you train your model with this cell it could NOT be used with tf.contrib.cudnn_rnn.CudnnGRUCell later. \n",
      "2020-06-15 22:03:28.999 INFO in 'deeppavlov.core.layers.tf_layers'['tf_layers'] at line 615: \n",
      "Warning! tf.contrib.cudnn_rnn.CudnnCompatibleGRUCell is used. It is okay for inference mode, but if you train your model with this cell it could NOT be used with tf.contrib.cudnn_rnn.CudnnGRUCell later. \n",
      "10:03:28 INFO:\n",
      "Warning! tf.contrib.cudnn_rnn.CudnnCompatibleGRUCell is used. It is okay for inference mode, but if you train your model with this cell it could NOT be used with tf.contrib.cudnn_rnn.CudnnGRUCell later. \n",
      "10:03:29 WARNING:From /opt/conda/lib/python3.7/site-packages/deeppavlov/models/squad/utils.py:101: The name tf.AUTO_REUSE is deprecated. Please use tf.compat.v1.AUTO_REUSE instead.\n",
      "\n",
      "10:03:31 WARNING:From /opt/conda/lib/python3.7/site-packages/deeppavlov/models/squad/utils.py:139: The name tf.get_variable_scope is deprecated. Please use tf.compat.v1.get_variable_scope instead.\n",
      "\n",
      "10:03:38 WARNING:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/ops/clip_ops.py:172: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "2020-06-15 22:03:42.135 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 51: [loading model from /home/jovyan/.deeppavlov/models/squad_model/model]\n",
      "10:03:42 INFO:[loading model from /home/jovyan/.deeppavlov/models/squad_model/model]\n",
      "10:03:42 INFO:Restoring parameters from /home/jovyan/.deeppavlov/models/squad_model/model\n",
      "10:03:43 DEBUG:Responses: []\n",
      "10:03:43 DEBUG:Top Responses: []\n",
      "10:03:49 DEBUG:Responses: [list([['Elon Musk'], [203], [36056324.0]])]\n",
      "10:03:49 DEBUG:Top Responses: ['Elon Musk']\n",
      "10:03:49 DEBUG:Responses: [list([['TJ'], [44], [564.4996948242188]])\n",
      " list([['north of mexico'], [22], [138151.90625]])]\n",
      "10:03:49 DEBUG:Top Responses: ['north of mexico', 'TJ']\n"
     ]
    }
   ],
   "source": [
    "#test select_squad_responses\n",
    "import tempfile\n",
    "from shutil import copyfile\n",
    "\n",
    "empty = {'topic': [], 'context': []}\n",
    "spacex = {\n",
    "    'topic': ['SpaceX'],\n",
    "    'context':\n",
    "        [\n",
    "            '''Space Exploration Technologies Corp., trading as SpaceX, is an American aerospace manufacturer and space transportation\n",
    "services company headquartered in Hawthorne, California. It was founded in 2002 by Elon Musk with the goal of reducing space \n",
    "transportation costs to enable the colonization of Mars. SpaceX has developed several launch vehicles, the Starlink satellite\n",
    "constellation, and the Dragon spacecraft. It is widely considered among the most successful private spaceflight companies.'''\n",
    "        ]\n",
    "}\n",
    "\n",
    "intekglobal = {\n",
    "    'topic': ['Intekglobal', 'InG'],\n",
    "    'context':\n",
    "        [\n",
    "            'Intekglobal has its headquarters located in TJ',\n",
    "            'Intekglobal is in the north of mexico'\n",
    "        ]\n",
    "}\n",
    "\n",
    "\n",
    "def assert_squad_model(\n",
    "    contexts, squad_model, question, expected_responses, **args\n",
    "):\n",
    "    responses, top_responses = select_squad_responses(\n",
    "        contexts=pd.DataFrame(contexts),\n",
    "        squad_model=squad_model,\n",
    "        question=question,\n",
    "        **args\n",
    "    )\n",
    "    assert top_responses == expected_responses\n",
    "\n",
    "\n",
    "def test_squad_bert():\n",
    "\n",
    "    bert = build_model(configs.squad.squad_bert, download=True)\n",
    "\n",
    "    assert_squad_model(\n",
    "        empty,\n",
    "        bert,\n",
    "        'Is an empty response expected?',\n",
    "        expected_responses=[],\n",
    "        best_results=2\n",
    "    )\n",
    "\n",
    "    assert_squad_model(\n",
    "        spacex, bert, 'Who founded SpaceX?', expected_responses=['Elon Musk']\n",
    "    )\n",
    "\n",
    "    assert_squad_model(\n",
    "        intekglobal,\n",
    "        bert,\n",
    "        'Where is Intekglobal located?',\n",
    "        expected_responses=['north of mexico','TJ'],\n",
    "        best_results=2\n",
    "    )\n",
    "\n",
    "\n",
    "def test_squad_rnet():\n",
    "\n",
    "    bert = build_model(configs.squad.squad, download=True)\n",
    "\n",
    "    assert_squad_model(\n",
    "        empty,\n",
    "        bert,\n",
    "        'Is an empty response expected?',\n",
    "        expected_responses=[],\n",
    "        best_results=5\n",
    "    )\n",
    "\n",
    "    assert_squad_model(\n",
    "        spacex, bert, 'Who founded SpaceX?', expected_responses=['Elon Musk']\n",
    "    )\n",
    "\n",
    "    assert_squad_model(\n",
    "        intekglobal,\n",
    "        bert,\n",
    "        'Where is Intekglobal located?',\n",
    "        expected_responses=['north of mexico','TJ'],\n",
    "        best_results=2\n",
    "    )\n",
    "\n",
    "test_squad_bert()\n",
    "test_squad_rnet()\n",
    "del spacex, empty, intekglobal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def load_qa_models(\n",
    "    config_rnet=configs.squad.squad,\n",
    "    config_bert=configs.squad.squad_bert,\n",
    "    config_tfidf=configs.faq.tfidf_logreg_en_faq,\n",
    "    download=True\n",
    "):\n",
    "    qa_models = {\n",
    "        'squad':\n",
    "            {\n",
    "                'rnet': build_model(config_rnet, download=download),\n",
    "                'bert': build_model(config_bert, download=download)\n",
    "            },\n",
    "        'faq': {\n",
    "            'tfidf': train_model(config_tfidf, download=download)\n",
    "        }\n",
    "    }\n",
    "    return qa_models\n",
    "\n",
    "\n",
    "def format_responses(question, responses):\n",
    "    formatted_response = f'{question}:\\n\\n'\n",
    "    for k, res in enumerate(responses):\n",
    "        formatted_response += f'{k}: {res}\\n'\n",
    "    return formatted_response\n",
    "\n",
    "\n",
    "def get_responses(contexts, question, qa_models, nb_squad_results=1):\n",
    "    responses = []\n",
    "    for squad_model in qa_models['squad'].values():\n",
    "        responses.extend(\n",
    "            select_squad_responses(\n",
    "                contexts, squad_model, question, best_results=nb_squad_results\n",
    "            )[1]\n",
    "        )\n",
    "    for faq_model in qa_models['faq'].values():\n",
    "        responses.extend(select_faq_responses(faq_model, question))\n",
    "    return responses, format_responses(\n",
    "        question, set([r for r in responses if r.strip()])\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-06-15 22:04:00.67 INFO in 'deeppavlov.models.preprocessors.squad_preprocessor'['squad_preprocessor'] at line 310: SquadVocabEmbedder: loading saved tokens vocab from /home/jovyan/.deeppavlov/models/squad_model/emb/vocab_embedder.pckl\n",
      "10:04:00 INFO:SquadVocabEmbedder: loading saved tokens vocab from /home/jovyan/.deeppavlov/models/squad_model/emb/vocab_embedder.pckl\n",
      "2020-06-15 22:04:00.304 INFO in 'deeppavlov.models.preprocessors.squad_preprocessor'['squad_preprocessor'] at line 310: SquadVocabEmbedder: loading saved chars vocab from /home/jovyan/.deeppavlov/models/squad_model/emb/char_vocab_embedder.pckl\n",
      "10:04:00 INFO:SquadVocabEmbedder: loading saved chars vocab from /home/jovyan/.deeppavlov/models/squad_model/emb/char_vocab_embedder.pckl\n",
      "2020-06-15 22:04:00.714 INFO in 'deeppavlov.core.layers.tf_layers'['tf_layers'] at line 615: \n",
      "Warning! tf.contrib.cudnn_rnn.CudnnCompatibleGRUCell is used. It is okay for inference mode, but if you train your model with this cell it could NOT be used with tf.contrib.cudnn_rnn.CudnnGRUCell later. \n",
      "10:04:00 INFO:\n",
      "Warning! tf.contrib.cudnn_rnn.CudnnCompatibleGRUCell is used. It is okay for inference mode, but if you train your model with this cell it could NOT be used with tf.contrib.cudnn_rnn.CudnnGRUCell later. \n",
      "2020-06-15 22:04:00.923 INFO in 'deeppavlov.core.layers.tf_layers'['tf_layers'] at line 615: \n",
      "Warning! tf.contrib.cudnn_rnn.CudnnCompatibleGRUCell is used. It is okay for inference mode, but if you train your model with this cell it could NOT be used with tf.contrib.cudnn_rnn.CudnnGRUCell later. \n",
      "10:04:00 INFO:\n",
      "Warning! tf.contrib.cudnn_rnn.CudnnCompatibleGRUCell is used. It is okay for inference mode, but if you train your model with this cell it could NOT be used with tf.contrib.cudnn_rnn.CudnnGRUCell later. \n",
      "2020-06-15 22:04:01.64 INFO in 'deeppavlov.core.layers.tf_layers'['tf_layers'] at line 615: \n",
      "Warning! tf.contrib.cudnn_rnn.CudnnCompatibleGRUCell is used. It is okay for inference mode, but if you train your model with this cell it could NOT be used with tf.contrib.cudnn_rnn.CudnnGRUCell later. \n",
      "10:04:01 INFO:\n",
      "Warning! tf.contrib.cudnn_rnn.CudnnCompatibleGRUCell is used. It is okay for inference mode, but if you train your model with this cell it could NOT be used with tf.contrib.cudnn_rnn.CudnnGRUCell later. \n",
      "2020-06-15 22:04:01.150 INFO in 'deeppavlov.core.layers.tf_layers'['tf_layers'] at line 615: \n",
      "Warning! tf.contrib.cudnn_rnn.CudnnCompatibleGRUCell is used. It is okay for inference mode, but if you train your model with this cell it could NOT be used with tf.contrib.cudnn_rnn.CudnnGRUCell later. \n",
      "10:04:01 INFO:\n",
      "Warning! tf.contrib.cudnn_rnn.CudnnCompatibleGRUCell is used. It is okay for inference mode, but if you train your model with this cell it could NOT be used with tf.contrib.cudnn_rnn.CudnnGRUCell later. \n",
      "2020-06-15 22:04:11.477 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 51: [loading model from /home/jovyan/.deeppavlov/models/squad_model/model]\n",
      "10:04:11 INFO:[loading model from /home/jovyan/.deeppavlov/models/squad_model/model]\n",
      "10:04:11 INFO:Restoring parameters from /home/jovyan/.deeppavlov/models/squad_model/model\n",
      "2020-06-15 22:04:25.923 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 51: [loading model from /home/jovyan/.deeppavlov/models/squad_bert/model]\n",
      "10:04:25 INFO:[loading model from /home/jovyan/.deeppavlov/models/squad_bert/model]\n",
      "10:04:26 INFO:Restoring parameters from /home/jovyan/.deeppavlov/models/squad_bert/model\n",
      "2020-06-15 22:04:28.93 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 203: Loading model sklearn.feature_extraction.text:TfidfVectorizer from /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/tfidf.pkl\n",
      "10:04:28 INFO:Loading model sklearn.feature_extraction.text:TfidfVectorizer from /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/tfidf.pkl\n",
      "2020-06-15 22:04:28.95 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 210: Model sklearn.feature_extraction.textTfidfVectorizer loaded  with parameters\n",
      "10:04:28 INFO:Model sklearn.feature_extraction.textTfidfVectorizer loaded  with parameters\n",
      "2020-06-15 22:04:28.96 WARNING in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 216: Fitting of loaded model can not be continued. Model can be fitted from scratch.If one needs to continue fitting, please, look at `warm_start` parameter\n",
      "10:04:28 WARNING:Fitting of loaded model can not be continued. Model can be fitted from scratch.If one needs to continue fitting, please, look at `warm_start` parameter\n",
      "2020-06-15 22:04:28.104 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 109: Fitting model sklearn.feature_extraction.textTfidfVectorizer\n",
      "10:04:28 INFO:Fitting model sklearn.feature_extraction.textTfidfVectorizer\n",
      "2020-06-15 22:04:28.107 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 241: Saving model to /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/tfidf.pkl\n",
      "10:04:28 INFO:Saving model to /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/tfidf.pkl\n",
      "2020-06-15 22:04:28.109 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 115: [loading vocabulary from /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/en_mipt_answers.dict]\n",
      "10:04:28 INFO:[loading vocabulary from /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/en_mipt_answers.dict]\n",
      "2020-06-15 22:04:28.129 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 101: [saving vocabulary to /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/en_mipt_answers.dict]\n",
      "10:04:28 INFO:[saving vocabulary to /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/en_mipt_answers.dict]\n",
      "2020-06-15 22:04:28.131 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 203: Loading model sklearn.linear_model:LogisticRegression from /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/logreg.pkl\n",
      "10:04:28 INFO:Loading model sklearn.linear_model:LogisticRegression from /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/logreg.pkl\n",
      "2020-06-15 22:04:28.132 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 210: Model sklearn.linear_model.logisticLogisticRegression loaded  with parameters\n",
      "10:04:28 INFO:Model sklearn.linear_model.logisticLogisticRegression loaded  with parameters\n",
      "2020-06-15 22:04:28.133 WARNING in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 216: Fitting of loaded model can not be continued. Model can be fitted from scratch.If one needs to continue fitting, please, look at `warm_start` parameter\n",
      "10:04:28 WARNING:Fitting of loaded model can not be continued. Model can be fitted from scratch.If one needs to continue fitting, please, look at `warm_start` parameter\n",
      "2020-06-15 22:04:28.142 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 109: Fitting model sklearn.linear_model.logisticLogisticRegression\n",
      "10:04:28 INFO:Fitting model sklearn.linear_model.logisticLogisticRegression\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "2020-06-15 22:04:28.144 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 241: Saving model to /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/logreg.pkl\n",
      "10:04:28 INFO:Saving model to /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/logreg.pkl\n",
      "2020-06-15 22:04:29.312 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 203: Loading model sklearn.feature_extraction.text:TfidfVectorizer from /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/tfidf.pkl\n",
      "10:04:29 INFO:Loading model sklearn.feature_extraction.text:TfidfVectorizer from /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/tfidf.pkl\n",
      "2020-06-15 22:04:29.313 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 210: Model sklearn.feature_extraction.textTfidfVectorizer loaded  with parameters\n",
      "10:04:29 INFO:Model sklearn.feature_extraction.textTfidfVectorizer loaded  with parameters\n",
      "2020-06-15 22:04:29.314 WARNING in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 216: Fitting of loaded model can not be continued. Model can be fitted from scratch.If one needs to continue fitting, please, look at `warm_start` parameter\n",
      "10:04:29 WARNING:Fitting of loaded model can not be continued. Model can be fitted from scratch.If one needs to continue fitting, please, look at `warm_start` parameter\n",
      "2020-06-15 22:04:29.317 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 115: [loading vocabulary from /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/en_mipt_answers.dict]\n",
      "10:04:29 INFO:[loading vocabulary from /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/en_mipt_answers.dict]\n",
      "2020-06-15 22:04:29.319 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 203: Loading model sklearn.linear_model:LogisticRegression from /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/logreg.pkl\n",
      "10:04:29 INFO:Loading model sklearn.linear_model:LogisticRegression from /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/logreg.pkl\n",
      "2020-06-15 22:04:29.322 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 210: Model sklearn.linear_model.logisticLogisticRegression loaded  with parameters\n",
      "10:04:29 INFO:Model sklearn.linear_model.logisticLogisticRegression loaded  with parameters\n",
      "2020-06-15 22:04:29.323 WARNING in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 216: Fitting of loaded model can not be continued. Model can be fitted from scratch.If one needs to continue fitting, please, look at `warm_start` parameter\n",
      "10:04:29 WARNING:Fitting of loaded model can not be continued. Model can be fitted from scratch.If one needs to continue fitting, please, look at `warm_start` parameter\n",
      "2020-06-15 22:04:30.143 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 203: Loading model sklearn.feature_extraction.text:TfidfVectorizer from /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/tfidf.pkl\n",
      "10:04:30 INFO:Loading model sklearn.feature_extraction.text:TfidfVectorizer from /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/tfidf.pkl\n",
      "2020-06-15 22:04:30.145 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 210: Model sklearn.feature_extraction.textTfidfVectorizer loaded  with parameters\n",
      "10:04:30 INFO:Model sklearn.feature_extraction.textTfidfVectorizer loaded  with parameters\n",
      "2020-06-15 22:04:30.146 WARNING in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 216: Fitting of loaded model can not be continued. Model can be fitted from scratch.If one needs to continue fitting, please, look at `warm_start` parameter\n",
      "10:04:30 WARNING:Fitting of loaded model can not be continued. Model can be fitted from scratch.If one needs to continue fitting, please, look at `warm_start` parameter\n",
      "2020-06-15 22:04:30.149 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 115: [loading vocabulary from /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/en_mipt_answers.dict]\n",
      "10:04:30 INFO:[loading vocabulary from /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/en_mipt_answers.dict]\n",
      "2020-06-15 22:04:30.151 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 203: Loading model sklearn.linear_model:LogisticRegression from /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/logreg.pkl\n",
      "10:04:30 INFO:Loading model sklearn.linear_model:LogisticRegression from /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/logreg.pkl\n",
      "2020-06-15 22:04:30.152 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 210: Model sklearn.linear_model.logisticLogisticRegression loaded  with parameters\n",
      "10:04:30 INFO:Model sklearn.linear_model.logisticLogisticRegression loaded  with parameters\n",
      "2020-06-15 22:04:30.153 WARNING in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 216: Fitting of loaded model can not be continued. Model can be fitted from scratch.If one needs to continue fitting, please, look at `warm_start` parameter\n",
      "10:04:30 WARNING:Fitting of loaded model can not be continued. Model can be fitted from scratch.If one needs to continue fitting, please, look at `warm_start` parameter\n",
      "10:04:31 DEBUG:Responses: [list([['TJ'], [44], [120.95974731445312]])\n",
      " list([['north of mexico'], [22], [174602.40625]])]\n",
      "10:04:31 DEBUG:Top Responses: ['north of mexico', 'TJ']\n",
      "10:04:33 DEBUG:Responses: [list([['TJ'], [44], [22507.34375]])\n",
      " list([['north of mexico'], [22], [269778.3125]])]\n",
      "10:04:33 DEBUG:Top Responses: ['north of mexico', 'TJ']\n",
      "10:04:33 DEBUG:['north of mexico', 'TJ', 'north of mexico', 'TJ', 'Yes it is!']\n",
      "10:04:33 DEBUG:Where is Intekglobal?:\n",
      "\n",
      "0: north of mexico\n",
      "1: Yes it is!\n",
      "2: TJ\n",
      "\n",
      "2020-06-15 22:04:33.289 INFO in 'deeppavlov.models.preprocessors.squad_preprocessor'['squad_preprocessor'] at line 310: SquadVocabEmbedder: loading saved tokens vocab from /home/jovyan/.deeppavlov/models/squad_model/emb/vocab_embedder.pckl\n",
      "10:04:33 INFO:SquadVocabEmbedder: loading saved tokens vocab from /home/jovyan/.deeppavlov/models/squad_model/emb/vocab_embedder.pckl\n",
      "2020-06-15 22:04:33.557 INFO in 'deeppavlov.models.preprocessors.squad_preprocessor'['squad_preprocessor'] at line 310: SquadVocabEmbedder: loading saved chars vocab from /home/jovyan/.deeppavlov/models/squad_model/emb/char_vocab_embedder.pckl\n",
      "10:04:33 INFO:SquadVocabEmbedder: loading saved chars vocab from /home/jovyan/.deeppavlov/models/squad_model/emb/char_vocab_embedder.pckl\n",
      "2020-06-15 22:04:34.134 INFO in 'deeppavlov.core.layers.tf_layers'['tf_layers'] at line 615: \n",
      "Warning! tf.contrib.cudnn_rnn.CudnnCompatibleGRUCell is used. It is okay for inference mode, but if you train your model with this cell it could NOT be used with tf.contrib.cudnn_rnn.CudnnGRUCell later. \n",
      "10:04:34 INFO:\n",
      "Warning! tf.contrib.cudnn_rnn.CudnnCompatibleGRUCell is used. It is okay for inference mode, but if you train your model with this cell it could NOT be used with tf.contrib.cudnn_rnn.CudnnGRUCell later. \n",
      "2020-06-15 22:04:34.255 INFO in 'deeppavlov.core.layers.tf_layers'['tf_layers'] at line 615: \n",
      "Warning! tf.contrib.cudnn_rnn.CudnnCompatibleGRUCell is used. It is okay for inference mode, but if you train your model with this cell it could NOT be used with tf.contrib.cudnn_rnn.CudnnGRUCell later. \n",
      "10:04:34 INFO:\n",
      "Warning! tf.contrib.cudnn_rnn.CudnnCompatibleGRUCell is used. It is okay for inference mode, but if you train your model with this cell it could NOT be used with tf.contrib.cudnn_rnn.CudnnGRUCell later. \n",
      "2020-06-15 22:04:34.386 INFO in 'deeppavlov.core.layers.tf_layers'['tf_layers'] at line 615: \n",
      "Warning! tf.contrib.cudnn_rnn.CudnnCompatibleGRUCell is used. It is okay for inference mode, but if you train your model with this cell it could NOT be used with tf.contrib.cudnn_rnn.CudnnGRUCell later. \n",
      "10:04:34 INFO:\n",
      "Warning! tf.contrib.cudnn_rnn.CudnnCompatibleGRUCell is used. It is okay for inference mode, but if you train your model with this cell it could NOT be used with tf.contrib.cudnn_rnn.CudnnGRUCell later. \n",
      "2020-06-15 22:04:34.475 INFO in 'deeppavlov.core.layers.tf_layers'['tf_layers'] at line 615: \n",
      "Warning! tf.contrib.cudnn_rnn.CudnnCompatibleGRUCell is used. It is okay for inference mode, but if you train your model with this cell it could NOT be used with tf.contrib.cudnn_rnn.CudnnGRUCell later. \n",
      "10:04:34 INFO:\n",
      "Warning! tf.contrib.cudnn_rnn.CudnnCompatibleGRUCell is used. It is okay for inference mode, but if you train your model with this cell it could NOT be used with tf.contrib.cudnn_rnn.CudnnGRUCell later. \n",
      "2020-06-15 22:04:47.257 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 51: [loading model from /home/jovyan/.deeppavlov/models/squad_model/model]\n",
      "10:04:47 INFO:[loading model from /home/jovyan/.deeppavlov/models/squad_model/model]\n",
      "10:04:47 INFO:Restoring parameters from /home/jovyan/.deeppavlov/models/squad_model/model\n",
      "2020-06-15 22:05:03.51 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 51: [loading model from /home/jovyan/.deeppavlov/models/squad_bert/model]\n",
      "10:05:03 INFO:[loading model from /home/jovyan/.deeppavlov/models/squad_bert/model]\n",
      "10:05:03 INFO:Restoring parameters from /home/jovyan/.deeppavlov/models/squad_bert/model\n",
      "2020-06-15 22:05:05.57 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 203: Loading model sklearn.feature_extraction.text:TfidfVectorizer from /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/tfidf.pkl\n",
      "10:05:05 INFO:Loading model sklearn.feature_extraction.text:TfidfVectorizer from /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/tfidf.pkl\n",
      "2020-06-15 22:05:05.59 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 210: Model sklearn.feature_extraction.textTfidfVectorizer loaded  with parameters\n",
      "10:05:05 INFO:Model sklearn.feature_extraction.textTfidfVectorizer loaded  with parameters\n",
      "2020-06-15 22:05:05.60 WARNING in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 216: Fitting of loaded model can not be continued. Model can be fitted from scratch.If one needs to continue fitting, please, look at `warm_start` parameter\n",
      "10:05:05 WARNING:Fitting of loaded model can not be continued. Model can be fitted from scratch.If one needs to continue fitting, please, look at `warm_start` parameter\n",
      "2020-06-15 22:05:05.65 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 109: Fitting model sklearn.feature_extraction.textTfidfVectorizer\n",
      "10:05:05 INFO:Fitting model sklearn.feature_extraction.textTfidfVectorizer\n",
      "2020-06-15 22:05:05.67 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 241: Saving model to /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/tfidf.pkl\n",
      "10:05:05 INFO:Saving model to /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/tfidf.pkl\n",
      "2020-06-15 22:05:05.69 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 115: [loading vocabulary from /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/en_mipt_answers.dict]\n",
      "10:05:05 INFO:[loading vocabulary from /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/en_mipt_answers.dict]\n",
      "2020-06-15 22:05:05.70 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 101: [saving vocabulary to /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/en_mipt_answers.dict]\n",
      "10:05:05 INFO:[saving vocabulary to /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/en_mipt_answers.dict]\n",
      "2020-06-15 22:05:05.71 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 203: Loading model sklearn.linear_model:LogisticRegression from /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/logreg.pkl\n",
      "10:05:05 INFO:Loading model sklearn.linear_model:LogisticRegression from /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/logreg.pkl\n",
      "2020-06-15 22:05:05.72 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 210: Model sklearn.linear_model.logisticLogisticRegression loaded  with parameters\n",
      "10:05:05 INFO:Model sklearn.linear_model.logisticLogisticRegression loaded  with parameters\n",
      "2020-06-15 22:05:05.73 WARNING in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 216: Fitting of loaded model can not be continued. Model can be fitted from scratch.If one needs to continue fitting, please, look at `warm_start` parameter\n",
      "10:05:05 WARNING:Fitting of loaded model can not be continued. Model can be fitted from scratch.If one needs to continue fitting, please, look at `warm_start` parameter\n",
      "2020-06-15 22:05:05.78 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 109: Fitting model sklearn.linear_model.logisticLogisticRegression\n",
      "10:05:05 INFO:Fitting model sklearn.linear_model.logisticLogisticRegression\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "2020-06-15 22:05:05.80 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 241: Saving model to /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/logreg.pkl\n",
      "10:05:05 INFO:Saving model to /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/logreg.pkl\n",
      "2020-06-15 22:05:05.859 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 203: Loading model sklearn.feature_extraction.text:TfidfVectorizer from /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/tfidf.pkl\n",
      "10:05:05 INFO:Loading model sklearn.feature_extraction.text:TfidfVectorizer from /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/tfidf.pkl\n",
      "2020-06-15 22:05:05.860 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 210: Model sklearn.feature_extraction.textTfidfVectorizer loaded  with parameters\n",
      "10:05:05 INFO:Model sklearn.feature_extraction.textTfidfVectorizer loaded  with parameters\n",
      "2020-06-15 22:05:05.861 WARNING in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 216: Fitting of loaded model can not be continued. Model can be fitted from scratch.If one needs to continue fitting, please, look at `warm_start` parameter\n",
      "10:05:05 WARNING:Fitting of loaded model can not be continued. Model can be fitted from scratch.If one needs to continue fitting, please, look at `warm_start` parameter\n",
      "2020-06-15 22:05:05.863 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 115: [loading vocabulary from /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/en_mipt_answers.dict]\n",
      "10:05:05 INFO:[loading vocabulary from /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/en_mipt_answers.dict]\n",
      "2020-06-15 22:05:05.865 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 203: Loading model sklearn.linear_model:LogisticRegression from /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/logreg.pkl\n",
      "10:05:05 INFO:Loading model sklearn.linear_model:LogisticRegression from /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/logreg.pkl\n",
      "2020-06-15 22:05:05.866 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 210: Model sklearn.linear_model.logisticLogisticRegression loaded  with parameters\n",
      "10:05:05 INFO:Model sklearn.linear_model.logisticLogisticRegression loaded  with parameters\n",
      "2020-06-15 22:05:05.867 WARNING in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 216: Fitting of loaded model can not be continued. Model can be fitted from scratch.If one needs to continue fitting, please, look at `warm_start` parameter\n",
      "10:05:05 WARNING:Fitting of loaded model can not be continued. Model can be fitted from scratch.If one needs to continue fitting, please, look at `warm_start` parameter\n",
      "2020-06-15 22:05:06.658 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 203: Loading model sklearn.feature_extraction.text:TfidfVectorizer from /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/tfidf.pkl\n",
      "10:05:06 INFO:Loading model sklearn.feature_extraction.text:TfidfVectorizer from /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/tfidf.pkl\n",
      "2020-06-15 22:05:06.660 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 210: Model sklearn.feature_extraction.textTfidfVectorizer loaded  with parameters\n",
      "10:05:06 INFO:Model sklearn.feature_extraction.textTfidfVectorizer loaded  with parameters\n",
      "2020-06-15 22:05:06.661 WARNING in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 216: Fitting of loaded model can not be continued. Model can be fitted from scratch.If one needs to continue fitting, please, look at `warm_start` parameter\n",
      "10:05:06 WARNING:Fitting of loaded model can not be continued. Model can be fitted from scratch.If one needs to continue fitting, please, look at `warm_start` parameter\n",
      "2020-06-15 22:05:06.663 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 115: [loading vocabulary from /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/en_mipt_answers.dict]\n",
      "10:05:06 INFO:[loading vocabulary from /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/en_mipt_answers.dict]\n",
      "2020-06-15 22:05:06.664 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 203: Loading model sklearn.linear_model:LogisticRegression from /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/logreg.pkl\n",
      "10:05:06 INFO:Loading model sklearn.linear_model:LogisticRegression from /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/logreg.pkl\n",
      "2020-06-15 22:05:06.665 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 210: Model sklearn.linear_model.logisticLogisticRegression loaded  with parameters\n",
      "10:05:06 INFO:Model sklearn.linear_model.logisticLogisticRegression loaded  with parameters\n",
      "2020-06-15 22:05:06.666 WARNING in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 216: Fitting of loaded model can not be continued. Model can be fitted from scratch.If one needs to continue fitting, please, look at `warm_start` parameter\n",
      "10:05:06 WARNING:Fitting of loaded model can not be continued. Model can be fitted from scratch.If one needs to continue fitting, please, look at `warm_start` parameter\n",
      "10:05:06 DEBUG:Responses: []\n",
      "10:05:06 DEBUG:Top Responses: []\n",
      "10:05:06 DEBUG:Responses: []\n",
      "10:05:06 DEBUG:Top Responses: []\n",
      "10:05:06 DEBUG:['Two']\n",
      "10:05:06 DEBUG:What is the minimun number of FAQ questions:\n",
      "\n",
      "0: Two\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# test get_responses\n",
    "import tempfile\n",
    "from shutil import copyfile\n",
    "\n",
    "intekglobal_context = {\n",
    "    'topic': ['Intekglobal', 'InG'],\n",
    "    'context':\n",
    "        [\n",
    "            'Intekglobal has its headquarters located in TJ',\n",
    "            'Intekglobal is in the north of mexico'\n",
    "        ]\n",
    "}\n",
    "\n",
    "intekglobal_faqs = {\n",
    "    'Question': ['Is Intekglobal an IT company?', 'Where can I apply?'],\n",
    "    'Answer':\n",
    "        ['Yes it is!', 'Please refer the our website for further information']\n",
    "}\n",
    "\n",
    "\n",
    "def mock_faq_files(tmpdirname, faqs):\n",
    "\n",
    "    faq_files = {\n",
    "        'data': path.join(tmpdirname, 'temp_faq.csv'),\n",
    "        'config': path.join(tmpdirname, 'temp_config_faq.json')\n",
    "    }\n",
    "\n",
    "    pd.DataFrame(faqs).to_csv(faq_files['data'], index=False)\n",
    "    copyfile(configs.faq.tfidf_logreg_en_faq, faq_files['config'])\n",
    "\n",
    "    updates_faq_config_file(\n",
    "        configs_path=faq_files['config'],\n",
    "        dataset_reader={'data_path': faq_files['data']}\n",
    "    )\n",
    "\n",
    "    return faq_files\n",
    "\n",
    "\n",
    "def test_get_intekglobal_responses():\n",
    "    with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "\n",
    "        faq_files = mock_faq_files(tmpdirname, intekglobal_faqs)\n",
    "        qa_models = load_qa_models(\n",
    "            config_tfidf=faq_files['config'], download=False\n",
    "        )\n",
    "\n",
    "        responses, format_responses = get_responses(\n",
    "            pd.DataFrame(intekglobal_context),\n",
    "            'Where is Intekglobal?',\n",
    "            qa_models,\n",
    "            nb_squad_results=2\n",
    "        )\n",
    "\n",
    "        logging.debug(responses)\n",
    "        logging.debug(format_responses)\n",
    "        assert all(\n",
    "            response in ('north of mexico', 'TJ', 'Yes it is!')\n",
    "            for response in responses\n",
    "        )\n",
    "        assert ''' Where is Intekglobal?:\n",
    "\n",
    "0: north of mexico\n",
    "1: Yes it is!\n",
    "2: TJ\n",
    "        '''.strip() == format_responses.strip()\n",
    "\n",
    "\n",
    "def test_get_responses_with_empty_context():\n",
    "    with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "        min_faqs = {'Question': ['Minimum number of questions?','This is the other question?'], 'Answer': ['Two','yes']}\n",
    "        faq_files = mock_faq_files(tmpdirname, min_faqs)\n",
    "\n",
    "        qa_models = load_qa_models(\n",
    "            config_tfidf=faq_files['config'], download=False\n",
    "        )\n",
    "        empty_context = {'topic': [], 'context': []}\n",
    "\n",
    "        responses, format_responses = get_responses(\n",
    "            pd.DataFrame(empty_context),\n",
    "            'What is the minimun number of FAQ questions',\n",
    "            qa_models,\n",
    "            nb_squad_results=2\n",
    "        )\n",
    "\n",
    "        logging.debug(responses)\n",
    "        logging.debug(format_responses)\n",
    "        assert responses == ['Two']\n",
    "\n",
    "\n",
    "test_get_intekglobal_responses()\n",
    "test_get_responses_with_empty_context()\n",
    "\n",
    "del intekglobal_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_input(text):\n",
    "    '''This redundancy is needed for testing'''\n",
    "    return input(text)\n",
    "\n",
    "\n",
    "def new_answer(question, data, qa_models):\n",
    "\n",
    "    if get_input('Give a better anwser [y/n]?')[0].lower() != 'y':\n",
    "        return 'no data updates..'\n",
    "\n",
    "    if get_input('Give the answer as a context [y/n]?')[0].lower() == 'y':\n",
    "        new_context = pd.DataFrame(\n",
    "            {\n",
    "                'topic': [get_input('Give context a title:\\n')],\n",
    "                'context': [get_input('Introduce the context:\\n')]\n",
    "            }\n",
    "        )\n",
    "        data['context']['df'] = data['context']['df'].append(new_context)\n",
    "        data['context']['df'].to_csv(data['context']['path'])\n",
    "\n",
    "        return 'contexts dataset updated..'\n",
    "    else:\n",
    "        new_faq = pd.DataFrame(\n",
    "            {\n",
    "                'Question': [question],\n",
    "                'Answer': [get_input('Introduce the answer:\\n')]\n",
    "            }\n",
    "        )\n",
    "        data['faq']['df'] = data['faq']['df'].append(new_faq)\n",
    "        data['faq']['df'].to_csv(data['faq']['path'])\n",
    "        qa_models['faq']['tfidf'] = train_model(\n",
    "            data['faq']['config'], download=False\n",
    "        )\n",
    "        return 'FAQ dataset and model updated..'\n",
    "\n",
    "\n",
    "def question_response(data, qa_models, num_returned_values_per_squad_model=1):\n",
    "    question = get_input('Introduce question:\\n')\n",
    "\n",
    "    _, formatted_responses = get_responses(\n",
    "        data['context']['df'], question, qa_models, nb_squad_results=2\n",
    "    )\n",
    "\n",
    "    return formatted_responses, new_answer(question, data, qa_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Test FAQ dialog system's part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-06-15 23:45:20.87 INFO in 'deeppavlov.models.preprocessors.squad_preprocessor'['squad_preprocessor'] at line 310: SquadVocabEmbedder: loading saved tokens vocab from /home/jovyan/.deeppavlov/models/squad_model/emb/vocab_embedder.pckl\n",
      "11:45:20 INFO:SquadVocabEmbedder: loading saved tokens vocab from /home/jovyan/.deeppavlov/models/squad_model/emb/vocab_embedder.pckl\n",
      "2020-06-15 23:45:23.357 INFO in 'deeppavlov.models.preprocessors.squad_preprocessor'['squad_preprocessor'] at line 310: SquadVocabEmbedder: loading saved chars vocab from /home/jovyan/.deeppavlov/models/squad_model/emb/char_vocab_embedder.pckl\n",
      "11:45:23 INFO:SquadVocabEmbedder: loading saved chars vocab from /home/jovyan/.deeppavlov/models/squad_model/emb/char_vocab_embedder.pckl\n",
      "2020-06-15 23:45:23.751 INFO in 'deeppavlov.core.layers.tf_layers'['tf_layers'] at line 615: \n",
      "Warning! tf.contrib.cudnn_rnn.CudnnCompatibleGRUCell is used. It is okay for inference mode, but if you train your model with this cell it could NOT be used with tf.contrib.cudnn_rnn.CudnnGRUCell later. \n",
      "11:45:23 INFO:\n",
      "Warning! tf.contrib.cudnn_rnn.CudnnCompatibleGRUCell is used. It is okay for inference mode, but if you train your model with this cell it could NOT be used with tf.contrib.cudnn_rnn.CudnnGRUCell later. \n",
      "2020-06-15 23:45:24.1 INFO in 'deeppavlov.core.layers.tf_layers'['tf_layers'] at line 615: \n",
      "Warning! tf.contrib.cudnn_rnn.CudnnCompatibleGRUCell is used. It is okay for inference mode, but if you train your model with this cell it could NOT be used with tf.contrib.cudnn_rnn.CudnnGRUCell later. \n",
      "11:45:24 INFO:\n",
      "Warning! tf.contrib.cudnn_rnn.CudnnCompatibleGRUCell is used. It is okay for inference mode, but if you train your model with this cell it could NOT be used with tf.contrib.cudnn_rnn.CudnnGRUCell later. \n",
      "2020-06-15 23:45:24.156 INFO in 'deeppavlov.core.layers.tf_layers'['tf_layers'] at line 615: \n",
      "Warning! tf.contrib.cudnn_rnn.CudnnCompatibleGRUCell is used. It is okay for inference mode, but if you train your model with this cell it could NOT be used with tf.contrib.cudnn_rnn.CudnnGRUCell later. \n",
      "11:45:24 INFO:\n",
      "Warning! tf.contrib.cudnn_rnn.CudnnCompatibleGRUCell is used. It is okay for inference mode, but if you train your model with this cell it could NOT be used with tf.contrib.cudnn_rnn.CudnnGRUCell later. \n",
      "2020-06-15 23:45:24.239 INFO in 'deeppavlov.core.layers.tf_layers'['tf_layers'] at line 615: \n",
      "Warning! tf.contrib.cudnn_rnn.CudnnCompatibleGRUCell is used. It is okay for inference mode, but if you train your model with this cell it could NOT be used with tf.contrib.cudnn_rnn.CudnnGRUCell later. \n",
      "11:45:24 INFO:\n",
      "Warning! tf.contrib.cudnn_rnn.CudnnCompatibleGRUCell is used. It is okay for inference mode, but if you train your model with this cell it could NOT be used with tf.contrib.cudnn_rnn.CudnnGRUCell later. \n",
      "2020-06-15 23:45:39.779 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 51: [loading model from /home/jovyan/.deeppavlov/models/squad_model/model]\n",
      "11:45:39 INFO:[loading model from /home/jovyan/.deeppavlov/models/squad_model/model]\n",
      "11:45:39 INFO:Restoring parameters from /home/jovyan/.deeppavlov/models/squad_model/model\n",
      "2020-06-15 23:45:56.252 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 51: [loading model from /home/jovyan/.deeppavlov/models/squad_bert/model]\n",
      "11:45:56 INFO:[loading model from /home/jovyan/.deeppavlov/models/squad_bert/model]\n",
      "11:45:56 INFO:Restoring parameters from /home/jovyan/.deeppavlov/models/squad_bert/model\n",
      "2020-06-15 23:45:58.269 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 203: Loading model sklearn.feature_extraction.text:TfidfVectorizer from /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/tfidf.pkl\n",
      "11:45:58 INFO:Loading model sklearn.feature_extraction.text:TfidfVectorizer from /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/tfidf.pkl\n",
      "2020-06-15 23:45:58.270 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 210: Model sklearn.feature_extraction.textTfidfVectorizer loaded  with parameters\n",
      "11:45:58 INFO:Model sklearn.feature_extraction.textTfidfVectorizer loaded  with parameters\n",
      "2020-06-15 23:45:58.271 WARNING in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 216: Fitting of loaded model can not be continued. Model can be fitted from scratch.If one needs to continue fitting, please, look at `warm_start` parameter\n",
      "11:45:58 WARNING:Fitting of loaded model can not be continued. Model can be fitted from scratch.If one needs to continue fitting, please, look at `warm_start` parameter\n",
      "2020-06-15 23:45:58.276 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 109: Fitting model sklearn.feature_extraction.textTfidfVectorizer\n",
      "11:45:58 INFO:Fitting model sklearn.feature_extraction.textTfidfVectorizer\n",
      "2020-06-15 23:45:58.278 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 241: Saving model to /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/tfidf.pkl\n",
      "11:45:58 INFO:Saving model to /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/tfidf.pkl\n",
      "2020-06-15 23:45:58.280 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 115: [loading vocabulary from /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/en_mipt_answers.dict]\n",
      "11:45:58 INFO:[loading vocabulary from /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/en_mipt_answers.dict]\n",
      "2020-06-15 23:45:58.281 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 101: [saving vocabulary to /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/en_mipt_answers.dict]\n",
      "11:45:58 INFO:[saving vocabulary to /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/en_mipt_answers.dict]\n",
      "2020-06-15 23:45:58.283 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 203: Loading model sklearn.linear_model:LogisticRegression from /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/logreg.pkl\n",
      "11:45:58 INFO:Loading model sklearn.linear_model:LogisticRegression from /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/logreg.pkl\n",
      "2020-06-15 23:45:58.284 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 210: Model sklearn.linear_model.logisticLogisticRegression loaded  with parameters\n",
      "11:45:58 INFO:Model sklearn.linear_model.logisticLogisticRegression loaded  with parameters\n",
      "2020-06-15 23:45:58.285 WARNING in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 216: Fitting of loaded model can not be continued. Model can be fitted from scratch.If one needs to continue fitting, please, look at `warm_start` parameter\n",
      "11:45:58 WARNING:Fitting of loaded model can not be continued. Model can be fitted from scratch.If one needs to continue fitting, please, look at `warm_start` parameter\n",
      "2020-06-15 23:45:58.291 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 109: Fitting model sklearn.linear_model.logisticLogisticRegression\n",
      "11:45:58 INFO:Fitting model sklearn.linear_model.logisticLogisticRegression\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "2020-06-15 23:45:58.293 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 241: Saving model to /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/logreg.pkl\n",
      "11:45:58 INFO:Saving model to /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/logreg.pkl\n",
      "2020-06-15 23:45:59.39 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 203: Loading model sklearn.feature_extraction.text:TfidfVectorizer from /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/tfidf.pkl\n",
      "11:45:59 INFO:Loading model sklearn.feature_extraction.text:TfidfVectorizer from /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/tfidf.pkl\n",
      "2020-06-15 23:45:59.40 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 210: Model sklearn.feature_extraction.textTfidfVectorizer loaded  with parameters\n",
      "11:45:59 INFO:Model sklearn.feature_extraction.textTfidfVectorizer loaded  with parameters\n",
      "2020-06-15 23:45:59.41 WARNING in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 216: Fitting of loaded model can not be continued. Model can be fitted from scratch.If one needs to continue fitting, please, look at `warm_start` parameter\n",
      "11:45:59 WARNING:Fitting of loaded model can not be continued. Model can be fitted from scratch.If one needs to continue fitting, please, look at `warm_start` parameter\n",
      "2020-06-15 23:45:59.43 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 115: [loading vocabulary from /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/en_mipt_answers.dict]\n",
      "11:45:59 INFO:[loading vocabulary from /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/en_mipt_answers.dict]\n",
      "2020-06-15 23:45:59.44 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 203: Loading model sklearn.linear_model:LogisticRegression from /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/logreg.pkl\n",
      "11:45:59 INFO:Loading model sklearn.linear_model:LogisticRegression from /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/logreg.pkl\n",
      "2020-06-15 23:45:59.45 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 210: Model sklearn.linear_model.logisticLogisticRegression loaded  with parameters\n",
      "11:45:59 INFO:Model sklearn.linear_model.logisticLogisticRegression loaded  with parameters\n",
      "2020-06-15 23:45:59.46 WARNING in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 216: Fitting of loaded model can not be continued. Model can be fitted from scratch.If one needs to continue fitting, please, look at `warm_start` parameter\n",
      "11:45:59 WARNING:Fitting of loaded model can not be continued. Model can be fitted from scratch.If one needs to continue fitting, please, look at `warm_start` parameter\n",
      "2020-06-15 23:45:59.788 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 203: Loading model sklearn.feature_extraction.text:TfidfVectorizer from /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/tfidf.pkl\n",
      "11:45:59 INFO:Loading model sklearn.feature_extraction.text:TfidfVectorizer from /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/tfidf.pkl\n",
      "2020-06-15 23:45:59.789 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 210: Model sklearn.feature_extraction.textTfidfVectorizer loaded  with parameters\n",
      "11:45:59 INFO:Model sklearn.feature_extraction.textTfidfVectorizer loaded  with parameters\n",
      "2020-06-15 23:45:59.790 WARNING in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 216: Fitting of loaded model can not be continued. Model can be fitted from scratch.If one needs to continue fitting, please, look at `warm_start` parameter\n",
      "11:45:59 WARNING:Fitting of loaded model can not be continued. Model can be fitted from scratch.If one needs to continue fitting, please, look at `warm_start` parameter\n",
      "2020-06-15 23:45:59.791 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 115: [loading vocabulary from /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/en_mipt_answers.dict]\n",
      "11:45:59 INFO:[loading vocabulary from /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/en_mipt_answers.dict]\n",
      "2020-06-15 23:45:59.793 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 203: Loading model sklearn.linear_model:LogisticRegression from /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/logreg.pkl\n",
      "11:45:59 INFO:Loading model sklearn.linear_model:LogisticRegression from /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/logreg.pkl\n",
      "2020-06-15 23:45:59.794 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 210: Model sklearn.linear_model.logisticLogisticRegression loaded  with parameters\n",
      "11:45:59 INFO:Model sklearn.linear_model.logisticLogisticRegression loaded  with parameters\n",
      "2020-06-15 23:45:59.795 WARNING in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 216: Fitting of loaded model can not be continued. Model can be fitted from scratch.If one needs to continue fitting, please, look at `warm_start` parameter\n",
      "11:45:59 WARNING:Fitting of loaded model can not be continued. Model can be fitted from scratch.If one needs to continue fitting, please, look at `warm_start` parameter\n",
      "11:46:01 DEBUG:Responses: [list([['AI research. The goal is to make\\n                an automatic pilot'], [19], [159.49786376953125]])]\n",
      "11:46:01 DEBUG:Top Responses: ['AI research. The goal is to make\\n                an automatic pilot']\n",
      "11:46:02 DEBUG:Responses: [list([['to make\\n                an automatic pilot'], [44], [129.35693359375]])]\n",
      "11:46:02 DEBUG:Top Responses: ['to make\\n                an automatic pilot']\n",
      "2020-06-15 23:46:04.320 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 203: Loading model sklearn.feature_extraction.text:TfidfVectorizer from /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/tfidf.pkl\n",
      "11:46:04 INFO:Loading model sklearn.feature_extraction.text:TfidfVectorizer from /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/tfidf.pkl\n",
      "2020-06-15 23:46:04.322 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 210: Model sklearn.feature_extraction.textTfidfVectorizer loaded  with parameters\n",
      "11:46:04 INFO:Model sklearn.feature_extraction.textTfidfVectorizer loaded  with parameters\n",
      "2020-06-15 23:46:04.323 WARNING in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 216: Fitting of loaded model can not be continued. Model can be fitted from scratch.If one needs to continue fitting, please, look at `warm_start` parameter\n",
      "11:46:04 WARNING:Fitting of loaded model can not be continued. Model can be fitted from scratch.If one needs to continue fitting, please, look at `warm_start` parameter\n",
      "2020-06-15 23:46:04.355 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 109: Fitting model sklearn.feature_extraction.textTfidfVectorizer\n",
      "11:46:04 INFO:Fitting model sklearn.feature_extraction.textTfidfVectorizer\n",
      "2020-06-15 23:46:04.358 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 241: Saving model to /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/tfidf.pkl\n",
      "11:46:04 INFO:Saving model to /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/tfidf.pkl\n",
      "2020-06-15 23:46:04.360 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 115: [loading vocabulary from /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/en_mipt_answers.dict]\n",
      "11:46:04 INFO:[loading vocabulary from /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/en_mipt_answers.dict]\n",
      "2020-06-15 23:46:04.362 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 101: [saving vocabulary to /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/en_mipt_answers.dict]\n",
      "11:46:04 INFO:[saving vocabulary to /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/en_mipt_answers.dict]\n",
      "2020-06-15 23:46:04.363 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 203: Loading model sklearn.linear_model:LogisticRegression from /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/logreg.pkl\n",
      "11:46:04 INFO:Loading model sklearn.linear_model:LogisticRegression from /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/logreg.pkl\n",
      "2020-06-15 23:46:04.364 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 210: Model sklearn.linear_model.logisticLogisticRegression loaded  with parameters\n",
      "11:46:04 INFO:Model sklearn.linear_model.logisticLogisticRegression loaded  with parameters\n",
      "2020-06-15 23:46:04.365 WARNING in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 216: Fitting of loaded model can not be continued. Model can be fitted from scratch.If one needs to continue fitting, please, look at `warm_start` parameter\n",
      "11:46:04 WARNING:Fitting of loaded model can not be continued. Model can be fitted from scratch.If one needs to continue fitting, please, look at `warm_start` parameter\n",
      "2020-06-15 23:46:04.386 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 109: Fitting model sklearn.linear_model.logisticLogisticRegression\n",
      "11:46:04 INFO:Fitting model sklearn.linear_model.logisticLogisticRegression\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "2020-06-15 23:46:04.390 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 241: Saving model to /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/logreg.pkl\n",
      "11:46:04 INFO:Saving model to /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/logreg.pkl\n",
      "2020-06-15 23:46:05.159 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 203: Loading model sklearn.feature_extraction.text:TfidfVectorizer from /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/tfidf.pkl\n",
      "11:46:05 INFO:Loading model sklearn.feature_extraction.text:TfidfVectorizer from /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/tfidf.pkl\n",
      "2020-06-15 23:46:05.161 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 210: Model sklearn.feature_extraction.textTfidfVectorizer loaded  with parameters\n",
      "11:46:05 INFO:Model sklearn.feature_extraction.textTfidfVectorizer loaded  with parameters\n",
      "2020-06-15 23:46:05.162 WARNING in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 216: Fitting of loaded model can not be continued. Model can be fitted from scratch.If one needs to continue fitting, please, look at `warm_start` parameter\n",
      "11:46:05 WARNING:Fitting of loaded model can not be continued. Model can be fitted from scratch.If one needs to continue fitting, please, look at `warm_start` parameter\n",
      "2020-06-15 23:46:05.164 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 115: [loading vocabulary from /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/en_mipt_answers.dict]\n",
      "11:46:05 INFO:[loading vocabulary from /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/en_mipt_answers.dict]\n",
      "2020-06-15 23:46:05.166 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 203: Loading model sklearn.linear_model:LogisticRegression from /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/logreg.pkl\n",
      "11:46:05 INFO:Loading model sklearn.linear_model:LogisticRegression from /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/logreg.pkl\n",
      "2020-06-15 23:46:05.167 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 210: Model sklearn.linear_model.logisticLogisticRegression loaded  with parameters\n",
      "11:46:05 INFO:Model sklearn.linear_model.logisticLogisticRegression loaded  with parameters\n",
      "2020-06-15 23:46:05.168 WARNING in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 216: Fitting of loaded model can not be continued. Model can be fitted from scratch.If one needs to continue fitting, please, look at `warm_start` parameter\n",
      "11:46:05 WARNING:Fitting of loaded model can not be continued. Model can be fitted from scratch.If one needs to continue fitting, please, look at `warm_start` parameter\n",
      "2020-06-15 23:46:05.915 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 203: Loading model sklearn.feature_extraction.text:TfidfVectorizer from /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/tfidf.pkl\n",
      "11:46:05 INFO:Loading model sklearn.feature_extraction.text:TfidfVectorizer from /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/tfidf.pkl\n",
      "2020-06-15 23:46:05.916 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 210: Model sklearn.feature_extraction.textTfidfVectorizer loaded  with parameters\n",
      "11:46:05 INFO:Model sklearn.feature_extraction.textTfidfVectorizer loaded  with parameters\n",
      "2020-06-15 23:46:05.917 WARNING in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 216: Fitting of loaded model can not be continued. Model can be fitted from scratch.If one needs to continue fitting, please, look at `warm_start` parameter\n",
      "11:46:05 WARNING:Fitting of loaded model can not be continued. Model can be fitted from scratch.If one needs to continue fitting, please, look at `warm_start` parameter\n",
      "2020-06-15 23:46:05.919 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 115: [loading vocabulary from /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/en_mipt_answers.dict]\n",
      "11:46:05 INFO:[loading vocabulary from /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/en_mipt_answers.dict]\n",
      "2020-06-15 23:46:05.922 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 203: Loading model sklearn.linear_model:LogisticRegression from /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/logreg.pkl\n",
      "11:46:05 INFO:Loading model sklearn.linear_model:LogisticRegression from /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/logreg.pkl\n",
      "2020-06-15 23:46:05.923 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 210: Model sklearn.linear_model.logisticLogisticRegression loaded  with parameters\n",
      "11:46:05 INFO:Model sklearn.linear_model.logisticLogisticRegression loaded  with parameters\n",
      "2020-06-15 23:46:05.924 WARNING in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 216: Fitting of loaded model can not be continued. Model can be fitted from scratch.If one needs to continue fitting, please, look at `warm_start` parameter\n",
      "11:46:05 WARNING:Fitting of loaded model can not be continued. Model can be fitted from scratch.If one needs to continue fitting, please, look at `warm_start` parameter\n",
      "11:46:05 DEBUG:  FAQ dataset and model updated..\n",
      "11:46:05 DEBUG:  What is Intekglobal?:\n",
      "\n",
      "0: No, it is life on earth\n",
      "1: to make\n",
      "                an automatic pilot\n",
      "2: AI research. The goal is to make\n",
      "                an automatic pilot\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tempfile\n",
    "from unittest.mock import patch\n",
    "from shutil import copyfile\n",
    "from collections import defaultdict\n",
    "\n",
    "def mock_faq_files(tmpdirname, faqs, faq_dic):\n",
    "\n",
    "    faq_dic['path'] = path.join(tmpdirname, 'temp_faq.csv')\n",
    "    faq_dic['config'] = path.join(tmpdirname, 'temp_config_faq.json')\n",
    "    faq_dic['df'] = pd.DataFrame(faqs)\n",
    "    faq_dic['df'].to_csv(faq_dic['path'], index=False)\n",
    "\n",
    "    copyfile(configs.faq.tfidf_logreg_en_faq, faq_dic['config'])\n",
    "\n",
    "    updates_faq_config_file(\n",
    "        configs_path=faq_dic['config'],\n",
    "        dataset_reader={'data_path': faq_dic['path']}\n",
    "    )\n",
    "\n",
    "\n",
    "def mock_context_file(tmpdirname, contexts, context_dic):\n",
    "\n",
    "    context_dic['path'] = path.join(tmpdirname, 'temp_context.csv')\n",
    "    context_dic['df'] = pd.DataFrame(contexts)\n",
    "    context_dic['df'].to_csv(context_dic['path'], index=False)\n",
    "\n",
    "\n",
    "@patch('__main__.get_input')\n",
    "def test_context_response_with_no_updates(mock_input):\n",
    "    mock_input.side_effect = ['Who is Enrique Jimenez?', 'N']\n",
    "    data = {'context': defaultdict(str), 'faq': defaultdict(str)}\n",
    "    contexts = {\n",
    "        'context':\n",
    "            [\n",
    "                'Intekglobal has its headquarters located in TJ',\n",
    "                'In Intekglobal we care about you',\n",
    "                '''Enrique Jimenez is one of the smartest minds on the planet, \n",
    "                   he currently works as Intekglobal employee'''\n",
    "            ],\n",
    "        'topic': ['headquarters', 'mission', 'Enrique\\'s biography']\n",
    "    }\n",
    "\n",
    "    faqs = {\n",
    "        'Question':\n",
    "            ['Minimum number of questions?', 'This is the other question?'],\n",
    "        'Answer': ['Two', 'yes']\n",
    "    }\n",
    "\n",
    "    with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "\n",
    "        mock_faq_files(tmpdirname, faqs, data['faq'])\n",
    "        mock_context_file(tmpdirname, contexts, data['context'])\n",
    "\n",
    "        qa_models = load_qa_models(\n",
    "            config_tfidf=data['faq']['config'], download=False\n",
    "        )\n",
    "\n",
    "        responses, status = question_response(data, qa_models)\n",
    "        logging.debug(f'  {status}')\n",
    "        logging.debug(f'  {responses}')\n",
    "        assert 'no data updates..' == status\n",
    "        assert 'one of the smartest minds on the planet' in responses\n",
    "\n",
    "@patch('__main__.get_input')\n",
    "def test_updating_faq_dataset(mock_input):\n",
    "\n",
    "    new_answer = 'Intekglobal is one of the best companies in the world'\n",
    "    mock_input.side_effect = ['What is Intekglobal?', 'Y', 'N', new_answer]\n",
    "\n",
    "    data = {'context': defaultdict(str), 'faq': defaultdict(str)}\n",
    "    contexts = {\n",
    "        'context':\n",
    "            [\n",
    "                '''Tesla do important AI research. \n",
    "                The goal is to make an automatic pilot.\n",
    "                '''\n",
    "            ],\n",
    "        'topic': ['Tesla AI']\n",
    "    }\n",
    "\n",
    "    faqs = {\n",
    "        'Question':\n",
    "            ['Who  owns Tesla Company?', 'Is this is heaven?'],\n",
    "        'Answer': ['Elon Musk is the owner of Tesla', 'No, it is life on earth']\n",
    "    }\n",
    "\n",
    "    with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "\n",
    "        mock_faq_files(tmpdirname, faqs, data['faq'])\n",
    "        mock_context_file(tmpdirname, contexts, data['context'])\n",
    "\n",
    "        qa_models = load_qa_models(\n",
    "            config_tfidf=data['faq']['config'], download=False\n",
    "        )\n",
    "\n",
    "        responses, status = question_response(data, qa_models)\n",
    "        \n",
    "        logging.debug(f'  {status}')\n",
    "        logging.debug(f'  {responses}')\n",
    "        \n",
    "        assert 'FAQ dataset and model updated..' == status\n",
    "\n",
    "        updated_faq = pd.read_csv(data['faq']['path'])\n",
    "\n",
    "        assert updated_faq[updated_faq['Answer'] == new_answer\n",
    "                              ].shape[0] == 1\n",
    "\n",
    "        \n",
    "test_context_response_with_no_updates()\n",
    "test_updating_faq_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-06-16 00:13:35.734 INFO in 'deeppavlov.models.preprocessors.squad_preprocessor'['squad_preprocessor'] at line 310: SquadVocabEmbedder: loading saved tokens vocab from /home/jovyan/.deeppavlov/models/squad_model/emb/vocab_embedder.pckl\n",
      "12:13:35 INFO:SquadVocabEmbedder: loading saved tokens vocab from /home/jovyan/.deeppavlov/models/squad_model/emb/vocab_embedder.pckl\n",
      "2020-06-16 00:13:38.250 INFO in 'deeppavlov.models.preprocessors.squad_preprocessor'['squad_preprocessor'] at line 310: SquadVocabEmbedder: loading saved chars vocab from /home/jovyan/.deeppavlov/models/squad_model/emb/char_vocab_embedder.pckl\n",
      "12:13:38 INFO:SquadVocabEmbedder: loading saved chars vocab from /home/jovyan/.deeppavlov/models/squad_model/emb/char_vocab_embedder.pckl\n",
      "2020-06-16 00:13:38.783 INFO in 'deeppavlov.core.layers.tf_layers'['tf_layers'] at line 615: \n",
      "Warning! tf.contrib.cudnn_rnn.CudnnCompatibleGRUCell is used. It is okay for inference mode, but if you train your model with this cell it could NOT be used with tf.contrib.cudnn_rnn.CudnnGRUCell later. \n",
      "12:13:38 INFO:\n",
      "Warning! tf.contrib.cudnn_rnn.CudnnCompatibleGRUCell is used. It is okay for inference mode, but if you train your model with this cell it could NOT be used with tf.contrib.cudnn_rnn.CudnnGRUCell later. \n",
      "2020-06-16 00:13:40.129 INFO in 'deeppavlov.core.layers.tf_layers'['tf_layers'] at line 615: \n",
      "Warning! tf.contrib.cudnn_rnn.CudnnCompatibleGRUCell is used. It is okay for inference mode, but if you train your model with this cell it could NOT be used with tf.contrib.cudnn_rnn.CudnnGRUCell later. \n",
      "12:13:40 INFO:\n",
      "Warning! tf.contrib.cudnn_rnn.CudnnCompatibleGRUCell is used. It is okay for inference mode, but if you train your model with this cell it could NOT be used with tf.contrib.cudnn_rnn.CudnnGRUCell later. \n",
      "2020-06-16 00:13:40.301 INFO in 'deeppavlov.core.layers.tf_layers'['tf_layers'] at line 615: \n",
      "Warning! tf.contrib.cudnn_rnn.CudnnCompatibleGRUCell is used. It is okay for inference mode, but if you train your model with this cell it could NOT be used with tf.contrib.cudnn_rnn.CudnnGRUCell later. \n",
      "12:13:40 INFO:\n",
      "Warning! tf.contrib.cudnn_rnn.CudnnCompatibleGRUCell is used. It is okay for inference mode, but if you train your model with this cell it could NOT be used with tf.contrib.cudnn_rnn.CudnnGRUCell later. \n",
      "2020-06-16 00:13:40.407 INFO in 'deeppavlov.core.layers.tf_layers'['tf_layers'] at line 615: \n",
      "Warning! tf.contrib.cudnn_rnn.CudnnCompatibleGRUCell is used. It is okay for inference mode, but if you train your model with this cell it could NOT be used with tf.contrib.cudnn_rnn.CudnnGRUCell later. \n",
      "12:13:40 INFO:\n",
      "Warning! tf.contrib.cudnn_rnn.CudnnCompatibleGRUCell is used. It is okay for inference mode, but if you train your model with this cell it could NOT be used with tf.contrib.cudnn_rnn.CudnnGRUCell later. \n",
      "2020-06-16 00:13:52.632 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 51: [loading model from /home/jovyan/.deeppavlov/models/squad_model/model]\n",
      "12:13:52 INFO:[loading model from /home/jovyan/.deeppavlov/models/squad_model/model]\n",
      "12:13:52 INFO:Restoring parameters from /home/jovyan/.deeppavlov/models/squad_model/model\n",
      "2020-06-16 00:14:09.850 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 51: [loading model from /home/jovyan/.deeppavlov/models/squad_bert/model]\n",
      "12:14:09 INFO:[loading model from /home/jovyan/.deeppavlov/models/squad_bert/model]\n",
      "12:14:10 INFO:Restoring parameters from /home/jovyan/.deeppavlov/models/squad_bert/model\n",
      "2020-06-16 00:14:16.767 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 203: Loading model sklearn.feature_extraction.text:TfidfVectorizer from /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/tfidf.pkl\n",
      "12:14:16 INFO:Loading model sklearn.feature_extraction.text:TfidfVectorizer from /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/tfidf.pkl\n",
      "2020-06-16 00:14:16.769 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 210: Model sklearn.feature_extraction.textTfidfVectorizer loaded  with parameters\n",
      "12:14:16 INFO:Model sklearn.feature_extraction.textTfidfVectorizer loaded  with parameters\n",
      "2020-06-16 00:14:16.770 WARNING in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 216: Fitting of loaded model can not be continued. Model can be fitted from scratch.If one needs to continue fitting, please, look at `warm_start` parameter\n",
      "12:14:16 WARNING:Fitting of loaded model can not be continued. Model can be fitted from scratch.If one needs to continue fitting, please, look at `warm_start` parameter\n",
      "2020-06-16 00:14:16.780 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 109: Fitting model sklearn.feature_extraction.textTfidfVectorizer\n",
      "12:14:16 INFO:Fitting model sklearn.feature_extraction.textTfidfVectorizer\n",
      "2020-06-16 00:14:16.783 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 241: Saving model to /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/tfidf.pkl\n",
      "12:14:16 INFO:Saving model to /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/tfidf.pkl\n",
      "2020-06-16 00:14:16.786 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 115: [loading vocabulary from /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/en_mipt_answers.dict]\n",
      "12:14:16 INFO:[loading vocabulary from /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/en_mipt_answers.dict]\n",
      "2020-06-16 00:14:16.789 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 101: [saving vocabulary to /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/en_mipt_answers.dict]\n",
      "12:14:16 INFO:[saving vocabulary to /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/en_mipt_answers.dict]\n",
      "2020-06-16 00:14:16.791 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 203: Loading model sklearn.linear_model:LogisticRegression from /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/logreg.pkl\n",
      "12:14:16 INFO:Loading model sklearn.linear_model:LogisticRegression from /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/logreg.pkl\n",
      "2020-06-16 00:14:16.793 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 210: Model sklearn.linear_model.logisticLogisticRegression loaded  with parameters\n",
      "12:14:16 INFO:Model sklearn.linear_model.logisticLogisticRegression loaded  with parameters\n",
      "2020-06-16 00:14:16.794 WARNING in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 216: Fitting of loaded model can not be continued. Model can be fitted from scratch.If one needs to continue fitting, please, look at `warm_start` parameter\n",
      "12:14:16 WARNING:Fitting of loaded model can not be continued. Model can be fitted from scratch.If one needs to continue fitting, please, look at `warm_start` parameter\n",
      "2020-06-16 00:14:16.802 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 109: Fitting model sklearn.linear_model.logisticLogisticRegression\n",
      "12:14:16 INFO:Fitting model sklearn.linear_model.logisticLogisticRegression\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "2020-06-16 00:14:16.806 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 241: Saving model to /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/logreg.pkl\n",
      "12:14:16 INFO:Saving model to /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/logreg.pkl\n",
      "2020-06-16 00:14:17.596 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 203: Loading model sklearn.feature_extraction.text:TfidfVectorizer from /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/tfidf.pkl\n",
      "12:14:17 INFO:Loading model sklearn.feature_extraction.text:TfidfVectorizer from /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/tfidf.pkl\n",
      "2020-06-16 00:14:17.598 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 210: Model sklearn.feature_extraction.textTfidfVectorizer loaded  with parameters\n",
      "12:14:17 INFO:Model sklearn.feature_extraction.textTfidfVectorizer loaded  with parameters\n",
      "2020-06-16 00:14:17.600 WARNING in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 216: Fitting of loaded model can not be continued. Model can be fitted from scratch.If one needs to continue fitting, please, look at `warm_start` parameter\n",
      "12:14:17 WARNING:Fitting of loaded model can not be continued. Model can be fitted from scratch.If one needs to continue fitting, please, look at `warm_start` parameter\n",
      "2020-06-16 00:14:17.602 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 115: [loading vocabulary from /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/en_mipt_answers.dict]\n",
      "12:14:17 INFO:[loading vocabulary from /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/en_mipt_answers.dict]\n",
      "2020-06-16 00:14:17.605 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 203: Loading model sklearn.linear_model:LogisticRegression from /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/logreg.pkl\n",
      "12:14:17 INFO:Loading model sklearn.linear_model:LogisticRegression from /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/logreg.pkl\n",
      "2020-06-16 00:14:17.607 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 210: Model sklearn.linear_model.logisticLogisticRegression loaded  with parameters\n",
      "12:14:17 INFO:Model sklearn.linear_model.logisticLogisticRegression loaded  with parameters\n",
      "2020-06-16 00:14:17.608 WARNING in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 216: Fitting of loaded model can not be continued. Model can be fitted from scratch.If one needs to continue fitting, please, look at `warm_start` parameter\n",
      "12:14:17 WARNING:Fitting of loaded model can not be continued. Model can be fitted from scratch.If one needs to continue fitting, please, look at `warm_start` parameter\n",
      "2020-06-16 00:14:18.422 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 203: Loading model sklearn.feature_extraction.text:TfidfVectorizer from /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/tfidf.pkl\n",
      "12:14:18 INFO:Loading model sklearn.feature_extraction.text:TfidfVectorizer from /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/tfidf.pkl\n",
      "2020-06-16 00:14:18.424 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 210: Model sklearn.feature_extraction.textTfidfVectorizer loaded  with parameters\n",
      "12:14:18 INFO:Model sklearn.feature_extraction.textTfidfVectorizer loaded  with parameters\n",
      "2020-06-16 00:14:18.425 WARNING in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 216: Fitting of loaded model can not be continued. Model can be fitted from scratch.If one needs to continue fitting, please, look at `warm_start` parameter\n",
      "12:14:18 WARNING:Fitting of loaded model can not be continued. Model can be fitted from scratch.If one needs to continue fitting, please, look at `warm_start` parameter\n",
      "2020-06-16 00:14:18.428 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 115: [loading vocabulary from /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/en_mipt_answers.dict]\n",
      "12:14:18 INFO:[loading vocabulary from /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/en_mipt_answers.dict]\n",
      "2020-06-16 00:14:18.431 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 203: Loading model sklearn.linear_model:LogisticRegression from /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/logreg.pkl\n",
      "12:14:18 INFO:Loading model sklearn.linear_model:LogisticRegression from /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/logreg.pkl\n",
      "2020-06-16 00:14:18.432 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 210: Model sklearn.linear_model.logisticLogisticRegression loaded  with parameters\n",
      "12:14:18 INFO:Model sklearn.linear_model.logisticLogisticRegression loaded  with parameters\n",
      "2020-06-16 00:14:18.433 WARNING in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 216: Fitting of loaded model can not be continued. Model can be fitted from scratch.If one needs to continue fitting, please, look at `warm_start` parameter\n",
      "12:14:18 WARNING:Fitting of loaded model can not be continued. Model can be fitted from scratch.If one needs to continue fitting, please, look at `warm_start` parameter\n",
      "12:14:20 DEBUG:Responses: [list([['greatest punk rock bands from all the time\\n                is the Ramones'], [11], [92.5745849609375]])]\n",
      "12:14:20 DEBUG:Top Responses: ['greatest punk rock bands from all the time\\n                is the Ramones']\n",
      "12:14:22 DEBUG:Responses: [list([['One of the greatest punk rock bands from all the time'], [0], [0.04293891042470932]])]\n",
      "12:14:22 DEBUG:Top Responses: ['One of the greatest punk rock bands from all the time']\n",
      "2020-06-16 00:14:23.134 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 203: Loading model sklearn.feature_extraction.text:TfidfVectorizer from /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/tfidf.pkl\n",
      "12:14:23 INFO:Loading model sklearn.feature_extraction.text:TfidfVectorizer from /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/tfidf.pkl\n",
      "2020-06-16 00:14:23.136 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 210: Model sklearn.feature_extraction.textTfidfVectorizer loaded  with parameters\n",
      "12:14:23 INFO:Model sklearn.feature_extraction.textTfidfVectorizer loaded  with parameters\n",
      "2020-06-16 00:14:23.137 WARNING in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 216: Fitting of loaded model can not be continued. Model can be fitted from scratch.If one needs to continue fitting, please, look at `warm_start` parameter\n",
      "12:14:23 WARNING:Fitting of loaded model can not be continued. Model can be fitted from scratch.If one needs to continue fitting, please, look at `warm_start` parameter\n",
      "2020-06-16 00:14:23.143 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 109: Fitting model sklearn.feature_extraction.textTfidfVectorizer\n",
      "12:14:23 INFO:Fitting model sklearn.feature_extraction.textTfidfVectorizer\n",
      "2020-06-16 00:14:23.145 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 241: Saving model to /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/tfidf.pkl\n",
      "12:14:23 INFO:Saving model to /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/tfidf.pkl\n",
      "2020-06-16 00:14:23.147 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 115: [loading vocabulary from /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/en_mipt_answers.dict]\n",
      "12:14:23 INFO:[loading vocabulary from /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/en_mipt_answers.dict]\n",
      "2020-06-16 00:14:23.148 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 101: [saving vocabulary to /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/en_mipt_answers.dict]\n",
      "12:14:23 INFO:[saving vocabulary to /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/en_mipt_answers.dict]\n",
      "2020-06-16 00:14:23.149 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 203: Loading model sklearn.linear_model:LogisticRegression from /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/logreg.pkl\n",
      "12:14:23 INFO:Loading model sklearn.linear_model:LogisticRegression from /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/logreg.pkl\n",
      "2020-06-16 00:14:23.150 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 210: Model sklearn.linear_model.logisticLogisticRegression loaded  with parameters\n",
      "12:14:23 INFO:Model sklearn.linear_model.logisticLogisticRegression loaded  with parameters\n",
      "2020-06-16 00:14:23.151 WARNING in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 216: Fitting of loaded model can not be continued. Model can be fitted from scratch.If one needs to continue fitting, please, look at `warm_start` parameter\n",
      "12:14:23 WARNING:Fitting of loaded model can not be continued. Model can be fitted from scratch.If one needs to continue fitting, please, look at `warm_start` parameter\n",
      "2020-06-16 00:14:23.156 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 109: Fitting model sklearn.linear_model.logisticLogisticRegression\n",
      "12:14:23 INFO:Fitting model sklearn.linear_model.logisticLogisticRegression\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "2020-06-16 00:14:23.158 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 241: Saving model to /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/logreg.pkl\n",
      "12:14:23 INFO:Saving model to /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/logreg.pkl\n",
      "2020-06-16 00:14:24.624 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 203: Loading model sklearn.feature_extraction.text:TfidfVectorizer from /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/tfidf.pkl\n",
      "12:14:24 INFO:Loading model sklearn.feature_extraction.text:TfidfVectorizer from /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/tfidf.pkl\n",
      "2020-06-16 00:14:24.625 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 210: Model sklearn.feature_extraction.textTfidfVectorizer loaded  with parameters\n",
      "12:14:24 INFO:Model sklearn.feature_extraction.textTfidfVectorizer loaded  with parameters\n",
      "2020-06-16 00:14:24.626 WARNING in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 216: Fitting of loaded model can not be continued. Model can be fitted from scratch.If one needs to continue fitting, please, look at `warm_start` parameter\n",
      "12:14:24 WARNING:Fitting of loaded model can not be continued. Model can be fitted from scratch.If one needs to continue fitting, please, look at `warm_start` parameter\n",
      "2020-06-16 00:14:24.627 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 115: [loading vocabulary from /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/en_mipt_answers.dict]\n",
      "12:14:24 INFO:[loading vocabulary from /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/en_mipt_answers.dict]\n",
      "2020-06-16 00:14:24.629 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 203: Loading model sklearn.linear_model:LogisticRegression from /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/logreg.pkl\n",
      "12:14:24 INFO:Loading model sklearn.linear_model:LogisticRegression from /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/logreg.pkl\n",
      "2020-06-16 00:14:24.630 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 210: Model sklearn.linear_model.logisticLogisticRegression loaded  with parameters\n",
      "12:14:24 INFO:Model sklearn.linear_model.logisticLogisticRegression loaded  with parameters\n",
      "2020-06-16 00:14:24.631 WARNING in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 216: Fitting of loaded model can not be continued. Model can be fitted from scratch.If one needs to continue fitting, please, look at `warm_start` parameter\n",
      "12:14:24 WARNING:Fitting of loaded model can not be continued. Model can be fitted from scratch.If one needs to continue fitting, please, look at `warm_start` parameter\n",
      "2020-06-16 00:14:25.433 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 203: Loading model sklearn.feature_extraction.text:TfidfVectorizer from /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/tfidf.pkl\n",
      "12:14:25 INFO:Loading model sklearn.feature_extraction.text:TfidfVectorizer from /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/tfidf.pkl\n",
      "2020-06-16 00:14:25.434 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 210: Model sklearn.feature_extraction.textTfidfVectorizer loaded  with parameters\n",
      "12:14:25 INFO:Model sklearn.feature_extraction.textTfidfVectorizer loaded  with parameters\n",
      "2020-06-16 00:14:25.435 WARNING in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 216: Fitting of loaded model can not be continued. Model can be fitted from scratch.If one needs to continue fitting, please, look at `warm_start` parameter\n",
      "12:14:25 WARNING:Fitting of loaded model can not be continued. Model can be fitted from scratch.If one needs to continue fitting, please, look at `warm_start` parameter\n",
      "2020-06-16 00:14:25.437 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 115: [loading vocabulary from /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/en_mipt_answers.dict]\n",
      "12:14:25 INFO:[loading vocabulary from /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/en_mipt_answers.dict]\n",
      "2020-06-16 00:14:25.439 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 203: Loading model sklearn.linear_model:LogisticRegression from /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/logreg.pkl\n",
      "12:14:25 INFO:Loading model sklearn.linear_model:LogisticRegression from /home/jovyan/.deeppavlov/models/faq/mipt/en_mipt_faq_v4/logreg.pkl\n",
      "2020-06-16 00:14:25.440 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 210: Model sklearn.linear_model.logisticLogisticRegression loaded  with parameters\n",
      "12:14:25 INFO:Model sklearn.linear_model.logisticLogisticRegression loaded  with parameters\n",
      "2020-06-16 00:14:25.441 WARNING in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 216: Fitting of loaded model can not be continued. Model can be fitted from scratch.If one needs to continue fitting, please, look at `warm_start` parameter\n",
      "12:14:25 WARNING:Fitting of loaded model can not be continued. Model can be fitted from scratch.If one needs to continue fitting, please, look at `warm_start` parameter\n",
      "12:14:25 DEBUG:Responses: [list([['greatest punk rock bands from all the time\\n                is the Ramones'], [11], [92.5745849609375]])]\n",
      "12:14:25 DEBUG:Top Responses: ['greatest punk rock bands from all the time\\n                is the Ramones']\n",
      "12:14:26 DEBUG:Responses: [list([['One of the greatest punk rock bands from all the time'], [0], [0.04293891042470932]])]\n",
      "12:14:26 DEBUG:Top Responses: ['One of the greatest punk rock bands from all the time']\n",
      "12:14:26 INFO:Old response:\n",
      " What is Intekglobal?:\n",
      "\n",
      "0: One of the greatest punk rock bands from all the time\n",
      "1: No, it is life on earth\n",
      "2: greatest punk rock bands from all the time\n",
      "                is the Ramones\n",
      "\n",
      "12:14:26 INFO:New response:\n",
      "What is Intekglobal?:\n",
      "\n",
      "0: One of the greatest punk rock bands from all the time\n",
      "1: Intekglobal is one of the best companies in the world\n",
      "2: greatest punk rock bands from all the time\n",
      "                is the Ramones\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tempfile\n",
    "from unittest.mock import patch\n",
    "from shutil import copyfile\n",
    "from collections import defaultdict\n",
    "\n",
    "def mock_faq_files(tmpdirname, faqs, faq_dic):\n",
    "\n",
    "    faq_dic['path'] = path.join(tmpdirname, 'temp_faq.csv')\n",
    "    faq_dic['config'] = path.join(tmpdirname, 'temp_config_faq.json')\n",
    "    faq_dic['df'] = pd.DataFrame(faqs)\n",
    "    faq_dic['df'].to_csv(faq_dic['path'], index=False)\n",
    "\n",
    "    copyfile(configs.faq.tfidf_logreg_en_faq, faq_dic['config'])\n",
    "\n",
    "    updates_faq_config_file(\n",
    "        configs_path=faq_dic['config'],\n",
    "        dataset_reader={'data_path': faq_dic['path']}\n",
    "    )\n",
    "\n",
    "\n",
    "def mock_context_file(tmpdirname, contexts, context_dic):\n",
    "\n",
    "    context_dic['path'] = path.join(tmpdirname, 'temp_context.csv')\n",
    "    context_dic['df'] = pd.DataFrame(contexts)\n",
    "    context_dic['df'].to_csv(context_dic['path'], index=False)\n",
    "\n",
    "@patch('__main__.get_input')\n",
    "def test_updated_faq_response(mock_input):\n",
    "\n",
    "    new_answer = 'Intekglobal is one of the best companies in the world'\n",
    "    question = 'What is Intekglobal?'\n",
    "    mock_input.side_effect = [question, 'Y', 'N', new_answer, question, 'N']\n",
    "\n",
    "    data = {'context': defaultdict(str), 'faq': defaultdict(str)}\n",
    "    contexts = {\n",
    "        'context':\n",
    "            [\n",
    "                '''One of the greatest punk rock bands from all the time\n",
    "                is the Ramones.\n",
    "                '''\n",
    "            ],\n",
    "        'topic': ['Ramones']\n",
    "    }\n",
    "\n",
    "    faqs = {\n",
    "        'Question':\n",
    "            ['Who was the leading vocal artist of the Ramones?', 'Is this is heaven?'],\n",
    "        'Answer': ['Joey Ramone', 'No, it is life on earth']\n",
    "    }\n",
    "\n",
    "    with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "\n",
    "        mock_faq_files(tmpdirname, faqs, data['faq'])\n",
    "        mock_context_file(tmpdirname, contexts, data['context'])\n",
    "\n",
    "        qa_models = load_qa_models(\n",
    "            config_tfidf=data['faq']['config'], download=False\n",
    "        )\n",
    "        old_responses, old_status = question_response(data, qa_models)\n",
    "        new_responses, new_status = question_response(data, qa_models)\n",
    "\n",
    "        logging.info(f'Old response:\\n {old_responses}')\n",
    "        logging.info(f'New response:\\n{new_responses}')\n",
    "        \n",
    "        new_answer not in old_responses\n",
    "        new_answer in new_responses\n",
    "\n",
    "test_updated_faq_response()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "from unittest.mock import patch\n",
    "from shutil import copyfile, copytree\n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "HOME_DIR = popen('echo $HOME').read().strip()\n",
    "\n",
    "example_contexts = pd.DataFrame(\n",
    "    {\n",
    "        'context':\n",
    "            [\n",
    "                'Intekglobal has its headquarters located in TJ',\n",
    "                'In Intekglobal we care about you'\n",
    "            ]\n",
    "    },\n",
    ")\n",
    "\n",
    "data = {\n",
    "    'context': {\n",
    "        'df': example_contexts,\n",
    "        'path': ...\n",
    "    },\n",
    "    'faq': {\n",
    "        'df': ...,\n",
    "        'path': ...\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "FAQ_DATA_FILE = path.join(\n",
    "    popen('dirname $PWD').read().strip(), 'data/faq_data.csv'\n",
    ")\n",
    "\n",
    "CONTEXT_DATA_FILE = path.join(\n",
    "    popen('dirname $PWD').read().strip(), 'data/context_data.csv'\n",
    ")\n",
    "\n",
    "\n",
    "def copy_data_files(data, tmpdirname):\n",
    "    data['context']['path'] = path.join(tmpdirname, 'tmp_context.csv')\n",
    "    data['faq']['path'] = path.join(tmpdirname, 'tmp_faq.csv')\n",
    "    data['faq']['df'] = pd.read_csv(FAQ_DATA_FILE)\n",
    "    copyfile(CONTEXT_DATA_FILE, data['context']['path'])\n",
    "    copyfile(FAQ_DATA_FILE, data['faq']['path'])\n",
    "\n",
    "\n",
    "def modify_configs(data, tmpdirname):\n",
    "\n",
    "    copy_data_files(data, tmpdirname)\n",
    "\n",
    "    tmp_configs_faq = path.join(tmpdirname, 'temp_config_faq.json')\n",
    "    tmp_model_dir = path.join(tmpdirname, 'temp_models_dir')\n",
    "\n",
    "    metadata = json.load(open(configs.faq.tfidf_logreg_en_faq)\n",
    "                        )['metadata']['variables']\n",
    "\n",
    "    models_dir = metadata['MODELS_PATH'].replace(\n",
    "        '{ROOT_PATH}', metadata['ROOT_PATH'].replace('~', HOME_DIR)\n",
    "    )\n",
    "\n",
    "    copytree(models_dir, tmp_model_dir)\n",
    "    copyfile(configs.faq.tfidf_logreg_en_faq, tmp_configs_faq)\n",
    "\n",
    "    configs.faq.tfidf_logreg_en_faq = tmp_configs_faq\n",
    "\n",
    "    updates_faq_config_file(\n",
    "        metadata={'variables': {\n",
    "            'MODELS_PATH': tmp_model_dir\n",
    "        }},\n",
    "        dataset_reader={'data_path': data['faq']['path']}\n",
    "    )\n",
    "    #pprint(json.load(open(configs.faq.tfidf_logreg_en_faq)))\n",
    "    assert path.isdir(tmp_model_dir)\n",
    "    assert path.isfile(configs.faq.tfidf_logreg_en_faq)\n",
    "\n",
    "\n",
    "@patch('__main__.get_input')\n",
    "def test_faq_answer_with_updating(mock_input):\n",
    "\n",
    "    new_answer = 'Intekglobal is one of the best companies in the world'\n",
    "    question = 'What is Intekglobal?'\n",
    "    mock_input.side_effect = [question, 'Y', 'N', new_answer, question, 'N']\n",
    "\n",
    "    original_config_file = configs.faq.tfidf_logreg_en_faq\n",
    "    qa_model_faq = qa_models['faq']['tfidf']\n",
    "\n",
    "    with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "\n",
    "        try:\n",
    "            modify_configs(data, tmpdirname)\n",
    "    \n",
    "            old_responses = question_response(data, qa_models)[0]\n",
    "            new_responses = question_response(data, qa_models)[0]\n",
    "           \n",
    "            logging.info(f'Old response:\\n {old_responses}')\n",
    "            logging.info(f'New response:\\n{new_responses}')\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "        finally:\n",
    "            configs.faq.tfidf_logreg_en_faq = original_config_file\n",
    "            qa_models['faq']['tfidf'] = qa_model_faq\n",
    "\n",
    "        assert new_answer not in old_responses\n",
    "        assert new_answer in new_responses\n",
    "\n",
    "\n",
    "test_faq_answer_with_updating()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Test Context Dialog System\n",
    "import tempfile\n",
    "from unittest.mock import patch\n",
    "from shutil import copyfile, copytree\n",
    "from pprint import pprint\n",
    "\n",
    "example_contexts = pd.DataFrame(\n",
    "    {\n",
    "        'topic': ['Headquarters', 'Mision'],\n",
    "        'context':\n",
    "            [\n",
    "                'Intekglobal has its headquarters located in TJ',\n",
    "                'In Intekglobal we care about you'\n",
    "            ]\n",
    "    }\n",
    ")\n",
    "\n",
    "data = {\n",
    "    'context': {\n",
    "        'df': example_contexts,\n",
    "        'path': ...\n",
    "    },\n",
    "    'faq': {\n",
    "        'df': ...,\n",
    "        'path': ...\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "FAQ_DATA_FILE = path.join(\n",
    "    popen('dirname $PWD').read().strip(), 'data/faq_data.csv'\n",
    ")\n",
    "\n",
    "CONTEXT_DATA_FILE = path.join(\n",
    "    popen('dirname $PWD').read().strip(), 'data/context_data.csv'\n",
    ")\n",
    "\n",
    "\n",
    "def copy_data_files(data, tmpdirname):\n",
    "    data['context']['path'] = path.join(tmpdirname, 'tmp_context.csv')\n",
    "    data['faq']['path'] = path.join(tmpdirname, 'tmp_faq.csv')\n",
    "    data['faq']['df'] = pd.read_csv(FAQ_DATA_FILE)\n",
    "    copyfile(CONTEXT_DATA_FILE, data['context']['path'])\n",
    "    copyfile(FAQ_DATA_FILE, data['faq']['path'])\n",
    "\n",
    "\n",
    "@patch('__main__.get_input')\n",
    "def test_context_new_answer(mock_input):\n",
    "\n",
    "    question = 'What is a Chatbot?'\n",
    "    new_topic = 'AI Tool & Chatbot Development'\n",
    "    new_context = '''\n",
    "\n",
    "A chatbot is an important tool for simulating intelligent conversations with humans.\n",
    "Intekglobal chatbots efficiently live message on platforms such as Facebook Messenger, \n",
    "Slack, and Telegram. Assisting consumers with a variety of purposes and industries. \n",
    "\n",
    "But chatbots are more than just a cool technology advancement. They actually transform the user experience.\n",
    "People want simple and convenient interactions with interface and products.\n",
    "\n",
    "'''\n",
    "\n",
    "    mock_input.side_effect = [\n",
    "        question, 'YES', 'yes', new_topic, new_context, question, 'N'\n",
    "    ]\n",
    "\n",
    "    with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "\n",
    "        try:\n",
    "\n",
    "            copy_data_files(data, tmpdirname)\n",
    "\n",
    "            old_responses = question_response(data, qa_models)[0]\n",
    "            logging.info(f'Old response:\\n {old_responses}')\n",
    "            new_responses = question_response(data, qa_models)[0]\n",
    "            logging.info(f'New response:\\n{new_responses}')\n",
    "        except Exception as e:\n",
    "            print(repr(e))\n",
    "\n",
    "        finally:\n",
    "            logging.info(' Test finished')\n",
    "\n",
    "        updated_context = pd.read_csv(data['context']['path'])\n",
    "\n",
    "        assert updated_context[updated_context['context'] == new_context\n",
    "                              ].shape[0] == 1\n",
    "\n",
    "        assert updated_context[updated_context['topic'] == new_topic\n",
    "                              ].shape[0] == 1\n",
    "        assert 'an important tool for simulating intelligent conversations with humans' not in old_responses\n",
    "        assert 'an important tool for simulating intelligent conversations with humans' in new_responses\n",
    "\n",
    "\n",
    "test_context_new_answer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def dialog_system(context_data_file=None, faq_data_file=None):\n",
    "    '''\n",
    "     Main Dialog System\n",
    "    '''\n",
    "\n",
    "    PARENT_DIR = popen('dirname $PWD').read().strip()\n",
    "    if context_data_file is None:\n",
    "        context_data_file = path.join(PARENT_DIR, 'data/context_data.csv')\n",
    "    if faq_data_file is None:\n",
    "        faq_data_file = path.join(PARENT_DIR, 'data/faq_data.csv')\n",
    "    \n",
    "    run_shell_installs()\n",
    "    updates_faq_config_file(dataset_reader={'data_path': faq_data_file}) \n",
    "    qa_models = load_qa_models()\n",
    "\n",
    "    context = {'df': pd.read_csv(CONTEXT_DATA_FILE), 'path': CONTEXT_DATA_FILE}\n",
    "    faq = {'df': pd.read_csv(FAQ_DATA_FILE), 'path': FAQ_DATA_FILE}\n",
    "\n",
    "    data = {'context': context, 'faq': faq}\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            question_response(data=data, qa_models=qa_models)\n",
    "        except (KeyboardInterrupt, EOFError, SystemExit):\n",
    "            logging.debug('Goodbye!')\n",
    "            return 'Goodbye!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test  dialog_system()\n",
    "\n",
    "from unittest.mock import patch\n",
    "\n",
    "@patch('__main__.run_shell_installs')\n",
    "@patch('__main__.load_qa_models')\n",
    "@patch('__main__.pd.read_csv')\n",
    "@patch('__main__.question_response')\n",
    "def test_main_keyboard_interrupt(\n",
    "    mock_question_response,\n",
    "    mock_pd_read_csv,\n",
    "    mock_load_qa_models,\n",
    "    mock_run_shell_installs,\n",
    "):\n",
    "    mock_question_response.side_effect = [\n",
    "        KeyboardInterrupt(), EOFError(),\n",
    "        SystemExit()\n",
    "    ]\n",
    "    assert 'Goodbye!' == dialog_system()\n",
    "    assert 'Goodbye!' == dialog_system()\n",
    "    assert 'Goodbye!' == dialog_system()\n",
    "\n",
    "\n",
    "test_main_keyboard_interrupt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
